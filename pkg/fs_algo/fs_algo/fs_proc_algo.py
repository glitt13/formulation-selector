import yaml
import pandas as pd
from pathlib import Path
import xarray as xr
import fs_algo.fs_algo_train_eval as fsate

print("BEGINNING algorithm training, testing, & evaluation.")

# Read in yaml file
path_config =  '/Users/guylitt/git/fsds/scripts/eval_ingest/xssa/xssa_attr_config.yaml' 

# attribute data of interest:
vars = None # TODO allow user to define basin attributes of interest, otherwise default None and all vars will be used
if vars == None:
    attrs_sel = 'all'
# Attribute data location:
with open(path_config, 'r') as file:
    attr_config = yaml.safe_load(file)

# TODO create a model configuration file
algo_config = {'rf': {'n_estimators':100},
                'mlp': {'hidden_layer_sizes' :(4,),
                            'activation':'relu',
                            'solver':'lbfgs',
                            'alpha':0.001,
                            'batch_size':'auto',
                            'learning_rate':'constant',
                            'power_t':0.5,
                            'max_iter':20000,
                            }}


home_dir = str(Path.home())
dir_base = list([x for x in attr_config['file_io'] if 'dir_base' in x][0].values())[0].format(home_dir=home_dir)
# Location of attributes (predictor data):
dir_db_attrs = list([x for x in attr_config['file_io'] if 'dir_db_attrs' in x][0].values())[0].format(dir_base = dir_base)

# parent location of response variable data:
dir_std_base =  list([x for x in attr_config['file_io'] if 'dir_std_base' in x][0].values())[0].format(dir_base = dir_base)



# Generate standardized output directories
dir_out = fsate.fs_save_algo_dir_struct(dir_base).get('dir_out')
dir_out_alg_base = fsate.fs_save_algo_dir_struct(dir_base).get('dir_out_alg_base')

config_paths = {'dir_std_base': dir_std_base,}




# Identify datasets of interest:
datasets = list([x for x in attr_config['formulation_metadata'] if 'datasets' in x][0].values())[0]

for ds in datasets: 
    print(f'PROCESSING {ds} dataset inside \n {dir_std_base}')

    dir_out_alg_ds = Path(dir_out_alg_base/Path(ds))
    dir_out_alg_ds.mkdir(exist_ok=True)

    # Read in the standardized dataset generated by fsds_proc
    dat_resp = fsate._open_response_data_fsds(dir_std_base,ds)

    # The metrics approach
    metrics = dat_resp.attrs['metric_mappings'].split('|')

    # %% COMID retrieval and assignment to response variable's coordinate
    [featureSource,featureID] = fsate._find_feat_srce_id(dat_resp,attr_config) # e.g. ['nwissite','USGS-{gage_id}']
    comids_resp = fsate.fs_retr_nhdp_comids(featureSource,featureID,gage_ids=dat_resp['gage_id'].values)
    dat_resp = dat_resp.assign_coords(comid = comids_resp)

    # TODO allow secondary option where featureSource and featureIDs already provided, not COMID 

    #%%  Read in predictor variable data (aka basin attributes) 

    # Read the predictor variable data (basin attributes) generated by fsds.attr.hydfab
    # TODO convert attrs_sel and vars into single object
    dd_attr = fsate.fs_read_attr_comid(dir_db_attrs, comids_resp, attrs_sel = attrs_sel,
                                       _s3 = None,storage_options=None)

    if not vars: # In case the user doesn't specify variables, grab them all
        vars = dd_attr['attribute'].unique().compute() # Extracts the catchment attributes of interest
    else:
        vars = pd.Series(vars)
    # Subset dask dataframe based on variables of interest
    df_attr = dd_attr[dd_attr['attribute'].isin(vars)].compute()

    # Run a check on the attribute availability at the locations of interest
    fsate._check_attributes_exist(df_attr, vars)

    # Convert into wide format for model training
    df_attr_wide = df_attr.pivot(index='featureID', columns = 'attribute', values = 'value')

# %% Train, test, and evaluate
    rslt_eval = dict()
    for metr in metrics:
        print(f' - Processing {metr}')
        # Subset response data to metric of interest & the comid
        df_metr_resp = pd.DataFrame({'comid': dat_resp['comid'],
                                     metr : dat_resp[metr].data})
        # Join attribute data and response data
        df_pred_resp = df_metr_resp.merge(df_attr_wide, left_on = 'comid', right_on = 'featureID')

        # Instantiate the training, testing, and evaluation class
        train_eval = fsate.AlgoTrainEval(df=df_pred_resp,
                                     vars=vars,algo_config=algo_config,
                                     dir_out_alg_ds=dir_out_alg_ds, dataset_id=ds,
                                     metr=metr,test_size=0.7, rs = 32)
        train_eval.train_eval() # Train, test, eval wrapper
        
        # Retrieve evaluation metrics dataframe
        rslt_eval[metr] = train_eval.eval_df

    # Compile results and write to file
    rslt_eval_df = pd.concat(rslt_eval).reset_index(drop=True)
    rslt_eval_df['dataset'] = ds
    rslt_eval_df.to_parquet(Path(dir_out_alg_ds)/Path('algo_eval_'+ds+'.parquet'))

    print(f'... Wrote training and testing evaluation to file for {ds}')

dat_resp.close()
print("FINISHED algorithm training, testing, & evaluation")