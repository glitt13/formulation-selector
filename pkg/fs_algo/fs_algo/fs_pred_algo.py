import argparse
import yaml
import joblib
import fs_algo.fs_algo_train_eval as fsate
import pandas as pd
from pathlib import Path
import ast
import warnings
import os
import numpy as np
import forestci as fci
from sklearn.model_selection import train_test_split

# TODO create a function that's flexible/converts user formatted checks (a la fs_proc)


# Predict values and evaluate predictions
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = 'process the prediction config file')
    parser.add_argument('path_pred_config', type=str, help='Path to the YAML configuration file specific for prediction.')
    # NOTE pred_config should contain the path for path_algo_config
    args = parser.parse_args()

    path_pred_config = Path(args.path_pred_config) #Path(f'~/git/formulation-selector/scripts/eval_ingest/xssa/xssa_pred_config.yaml') 
    with open(path_pred_config, 'r') as file:
        pred_cfg = yaml.safe_load(file)
    
    mapie_alpha = pred_cfg.get('MAPIE_alpha', None)  

    #%%  READ CONTENTS FROM THE ATTRIBUTE CONFIG
    path_attr_config = fsate.build_cfig_path(path_pred_config,pred_cfg.get('name_attr_config',None))
    path_algo_config = fsate.build_cfig_path(path_pred_config,pred_cfg.get('name_algo_config',None))
    
    attr_cfig = fsate.AttrConfigAndVars(path_attr_config)
    attr_cfig._read_attr_config()

    dir_base = attr_cfig.attrs_cfg_dict.get('dir_base')
    dir_std_base = attr_cfig.attrs_cfg_dict.get('dir_std_base')
    dir_db_attrs = attr_cfig.attrs_cfg_dict.get('dir_db_attrs')
    datasets = attr_cfig.attrs_cfg_dict.get('datasets') # Identify datasets of interest

    # Grab the attributes used for training - from the attribute config file,
    #  OR a .csv file. Whatever is specified in the algo config file.
    with open(path_algo_config, 'r') as file:
        algo_cfg = yaml.safe_load(file)
    name_attr_csv = algo_cfg.get('name_attr_csv',None)
    colname_attr_csv = algo_cfg.get('colname_attr_csv',None)
    attrs_sel = fsate._id_attrs_sel_wrap(attr_cfig=attr_cfig,
                    path_cfig=path_attr_config,
                    name_attr_csv = name_attr_csv,
                    colname_attr_csv = colname_attr_csv)

    #%% ESTABLISH ALGORITHM FILE I/O
    dir_out = fsate.fs_save_algo_dir_struct(dir_base).get('dir_out')
    dir_out_alg_base = fsate.fs_save_algo_dir_struct(dir_base).get('dir_out_alg_base')
    #%% PREDICTION FILE'S COMIDS (IMPLICIT ASSUMPTION: Each dataset processes the same IDS)
    path_meta_pred = pred_cfg.get('path_meta')
    comid_pred_col = pred_cfg.get('pred_file_comid_colname')
    write_type = pred_cfg.get('write_type')
    ds_type = pred_cfg.get('ds_type')
    
    #%% prediction config
    resp_vars = pred_cfg.get('algo_response_vars')
    algos = pred_cfg.get('algo_type')



    #%% Run prediction
    for ds in datasets:

         # f-string formatting of the attribute metadata's filepath
        path_pred_locs = f'{path_meta_pred}'.format(dir_std_base=dir_std_base,ds=ds,ds_type=ds_type, write_type=write_type)

        comids_pred = fsate._read_pred_comid(path_pred_locs, comid_pred_col )

        #%%  Read in predictor variable data (aka basin attributes) 
        # Read the predictor variable data (basin attributes) generated by proc.attr.hydfab
        df_attr = fsate.fs_read_attr_comid(dir_db_attrs, comids_pred, attrs_sel = attrs_sel,
                                           read_type = 'all', # 'all' tends to be the fastest
                                        _s3 = None,storage_options=None)
        df_attr = df_attr.drop(columns='dl_timestamp')
        # Constrain the values in the value column to two digits after the decimal point (to help ID duplicates)
        df_attr['value'] = df_attr['value'].apply(lambda x: round(x, 2))

        # Drop any duplicate rows
        df_attr.drop_duplicates(inplace=True)

        # Reset the index the dataframe
        df_attr.reset_index(inplace=True)

        # Remove the old index column
        df_attr.drop(columns=['index'], inplace=True)

        new_df_attr = df_attr[['featureID', 'attribute', 'value']]
        # Convert into wide format for model training
        df_attr_wide = new_df_attr.pivot(index='featureID', columns = 'attribute', values = 'value')
        # df_attr_wide = df_attr.pivot(index='featureID', columns = 'attribute', values = 'value')

        # Run predictions & save output
        dir_out_alg_ds = Path(dir_out_alg_base/Path(ds))
        print(f"PREDICTING algorithm for {ds}")
        for metric in resp_vars:
            for algo in algos:
                path_algo = fsate.std_algo_path(dir_out_alg_ds, algo=algo, metric=metric, dataset_id=ds)
                if not Path(path_algo).exists():
                    raise FileNotFoundError(f"The following algorithm path does not exist: \n{path_algo}")

                # Read in the algorithm's pipeline
                # pipe = joblib.load(path_algo)
                pipeline_data = joblib.load(path_algo)
                
                pipe = pipeline_data['pipeline']
                X_train_shape = pipeline_data['X_train_shape']  # Retrieve X_train.shape

                rf_model = pipe.named_steps['randomforestregressor']  # Use the correct step name
                feat_names = list(pipe.feature_names_in_)
                df_attr_sub = df_attr_wide[feat_names]

                # Remove na values
                df_attr_sub_rmna = df_attr_sub.dropna()
                if df_attr_sub_rmna.shape[0] < df_attr_sub.shape[0]:

                    ids_na = set(df_attr_sub.index) - set(df_attr_sub_rmna.index)
                    msg_rm_na = f"Removing the following featureIDs from prediction due " + \
                     f"to NA values:\n{'\n'.join(ids_na)}"
                    warnings.warn(msg_rm_na)

                # Perform prediction
                resp_pred = pipe.predict(df_attr_sub_rmna)

                # Initialize DataFrame for storing results
                df_pred = pd.DataFrame({'featureID': comids_pred, 'prediction': resp_pred, 'metric': metric, 'dataset': ds, 'algo': algo, 'name_algo': Path(path_algo).name})
        
                # If using RandomForest, calculate confidence intervals using forestci
                if algo == 'rf':
                    forest_ci = fci.random_forest_error(forest=rf_model, X_train_shape=X_train_shape, X_test=df_attr_sub.to_numpy())
                    df_pred['forestci'] = forest_ci
        
                # If MAPIE is available, compute prediction intervals
                if 'mapie' in pipeline_data and mapie_alpha:
                    mapie = pipeline_data['mapie']
                    y_pred_mapie, y_pis = mapie.predict(df_attr_sub, alpha=mapie_alpha)
        
                    # Rename columns based on self.mapie_alpha values
                    for i, alpha in enumerate(mapie_alpha):
                        df_pred[f'mapie_lower_{alpha:.2f}'] = y_pis[:, 0, i]
                        df_pred[f'mapie_upper_{alpha:.2f}'] = y_pis[:, 1, i]
                
                path_pred_out = fsate.std_pred_path(dir_out,algo=algo,metric=metric,dataset_id=ds)
                # Write prediction results
                df_pred.to_parquet(path_pred_out)
                print(f"   Completed {algo} prediction of {metric}")
