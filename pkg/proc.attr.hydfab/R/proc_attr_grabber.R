# Functions to grab catchment attributes using hydrofabric and connected datasets

# Changelog / Contributions
#   2024-07-24 Originally created, GL


library(glue)
library(tidync)
library(dplyr)
library(arrow)
library(nhdplusTools)
library(hydrofabric)
library(hfsubsetR)
library(data.table)
library(pkgcond)
library(yaml)
library(future)
library(purrr)
library(tidyr)
library(tools)

attr_cfig_parse <- function(path_attr_config){
  #' @title Read and parse the attribute config yaml file to create parameter
  #' list object
  #' @param path_attr_config full path to the attribute config file
  #' @details Parses the attribute config file to generate the parameter
  #' list `Retr_Params` used throughout proc.attr.hydfab
  #' @export
  raw_config <- yaml::read_yaml(path_attr_config)

  # Define directory paths from the config file
  # Determine if home_dir. Either defined in attribute config file or assumed to be system default.
  home_dir_read <- glue::glue(base::unlist(raw_config$file_io)[['home_dir']])
  if (length(home_dir_read)==0){
    home_dir <- Sys.getenv("HOME")
  } else if (!dir.exists(home_dir_read)){
    home_dir <- Sys.getenv("HOME")
  } else {
    home_dir <- home_dir_read
  }
  dir_base <- glue::glue(base::unlist(raw_config$file_io)[['dir_base']])#file.path(home_dir,'noaa','regionalization','data')
  dir_std_base <- glue::glue(base::unlist(raw_config$file_io)[['dir_std_base']]) #file.path(dir_base,"input","user_data_std") # The location of standardized data generated by fs_proc python package
  dir_db_hydfab <- glue::glue(base::unlist(raw_config$file_io)[['dir_db_hydfab']]) # file.path(dir_base,'input','hydrofabric') # The local dir where hydrofabric data are stored to limit s3 connections
  dir_db_attrs <- glue::glue(base::unlist(raw_config$file_io)[['dir_db_attrs']])  # file.path(dir_base,'input','attributes') # The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}

  # datasets <- try(base::unlist(raw_config$formulation_metadata)[['datasets']])
  # if("try-error" %in% class(datasets)){
  #   # Consider multiple datasets:
  names_form_meta <- unlist(lapply(raw_config$formulation_metadata, function (x) names(x)))
  datasets <- raw_config$formulation_metadata[[which(names_form_meta=="datasets")]][['datasets']]
  # }
  ds_type <- try(base::unlist(raw_config$file_io)[['ds_type']])
  if('try-error' %in% base::class(ds_type) || is.null(ds_type)){
    warning('ds_type undefined in the attribute config file. It is generally
    expected to be "training" or "prediction"')
    ds_type <- '' # !!! Generally expected to be 'training' or 'prediction' !!!
  }
  write_type <- try(base::unlist(raw_config$file_io[['write_type']]))
  if('try-error' %in% base::class(write_type) || is.null(write_type)){
    write_type <- 'parquet'
  }

  # Figure out the dataset name(s) in order to generate path_meta appropriately
  path_meta <- base::unlist(raw_config$file_io)[['path_meta']] # Still needs glue substitution


  # Read s3 connection details
  s3_base <- base::unlist(raw_config$hydfab_config)[['s3_base']]#s3://lynker-spatial/tabular-resources" # s3 path containing hydrofabric-formatted attribute datasets
  s3_bucket <- base::unlist(raw_config$hydfab_config)[['s3_bucket']] #'lynker-spatial' # s3 bucket containing hydrofabric data

  # s3 path to hydroatlas data formatted for hydrofabric (may also be a local path)
  if ("s3_path_hydatl" %in% names(base::unlist(raw_config$attr_select))){
    s3_path_hydatl <- glue::glue(base::unlist(raw_config$attr_select)[['s3_path_hydatl']])  # glue::glue('{s3_base}/hydroATLAS/hydroatlas_vars.parquet')
  } else {
    s3_path_hydatl <- NULL
  }

  # Additional config options
  hf_cat_sel <-  base::unlist(raw_config$hydfab_config)[['hf_cat_sel']] #c("total","all")[1] # total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
  ext <- base::unlist(raw_config$hydfab_config)[['ext']] # 'gpkg'

  #-----------------------------------------------------
  # Variable listings:
  names_attr_sel <- base::unlist(base::lapply(raw_config$attr_select,
                                              function(x) base::names(x)))

  # Transform into single named list of lists rather than nested sublists
  idxs_vars <- base::grep("_vars", names_attr_sel)
  var_names <- names_attr_sel[idxs_vars]
  sub_attr_sel <- base::lapply(idxs_vars, function(i)
    raw_config$attr_select[[i]][[1]])
  base::names(sub_attr_sel) <- var_names

  # Subset to only those non-null variables:
  sub_attr_sel <- sub_attr_sel[base::unlist(base::lapply(sub_attr_sel,
                                                         function(x) base::any(!base::is.null(unlist(x)))))]
  var_names_sub <- names(sub_attr_sel)
  #-----------------------------------------------------

  Retr_Params <- base::list(paths = base::list(
    # Note that if a path is provided, ensure the
    # name includes 'path'. Same for directory having variable name with 'dir'
    dir_db_hydfab=dir_db_hydfab,
    dir_db_attrs=dir_db_attrs,
    s3_path_hydatl = s3_path_hydatl,
    dir_std_base = dir_std_base,
    home_dir = home_dir,
    path_meta = path_meta),
    vars = sub_attr_sel,
    datasets = datasets,
    ds_type = ds_type,
    write_type = write_type
  )
  return(Retr_Params)
}


retrieve_attr_exst <- function(comids, vars, dir_db_attrs, bucket_conn=NA){
  #' @title Grab previously-aggregated attributes from locations of interest
  #' @description Retrieves existing attribute data already stored in the
  #' dir_db_attrs directory as .parquet files & return tbl of all comids and
  #' attributes of interest.
  #' @details Only considers data already generated inside dir_db_attrs. If
  #' more data are needed, acquire attribute data acquisition using proc_attr_wrap().
  #' Runs checks on input arguments and retrieved contents, generating warnings
  #' if requested comids and/or variables were completely absent from the dataset
  #' @param comids character class. The comids of interest.
  #' @param vars character class. The attribute variables of interest.
  #' @param dir_db_attrs character class. The path where data
  #' @param bucket_conn Default NA. Placeholder in case a bucket connection is
  #' ever created
  #' @seealso [proc_attr_wrap]
  #' @export
  # Changelog/Contributions
  #  2024-07-26 Originally created, GL

  # Run checks on input args
  if(!'character' %in% base::class(comids) ){
    # Let's try unlisting and unnaming just-in-case
    comids <- comids %>% base::unlist() %>% base::unname()
    if(!'character' %in% base::class(comids) ){
      warning("comids expected to be character class. converting")
      comids <- base::as.character(comids)
    }
  }
  if(!'character' %in% base::class(vars)){
    # Let's try unlisting and unnaming just-in-case
    vars <- vars %>% base::unlist() %>% base::unname()
    if(!'character' %in% base::class(vars)){
      stop("vars expected to be character class")
    }
  }
  if(!base::dir.exists(dir_db_attrs)){
    stop(glue::glue("The attribute database path does not exist:
                      {dir_db_attrs}"))
  }
  if(!any(base::grepl(".parquet", base::list.files(dir_db_attrs)))){
    warning(glue::glue("The following path does not contain expected
                          .parquet files: {dir_db_attrs}"))
  }

  if(base::is.na(bucket_conn)){
    # Query based on COMID & variables, then retrieve data
    dat_all_attrs <- try(arrow::open_dataset(dir_db_attrs) %>%
                           dplyr::mutate(across(where(is.factor), as.character)) %>% # factors are a pain!!
                           dplyr::filter(featureID %in% !!comids) %>%
                           dplyr::filter(attribute %in% !!vars) %>%
                           dplyr::distinct() %>%
                           dplyr::collect())

    if('try-error' %in% base::class(dat_all_attrs)){
      stop(glue::glue("Could not acquire attribute data from {dir_db_attrs}"))
    }
  } else {# TODO add bucket connection here if it ever becomes a thing
    stop("Need to accommodate a different type of source here, e.g. s3")
  }

  # Run simple checks on retrieved data
  if (base::any(!comids %in% dat_all_attrs$featureID)){
    missing_comids <- comids[base::which(!comids %in% dat_all_attrs$featureID)]
    if (length(missing_comids) > 0){
      warning(base::paste0("Datasets missing the following comids: ",
                           base::paste(missing_comids,collapse=","),
                           "\nConsider running proc.attr.hydfab::proc_attr_wrap()"))
    } else {
      message("There's a logic issue on missing_comids inside retrieve_attr_exst")
    }


  }

  if (base::any(!vars %in% dat_all_attrs$attribute)){
    missing_vars <- vars[base::which(!vars %in% dat_all_attrs$attribute)]
    if(length(missing_vars) >0 ){
      warning(base::paste0("Datasets entirely missing the following vars: ",
                           base::paste(missing_vars,collapse=","),
                           "\nConsider running proc.attr.hydfab::proc_attr_wrap()"))
    } else {
      message("There's a logic issue on missing_vars inside retrieve_attr_exst")
    }

  }

  # Run check on all comid-attribute pairings by counting comid-var pairings
  sum_var_df <- dat_all_attrs %>%
    dplyr::group_by(featureID) %>%
    dplyr::summarise(dplyr::n_distinct(attribute))
  idxs_miss_vars <- base::which(sum_var_df$`n_distinct(attribute)` != length(vars))
  if(base::length(idxs_miss_vars)>0){
    warning(glue::glue("The following comids are missing desired variables:
              {paste(sum_var_df$featureID[idxs_miss_vars],collapse='\n')}
                       \nConsider running proc.attr.hydfab::proc_attr_wrap()"))
  }

  return(dat_all_attrs)
}


proc_attr_std_hfsub_name <- function(comid,custom_name='', fileext='gpkg'){
  #' @title Standardidze hydrofabric subsetter's local filename
  #' @description Internal function that ensures consistent filename
  #' @param comid the USGS common identifier, generated by nhdplusTools
  #' @param custom_name Desired custom name following 'hydrofab_'
  #' @param fileext file extension of the hydrofrabric data. Default 'gpkg'

  hfsub_fn <- base::gsub(pattern = paste0(custom_name,"__"),
                         replacement = "_",
                         base::paste0('hydrofab_',custom_name,'_',comid,'.',fileext))
  return(hfsub_fn)
}

proc_attr_hydatl <- function(hf_id, path_ha, ha_vars,
                             s3_ha='s3://lynker-spatial/tabular-resources/hydroATLAS/hydroatlas_vars.parquet'){
  #' @title Retrieve hydroatlas variables
  #' @description retrieves hydrofabric variables from s3 bucket
  #' @param hf_id character or numeric. the hydrofabric id, usually the COMID, may be vector
  #' @param path_ha character. full path to the local parquet or s3 bucket's
  #'  parquet holding the hydroatlas data as formatted for the hydrofabric.
  #' @param ha_vars list of characters. The variables of interest in the hydroatlas v1
  #' @param s3_ha character. The s3 path containing original
  #' hydroatlas-hydrofabric dataset.
  #' @export
  # Reads hydroatlas variables https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf
  #  in a form adapted to the hydrofabric

  if(base::grepl("s3",path_ha)){ # Run a check that the bucket connection works
    bucket <- try(arrow::s3_bucket(path_ha))
    if('try-error' %in% base::class(bucket)){
      stop(glue::glue("Could not connect to an s3 bucket path for hydroatlas
                      data retrieval. Reconsider the path_ha of {path_ha}"))
    }
  } else { # presumed to be local path location
    if(!file.exists(path_ha)){
      warning(glue::glue(
       "Local filepath does not exist for hydroatlas parquet file:\n{path_ha}
       \nAssigning lynker-spatial s3 path:\n{s3_ha}"))
      path_ha <- s3_ha
    }
  }

  # Ensure hf_id is numeric
  hf_id <- base::as.numeric(hf_id)

  ha <- arrow::open_dataset(path_ha) %>%
    dplyr::filter(hf_id %in% !!hf_id) %>%
    dplyr::select("hf_id", dplyr::any_of(ha_vars)) %>%
    dplyr::collect()

  return(ha)
}

proc_attr_usgs_nhd <- function(comid,usgs_vars){
  #' @title Retrieve USGS variables based on comid
  #' @param comid character or numeric class. The common identifier USGS
  #' location code for a surface water feature. May be multiple comids.
  #' @param usgs_vars list class. The standardized names of NHDplus variables.
  #' @seealso [nhdplusTools::get_characteristics_metadata]
  #' @export
  #'
  # Changelog/contributions
  #. 2024-12-20 Adapt to parallel processing and multi-comid retrieval, GL

  comid <- base::as.numeric(comid) # Ensure comid is numeric in order to run query

  # Get the s3 urls for each variable of interest
  usgs_meta <- nhdplusTools::get_characteristics_metadata() %>%
    dplyr::filter(ID %in% usgs_vars)

  # Plan for parallel processing
  future::plan(multisession)

  # Extract the variable data corresponding to the COMID in parallel
  ls_usgs_mlti <- try(future.apply::future_lapply(1:nrow(usgs_meta), function(r) {
    var_id <- usgs_meta$ID[r]
    arrow::open_dataset(usgs_meta$s3_url[r]) %>%
      dplyr::select(dplyr::all_of(c("COMID", var_id))) %>%
      dplyr::filter(COMID %in% comid) %>%
      dplyr::collect() %>%
      suppressWarnings()
  }))

  # Combine all the results
  usgs_subvars <- purrr::reduce(ls_usgs_mlti, dplyr::full_join, by = 'COMID')

  # Combining it all
  usgs_subvars <- ls_usgs_mlti %>% purrr::reduce(dplyr::full_join, by = 'COMID')

  return(usgs_subvars)
}


proc_attr_hf <- function(comid, dir_db_hydfab,custom_name="{lyrs}_",fileext = 'gpkg',
                         lyrs=c('divides','network')[2],
                         hf_cat_sel=TRUE,
                         overwrite=FALSE,
                         hf_version = NULL,
                         type = NULL,
                         domain = NULL
                         ){

  #' @title Retrieve hydrofabric data of interest based on location identifier
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Checks to see if a local dataset exists. If not, retrieve from lynker-spatial s3 bucket
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param dir_db_hydfab character class. Local directory path for storing hydrofabric data
  #' @param custom_name character class. A custom name to insert into hydrofabric file. Default \code{glue("{lyrs}_")}
  #' @param fileext character class. file extension of hydrofabric file. Default 'gpkg'
  #' @param lyrs character class. The layer name(s) of interest from hydrofabric. Default 'network'.
  #' @param hf_cat_sel boolean. TRUE for a total catchment characterization specific to a single comid, FALSE (or anything else) for all subcatchments
  #' @param overwrite boolean. Overwrite local data when pulling from hydrofabric s3 bucket? Default to FALSE.
  #' @param hf_version character class. The hydrofabric version. When NULL, defaults to same as \code{hfsubsetR::get_subset()}
  #' @param type hydrofabric type. When NULL, defaults to same as \code{hfsubsetR::get_subset()}, likely 'nextgen'
  #' @param domain hydrofabric domain. When NULL, defaults to same as \code{hfsubsetR::get_subset()}, likely 'conus'
  #' @export

  warning("proc_attr_hf DOES NOT WORK AS EXPECTED!!")

  # Build the hydfab filepath
  name_file <- proc.attr.hydfab:::proc_attr_std_hfsub_name(comid=comid,
                                   custom_name=glue::glue('{lyrs}_'),
                                   fileext=fileext)
  fp_cat <- base::file.path(dir_db_hydfab, name_file)

  # Set to the defaults in hfsubsetR if not defined.
  if(is.null(type)){
    type <- base::formals(hfsubsetR::get_subset)$type
  }
  if(is.null(hf_version)){
    hf_version <- base::formals(hfsubsetR::get_subset)$hf_version
  }
  if(is.null(domain)){
    domain <- base::formals(hfsubsetR::get_subset)$domain
  }
  if(is.null(overwrite)){
    overwrite <- base::formals(hfsubsetR::get_subset)$overwrite
  }


  if(!base::dir.exists(dir_db_hydfab)){
    warning(glue::glue("creating the following directory: {dir_db_hydfab}"))
    base::dir.create(dir_db_hydfab)
  }

  # Generate the nldi feature listing
  nldi_feat <- base::list(featureSource ="comid",
                         featureID = as.character(comid))

  # Download hydrofabric file if it doesn't exist already
  # Utilize hydrofabric subsetter for the catchment and download to local path
  pkgcond::suppress_warnings(hfsubsetR::get_subset(
                             comid = as.character(comid),
                        outfile = fp_cat,
                        lyrs = lyrs,
                        hf_version = hf_version,
                        type = type,
                        domain = domain,
                        overwrite=overwrite),pattern="exists and overwrite is FALSE")

  # Read the hydrofabric file gpkg for each layer
  hfab_ls <- list()
  if (fileext == 'gpkg') {
    # Define layers
    layers <- sf::st_layers(dsn = fp_cat)
    for (lyr in layers$name){
      hfab_ls[[lyr]] <- sf::read_sf(fp_cat,layer=lyr)
    }
  } else {
    stop("# TODO add in the type of hydrofabric file to read based on extension")
  }
  net <- hfab_ls[[lyrs]] %>%
    dplyr::select(divide_id, hf_id) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(divide_id) %>% dplyr::slice(1)

  if (hf_cat_sel==TRUE){
    # interested in the single location's aggregated catchment data
    net <- net %>% base::subset(hf_id==base::as.numeric(comid))
  }
  return(net)
}

proc_attr_exst_wrap <- function(path_attrs,vars_ls,bucket_conn=NA){
  #' @title Existing attribute data checker
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Retrieves what attribute data already exists in a data storage
  #'  path for a given comid and identifies missing attributes.
  #'  Returns list of
  #'   - dt_all: a data.table of existing comid data,
  #'   - need_vars: a list of datasource ids containing a list of variable
  #'        names that will be downloaded.

  #' @param path_attrs character. Path to attribute file data storage location
  #' @param vars_ls list. Variable names
  #' @param bucket_conn TODO add cloud conn details in case data stored in s3
  #' @seealso [proc_attr_wrap]
  #' @export
  #'
  # Changelog / Contributions
  #  2024-07-25 Originally created, GL
  #. 2024-12-23 remove comid as arg, GL

  # TODO adapt this check if stored in cloud (e.g. s3 connection checker)
  # Check that data has been created
  path_attrs_exst <- any(c(base::file.exists(path_attrs)))

  # Also make sure the directory exists:
  if(!dir.exists(base::dirname(path_attrs)) && is.na(bucket_conn)){
    dir.create(base::dirname(path_attrs))
  } # TODO adapt if stored in cloud (e.g. s3 connection checker)

  if(path_attrs_exst==TRUE){
    if(tools::file_ext(path_attrs)==""){
      # This is a directory, so list all parquet files inside it
      files_attrs <-  base::list.files(path_attrs, pattern = "parquet")
      if(length(files_attrs)==0){
        stop(glue::glue("No parquet files found inside {path_attrs}"))
      }
      # Read in all parquet files inside the directory
      paths_file_attrs <- base::file.path(path_attrs, files_attrs)
      dt_all <- arrow::open_dataset(paths_file_attrs) %>%
        data.table::as.data.table()
    } else { # Read in the parquet file(s) passed into this function
      dt_all <- arrow::open_dataset(path_attrs) %>%
        data.table::as.data.table()
    }

    need_vars <- list()
    for(var_srce in names(vars_ls)){
      # Compare/contrast what is there vs. desired
      attrs_reqd <- vars_ls[[var_srce]]
      attrs_needed <- attrs_reqd[which(!attrs_reqd %in% dt_all$attribute)]

      if(length(attrs_needed)>0){ # Only build list of variables needed
        need_vars[[var_srce]] <- attrs_needed
      }
    }
  } else {
    # No variable subsetting required. Grab them all for this comid
    need_vars <- vars_ls
    dt_all <- data.table::data.table() # to be populated.
  }
  return(list(dt_all=dt_all,need_vars=need_vars))
}


std_attr_data_fmt <- function(attr_data){
  #' @title Standardize the catchment attribute data to read/write in parquet files
  #' @param attr_data list of data.frame of attribute data
  #' @seealso [retr_attr_new]
  #' @export
  # Changelog/Contributions
  #. 2024-12-23 Originally created, GL
  # Ensure consistent format of dataset
  attr_data_ls <- list()
  for(dat_srce in base::names(attr_data)){
    sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
    if(base::nrow(sub_dt_dat)==0){
      warning(glue::glue("Unexpected missing data with {dat_srce}"))
      next()
    } else {
      # Even though COMID always expected, use featureSource and featureID for
      #.  full compatibility with potential custom datasets
      sub_dt_dat$featureID <- base::as.character(sub_dt_dat$COMID)
      sub_dt_dat$featureSource <- "COMID"
      sub_dt_dat$data_source <- base::as.character(dat_srce)
      sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
        base::format(Sys.time()),tz="UTC"))
      sub_dt_dat <- sub_dt_dat %>% dplyr::select(-COMID)
      # Convert from wide to long format, convert factors to char
      attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
           id.vars = c('featureID','featureSource','data_source','dl_timestamp'),
           variable.name = 'attribute') %>% dplyr::arrange(featureID) %>%
           dplyr::mutate(dplyr::across(dplyr::where(is.factor), as.character))
    }
  }
  return(attr_data_ls)
}

retr_attr_new <- function(comids,need_vars,Retr_Params){
  #' @title Retrieve new attributes that haven't been acquired yet
  #' @param comids The list of of the comid identifier
  #' @param need_vars The needed attributes that haven't been acquired yet
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @seealso [proc_attr_wrap]
  #' @seealso [proc_attr_mlti_wrap]
  #' @export
  # -------------------------------------------------------------------------- #
  # --------------- dataset grabber ---------------- #
  attr_data <- list()

  # --------------- Hydroatlas version 1 ---------------
  if (('ha_vars' %in% base::names(need_vars)) &&
      (base::all(!base::is.na(need_vars$ha_vars)))){
    # Hydroatlas variable query; list name formatted as {dataset_name}__v{ver_num}
    attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(
        path_ha=Retr_Params$paths$s3_path_hydatl,
        hf_id=comids,
        ha_vars=need_vars$ha_vars) %>%
      # ensures 'COMID' exists as colname
      dplyr::rename("COMID" = "hf_id")
  }

  # --------------- USGS NHD Plus attributes ---------------
  if( (base::any(base::grepl("usgs_vars", base::names(need_vars)))) &&
      (base::all(!base::is.na(need_vars$usgs_vars))) ){
    # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{ver_number}
    attr_data[['usgs_nhdplus__v2']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=comids,
                                                                            usgs_vars=need_vars$usgs_vars)
  }

  ########## May add more data sources here and append to attr_data ###########

  # ----------- dataset standardization ------------ #
  if (!base::all(base::unlist(( # A qa/qc check
    base::lapply(attr_data, function(x)
      base::any(base::grepl("COMID", base::colnames(x)))))))){
    stop("Expecting 'COMID' as a column name identifier in every dataset")
  }

  # Convert from wide to long format
  attr_data <- proc.attr.hydfab::std_attr_data_fmt(attr_data)

  return(attr_data)
}

std_path_attrs <- function(comid, dir_db_attrs){
  #' @title standardized path to attribute parquet file
  #' @param comid character. USGS COMID value of interest
  #' @param dir_db_attrs character. Directory where attribute .parquet files live
  #' @seealso [proc_attr_wrap]
  #' @seealso fs_algo.fs_algo_train_eval.fs_read_attr_comid() python function
  #' that reads these files
  #' @export

  path_attrs <- base::file.path(dir_db_attrs,
                                base::paste0("comid_",comid,"_attrs.parquet"))
  return(path_attrs)
}

io_attr_dat <- function(dt_new_dat,path_attrs,
                        distinct_cols=c("featureID", "data_source",
                                        "attribute")  ){
  #' @title Write the updated basin attribute data.table
  #' @details Checks to see if data already exists. If so, read it in. Then
  #' merges new data with existing data and remove any duplicates
  #' @param dt_cmbo The standardized data.table of attributes
  #' @param path_attrs parquet filepath for attribute data
  #' @param distinct_cols The column names in dt_new_dat that must be distinct
  #' @seealso [retrieve_attr_exst] for retrieving existing attributes
  #' @seealso [std_attr_data_fmt]
  #' @seealso [std_path_attrs]
  #' @export
  # TODO consider implementing the read existing/update/write all here.

  logl_write_parq <- TRUE
  # Double-check by first reading a possible dataset
  dt_exist <- try(arrow::read_parquet(path_attrs))
  if ('try-error' %in% base::class(dt_exist)){
    dt_cmbo <- dt_new_dat
  } else if(base::nrow(dt_exist)>0 && base::nrow(dt_new_dat)>0){
      # Merge & duplicate check based on a subset of columns
      dt_cmbo <- data.table::merge.data.table(dt_exist,dt_new_dat,
                                              all=TRUE,no.dups=TRUE) %>%
                  dplyr::group_by(dplyr::across(dplyr::all_of(distinct_cols))) %>%
                  dplyr::arrange(dl_timestamp) %>%
                  dplyr::slice(1) %>% dplyr::ungroup()
  } else { # If dt_new_dat is empty, then nothing changes
    dt_cmbo <- dt_exist
    logl_write_parq <- FALSE
  }

  # Remove all factors to make arrow::open_dataset() easier to work with
  dt_cmbo <- dt_cmbo %>% dplyr::mutate(dplyr::across(
    dplyr::where(is.factor), as.character))

  # Run a data quality check - a single comid file should only contain one comid
  if (base::length(base::unique(dt_cmbo$featureID))>1){
    stop(glue::glue("PROBLEM: more than one comid destined for {path_attrs}"))
  }

  if(logl_write_parq){ # Write update to file
    try_to_write <- try(arrow::write_parquet(dt_cmbo,sink=path_attrs))
    if("try-error" %in% class(try_to_write)){
      # Try deleting the file first, then writing it.
      # We can do this because of merge.data.table(dt_exist,dt_new_dat)
      base::file.remove(path_attrs)
      arrow::write_parquet(dt_cmbo,path_attrs)
    }
  }
  return(dt_cmbo)
}


proc_attr_mlti_wrap <- function(comids, Retr_Params,lyrs="network",
                                overwrite=FALSE){
  #' @title Wrapper to retrieve variables from multiple comids when processing
  #' attributes. Returns all attribute data for all comid locations
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Identifies a comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Re-processing runs only download data that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   featureID - e.g. USGS common identifier (default)
  #'   featureSource - e.g. "COMID" (default)
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comids list of character. The common identifier USGS location codes for surface water features.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @seealso [proc_attrs_gageids]
  #' @export

  vars_ls <- Retr_Params$vars

  # ------- Retr_Params$vars format checker --------- #
  # Check requested variables for retrieval are compatible/correctly formatted:
  proc.attr.hydfab:::wrap_check_vars(vars_ls)

  # ----------- existing dataset checker ----------- #
  # Define the path to the attribute parquet file (name contains comid)
  # All the filepaths for each comid
  paths_attrs <- proc.attr.hydfab::std_path_attrs(comid=comids,
                                                  dir_db_attrs=Retr_Params$paths$dir_db_attrs)
  # The comids that are stored already (have) & those that are new (need)
  comids_attrs_have <- comids[unlist(lapply(paths_attrs, function(x) file.exists(x)))]
  comids_attrs_need <- comids[unlist(lapply(paths_attrs, function(x) !file.exists(x)))]
  # The full paths of attribute data for e/ comid that we (1) have and (2) need
  paths_attrs_have <- paths_attrs[base::unlist( # Do have these comids
    base::lapply(paths_attrs, function(x) base::file.exists(x)))]
  paths_attrs_need <-paths_attrs[base::unlist( # Don't have these comids
    base::lapply(paths_attrs, function(x) !base::file.exists(x)))]

  # From those comid locs that we do have, do we have all needed attrs?
  ls_attr_exst <- base::lapply(paths_attrs_have,
                            function(x) proc.attr.hydfab::proc_attr_exst_wrap(
                              path_attrs=x,
                              vars_ls=vars_ls,
                              bucket_conn=NA))
  base::names(ls_attr_exst) <- paths_attrs_have
  # Extract the need vars
  need_vars <- base::lapply(ls_attr_exst, function(x) x$need_vars) %>%
                          base::unique() %>% base::unlist(recursive=FALSE)
  ls_dt_exst <- base::lapply(ls_attr_exst, function(x) x$dt_all)
  dt_exst_all <- data.table::rbindlist(ls_dt_exst)
  need_vars_og <- need_vars # Create a copy in case this gets modified
  comids_all <- comids

  # -------------------------------------------------------------------------- #
  # ------------------ new attribute grab & write updater -------------------- #
  # This section retrieves attribute data that is not yet part of the database
  #. and then updates the database with the new data
  ls_attr_data <- list()
  ls_attr_data[['already_exist']] <- list('pre-exist'=dt_exst_all)
  # Acquire attributes for locations that haven't been retrieved yet
  if(base::length(comids_attrs_need)>0 )  {
    # We'll need all variables for these new locations that don't have data
    # Grab all the attribute data for these comids that don't exist yet
    ls_attr_data[['new_comid']] <- proc.attr.hydfab::retr_attr_new(
                                          comids=comids_attrs_need,
                                          need_vars=Retr_Params$vars,
                                          Retr_Params=Retr_Params)
    # Compile all locations into a single datatable
    dt_new_dat <- data.table::rbindlist(ls_attr_data[['new_comid']] )

    # Write new data to file for e/ comid because we know comid has no attributes
    for(new_comid in dt_new_dat$featureID){
      sub_dt_new_loc <- dt_new_dat[dt_new_dat$featureID==new_comid,]
      path_new_comid <- proc.attr.hydfab::std_path_attrs(comid=new_comid,
                            dir_db_attrs=Retr_Params$paths$dir_db_attrs)
      # if(base::file.exists(path_new_comid)){
      #   warning(glue::glue("Problem with logic\n{path_new_comid} should not exist"))
      # }
      # ------------------- Write data to file -------------------
      dat_cmbo_comid <- proc.attr.hydfab::io_attr_dat(dt_new_dat=sub_dt_new_loc,
                                                      path_attrs=path_new_comid)
    }
  }

  # Acquire attributes that still haven't been retrieved (but some attrs exist)
  if(base::length(base::unlist(need_vars))>0){
    # retrieve the needed attributes:
    ls_attr_data[['pre-exist']] <- proc.attr.hydfab::retr_attr_new(
                                                comids=comids_attrs_have,
                                                 need_vars=need_vars,
                                                 Retr_Params=Retr_Params)

    dt_prexst_dat <- data.table::rbindlist(ls_attr_data[['pre-exist']] )
    # Write new attribute data to pre-existing comid file
    for(exst_comid in dt_prexst_dat$featureID){
      sub_dt_new_attrs <- dt_prexst_dat[dt_prexst_dat$featureID==exst_comid,]
      path_exst_comid <- proc.attr.hydfab::std_path_attrs(
                            comid=exst_comid,
                            dir_db_attrs=Retr_Params$paths$dir_db_attrs)
      # ------------------- Write data to file -------------------
      dat_cmbo_comid <- proc.attr.hydfab::io_attr_dat(
                                  dt_new_dat=sub_dt_new_attrs,
                                  path_attrs=path_exst_comid)
    }
  }
  # -------------------------------------------------------------------------- #
  # Compile all requested data of interest (e.g. to use for training/testing)
  # Merge the existing data with new data
  ls_attrs <- purrr::flatten(ls_attr_data)
  dt_all <- data.table::rbindlist(ls_attrs) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.factor), as.character))

  # Check/reporting which comids could not acquire certain attributes
  # Find comid values that do not have all expected attribute values
  proc.attr.hydfab::check_miss_attrs_comid_io(dt_all=dt_all,
                attr_vars = Retr_Params$vars,
                dir_db_attrs <- Retr_Params$paths$dir_db_attrs)
  return(dt_all)
}

check_miss_attrs_comid_io <- function(dt_all, attr_vars, dir_db_attrs){
  #' @title Find comid values that do not have all expected attribute values
  #' @details Writes to file the missing comid-attribute pairings after
  #' first updating the existing known missing data
  #' @param dt_all Dataframe/datatable of all locations and attributes
  #' @param attr_vars List of the data source and expected attributes
  #' (e.g. list('usgs_vars' = c("TOT_BFI","TOT_TWI")) from Retr_Params$vars)
  #' @param dir_db_attrs Directory where attribute data are stored.
  #' @seealso [proc_attr_mlti_wrap]
  #' @seealso [retr_attr_new]
  #' @export

  # The standard path for recording missing attributes
  path_miss_attrs <- file.path(dir_db_attrs,'missing_data',"missing_attrs_locs.csv")
  base::dir.create(base::dirname(path_miss_attrs),
                   showWarnings=FALSE,recursive=FALSE)
  # Run check
  exp_attrs <- base::unique(base::unlist(base::unname(attr_vars)))
  df_miss_attrs_nest <- dt_all %>% dplyr::group_by(featureID) %>%
    dplyr::summarize(attribute = base::list(base::setdiff(exp_attrs,
                              base::unique(attribute)))) %>%
    dplyr::filter(base::lengths(attribute) > 0)
  # Convert to long format & add timestamp:
  df_miss_attrs <- df_miss_attrs_nest  %>% tidyr::unnest(attribute)


  if(base::nrow(df_miss_attrs)>0){
    df_miss_attrs$dl_timestamp <- base::as.character(base::as.POSIXct(
      base::format(Sys.time()),tz="UTC"))

    # Add the data source id compatible with `proc.attr.hydfab::retr_attr_new`
    df_miss_attrs$data_source_type <- NA
    idxs_in <- list()
    for(srce in base::names(attr_vars)){
      print(srce)
      idxs_in[[srce]] <- base::which(df_miss_attrs$attribute %in% attr_vars[[srce]])
      if(base::length(idxs_in)>0){
        df_miss_attrs$data_source_type[idxs_in[[srce]]] <- srce
      }
    }#Finish associated attribute source type to df (usgs_vars, ha_vars,etc)

    warn_msg <- "The following comids could not acquire some attributes: \n"

    for(n in 1:base::nrow(df_miss_attrs_nest)){
      row_msg <- paste0(df_miss_attrs_nest[n,'featureID'],": ",
                        paste0(df_miss_attrs_nest[n,'attribute'][[1]][[1]],
                               collapse="|"))
      warn_msg <- paste0(warn_msg,'\n',row_msg,'\n')
    }
    warning(warn_msg)
    # First check to see if missing dataset exists, if so - update
    if(base::file.exists(path_miss_attrs)){
      exst_data <- utils::read.csv(path_miss_attrs,stringsAsFactors = FALSE)
      exst_data$featureID <- as.character(exst_data$featureID)
      # Check for new data
      new_data <- dplyr::anti_join(df_miss_attrs, exst_data,
                                   by = c("featureID", "attribute"))
      updt_data <- dplyr::bind_rows(exst_data, new_data)
    } else{
      updt_data <- df_miss_attrs
    }
    utils::write.csv(updt_data, path_miss_attrs,row.names = FALSE)
  }
}


proc_attr_wrap <- function(comid, Retr_Params, lyrs='network',overwrite=FALSE,hfab_retr=FALSE){
  #' @title DEPRECATED. Wrapper to retrieve variables when processing attributes
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description DEPRECATED. Use [proc_attr_mlti_wrap] instead.
  #' Identifies a single comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Re-processing runs only download data that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   featureID - e.g. USGS common identifier (default)
  #'   featureSource - e.g. "COMID" (default)
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comid character. The common identifier USGS location code for a surface water feature.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @param hfab_retr boolean. Should the hydrofabric geopackage data be retrieved? Default FALSE.
  #' @seealso [proc_attrs_gageids]
  #' @seealso [proc_attr_mlti_wrap]
  #' @export

  # Changelog / Contributions
  #   2024-07-25 Originally created, GL
  message(base::paste0("Processing COMID ",comid))

  if(hfab_retr){ # Retreive the hydrofabric data, downloading to dir_db_hydfab
    # Retrieve the hydrofabric id
    net <- try(proc.attr.hydfab::proc_attr_hf(comid=comid,
                                              dir_db_hydfab=Retr_Params$paths$dir_db_hydfab,
                                              custom_name ="{lyrs}_",
                                              lyrs=Retr_Params$xtra_hfab$lyrs,
                                              hf_version = Retr_Params$xtra_hfab$hf_version,
                                              type = Retr_Params$xtra_hfab$type,
                                              domain = Retr_Params$xtra_hfab$domain,
                                              overwrite=overwrite))
    if ('try-error' %in% base::class(net)){
      warning(glue::glue("Could not acquire hydrofabric for comid {comid}. Proceeding to acquire variables of interest without hydrofabric."))
      net <- list()
      net$hf_id <- comid
    }
  } else {
    net <- list()
    net$hf_id <- comid
  }

  # Define the path to the attribute parquet file (name contains comid)
  path_attrs <- proc.attr.hydfab::std_path_attrs(comid=net$hf_id,
                                   dir_db_attrs=Retr_Params$paths$dir_db_attrs)

  vars_ls <- Retr_Params$vars
  # ------- Retr_Params$vars format checker --------- #
  # Run check on requested variables for retrieval:
  proc.attr.hydfab:::wrap_check_vars(vars_ls)

  # ----------- existing dataset checker ----------- #
  ls_chck <- proc.attr.hydfab::proc_attr_exst_wrap(comid,path_attrs,
                                                   vars_ls,bucket_conn=NA)
  dt_all <- ls_chck$dt_all
  need_vars <- ls_chck$need_vars

  # --------------- dataset grabber ---------------- #
  # attr_data <- list()
  # if (('ha_vars' %in% base::names(need_vars)) &&
  #     (base::all(!base::is.na(need_vars$ha_vars)))){
  #   # Hydroatlas variable query; list name formatted as {dataset_name}__v{version_number}
  #   attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(path_ha=Retr_Params$paths$s3_path_hydatl,
  #                                         hf_id=net$hf_id,
  #                                         ha_vars=need_vars$ha_vars) %>%
  #                               # ensures 'COMID' exists as colname
  #                               dplyr::rename("COMID" = "hf_id")
  # }
  # if( (base::any(base::grepl("usgs_vars", base::names(need_vars)))) &&
  #     (base::all(!base::is.na(need_vars$usgs_vars))) ){
  #   # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{version_number}
  #   attr_data[['usgs_nhdplus__v2']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=net$hf_id,
  #                                                               usgs_vars=need_vars$usgs_vars)
  # }
  attr_data <- proc.attr.hydfab::retr_attr_new(comids=net$hf_id,need_vars=need_vars,
                             Retr_Params=Retr_Params)

  ########## May add more data sources here and append to attr_data ###########
  # ----------- dataset standardization ------------ #
  # if (!base::all(base::unlist(( # A qa/qc check
  #         base::lapply(attr_data, function(x)
  #                 base::any(base::grepl("COMID", colnames(x)))))))){
  #   stop("Expecting 'COMID' as a column name identifier in every dataset")
  # }

  # Ensure consistent format of dataset
  # attr_data_ls <- list()
  # for(dat_srce in base::names(attr_data)){
  #   sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
  #   # Even though COMID always expected, use featureSource and featureID for
  #   #.  full compatibility with potential custom datasets
  #   sub_dt_dat$featureID <- base::as.character(sub_dt_dat$COMID)
  #   sub_dt_dat$featureSource <- "COMID"
  #   sub_dt_dat$data_source <- base::as.character(dat_srce)
  #   sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
  #     base::format(Sys.time()),tz="UTC"))
  #   sub_dt_dat <- sub_dt_dat %>% dplyr::select(-COMID)
  #   # Convert from wide to long format
  #   attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
  #                            id.vars = c('featureID','featureSource', 'data_source','dl_timestamp'),
  #                            variable.name = 'attribute')
  # }

  # Combine freshly-acquired data
  dt_new_dat <- data.table::rbindlist(attr_data)
  #dt_new_dat <- data.table::rbindlist(attr_data_ls)

  # Combined dt of existing data and newly acquired data
  if(base::dim(dt_all)[1]>0 && base::dim(dt_new_dat)[1]>0){
    dt_cmbo <- data.table::merge.data.table(dt_all,dt_new_dat,
                                            all=TRUE,no.dups=TRUE)
  } else if (base::dim(dt_new_dat)[1] >0){
    dt_cmbo <- dt_new_dat
  } else {
    dt_cmbo <- dt_all
  }
  # Remove all factors to make arrow::open_dataset() easier to work with
  dt_cmbo <- dt_cmbo %>% dplyr::mutate(across(where(is.factor), as.character))

  # Write attribute variable data specific to a comid here
  arrow::write_parquet(dt_cmbo,path_attrs)
  return(dt_cmbo)
}

std_path_map_loc_ids <- function(dir_db_attrs){
  #' @title Standardize the path of the csv file that maps NLDI IDs to comids
  #' @description Uses a sub-directory in the dir_db_attrs to place data
  #' @param dir_db_attrs The attributes database path
  dir_meta_loc <- file.path(Retr_Params$paths$dir_db_attrs,'meta_loc')
  path_meta_loc <- file.path(dir_meta_loc,"comid_featID_map.csv")
  if(!dir.exists(dir_meta_loc)){
    base::dir.create(base::dirname(path_meta_loc),showWarnings = FALSE)
  }
  return(path_meta_loc)
}

retr_comids <- function(gage_ids,featureSource,featureID,dir_db_attrs){
  #' @title Retrieve comids based on provided gage_ids and expected NLDI format
  #' @details The gage_id-comid mappings are saved to file to avoid exceeding
  #' the NLDI database connection rate limit
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The [nhdplusTools::get_nldi_feature]feature featureSource,
  #' e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for [nhdplusTools::get_nldi_feature]. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #' @export
  # ---------------- COMID RETRIEVAL ------------------- #
  # TODO create a std function that makes the path_meta_loc
  path_meta_loc <- proc.attr.hydfab:::std_path_map_loc_ids(Retr_Params$paths$dir_db_attrs)
  if(file.exists(path_meta_loc)){
    if(!base::grepl('csv',path_meta_loc)){
      stop(glue::glue("Expecting the file path to metadata to be a csv:
                      \n{path_meta_loc}"))
    }
    df_comid_featid <- utils::read.csv(path_meta_loc,colClasses = 'character')
  } else {
    df_comid_featid <- base::data.frame()
  }
  ls_featid <- base::list()
  ls_comid <- base::list()
  for (gage_id in gage_ids){ #
    if(!base::exists("gage_id")){
      stop("MUST use 'gage_id' as the object name!!! \n
        Expected when defining nldi_feat$featureID")
    }

    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html
    nldi_feat <- base::list(featureSource =featureSource,
                            featureID = as.character(glue::glue(featureID)) # This should expect {'gage_id'} as a variable!
    )
    ls_featid[[gage_id]] <- nldi_feat

    if(base::any(df_comid_featid$featureID == nldi_feat$featureID)){
      # Check the comid-featureID mapped database first

      comid <- df_comid_featid$comid[df_comid_featid$featureID == nldi_feat$featureID]
      if(base::length((comid))!=1){
        stop(glue::glue("Problem with comid database logic. Look at how many
        entries exist for comid {comid} in the comid_featID_map.csv"))
      }
    } else {
      comid <- try(nhdplusTools::discover_nhdplus_id(nldi_feature = nldi_feat))
      if('try-error' %in% base::class(comid)||length(comid)==0){
        site_feature <- try(nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat))

        if('try-error' %in% base::class(site_feature)){
          stop(glue::glue("The following nldi features didn't work. You may need to
                 revisit the configuration yaml file that processes this dataset in
                fs_proc: \n {featureSource}, and featureID={featureID}"))
        } else if (!is.null(site_feature)){
          if(!base::is.na(site_feature['comid']$comid)){
            comid <- site_feature['comid']$comid
          } else {
            message(glue::glue("Could not retrieve comid for {nldi_feat$featureID}."))
            comid <- nhdplusTools::discover_nhdplus_id(point=site_feature$geometry)
            message(glue::glue("Geospatial search found a comid value of: {comid}"))
          }
        }
      }
    }
    ls_comid[[gage_id]] <- comid
  }

  # Combine the custom mapper and write to file:
  df_featid_new <- data.frame(featureID = as.character(unname(unlist(base::lapply(ls_featid, function(x) (x$featureID))))),
                              featureSource = as.character(featureSource),
                              gage_id = as.character(base::names(ls_featid)))
  df_featid_new$comid <- as.character(unlist(base::unname(ls_comid)))
  if(base::nrow(df_comid_featid)>0){
    df_featid_cmbo <- dplyr::bind_rows(df_featid_new,df_comid_featid[,c("featureID","featureSource","gage_id","comid")]) %>%
      dplyr::distinct()
  } else {
    df_featid_cmbo <- df_featid_new %>% dplyr::distinct()
  }

  if(!dir.exists(dirname(path_meta_loc))){
    dir.create(dirname(path_meta_loc),recursive = TRUE)
  }

  utils::write.csv(x = df_featid_cmbo,file = path_meta_loc,row.names = FALSE)

  return(ls_comid)
}


proc_attr_gageids <- function(gage_ids,featureSource,featureID,Retr_Params,
                              lyrs="network",overwrite=FALSE){
  #' @title Process catchment attributes based on vector of gage ids.
  #' @description
  #' Prepares inputs for main processing step. Iterates over each location
  #' for grabbing catchment attribute data corresponding to the gage_id
  #' location. Acquires user-requested variables from multiple catchment
  #' attribute sources. Calls [proc_attr_wrap] which writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Returns a data.table of all data returned from \code{nhdplusTools::get_nldi_feature}
  #' that corresponded to the gage_ids
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The [nhdplusTools::get_nldi_feature]feature featureSource,
  #' e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for [nhdplusTools::get_nldi_feature]. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #' @param Retr_Params list. List of list structure with parameters/paths
  #' needed to acquire variables of interest. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fs_proc}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. Not used in \code{proc_attr_gageids}
  #'  }
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @export
  #  Changelog/Contributions
  #   2024-07-29 Originally created, GL

  # Path checker/maker of anything that's a directory not formatted for later glue::glue() calls
  for(dir in Retr_Params$paths){
    if(base::grepl('dir',dir)){
      if(!base::dir.exists(dir) && !base::grepl("\\{",dir)){
        message(glue::glue("Creating {dir}"))
        base::dir.create(dir)
      }
    }
  }

  # Should hydrofabric data be retrieved?
  hfab_retr <- Retr_Params$xtra_hfab$hfab_retr
  if(base::is.null(hfab_retr)){ # Use default in the proc_attr_wrap() function
    hfab_retr <- base::formals(proc.attr.hydfab::proc_attr_wrap)$hfab_retr
  }
  # Populate the comids for each gage_id
  ls_comid <- proc.attr.hydfab::retr_comids(gage_ids=gage_ids,
                          featureSource=featureSource,
                          featureID=featureID,
                          dir_db_attrs=Retr_Params$paths$dir_db_attrs)

  just_comids <- ls_comid %>% base::unname() %>% base::unlist()
  # ---------- RETRIEVE DESIRED ATTRIBUTE DATA FOR EACH LOCATION ------------- #
  dt_site_feat_retr <- proc.attr.hydfab::proc_attr_mlti_wrap(
                    comids=just_comids,Retr_Params=Retr_Params,
                    lyrs=lyrs,overwrite=overwrite)

  # Add the original gage_id back into dataset **and ensure character class!!**
  df_map_comid_gageid <- base::data.frame(featureID=as.character(just_comids),
                                          gage_id=as.character(names(ls_comid)))
  dt_site_feat_retr$featureID <- as.character(dt_site_feat_retr$featureID)
  non_dupe_dt_site_feat_retr <- dt_site_feat_retr %>% dplyr::distinct()
  dt_site_feat <- base::merge(non_dupe_dt_site_feat_retr,df_map_comid_gageid,by="featureID")

  if(any(!names(ls_comid) %in% dt_site_feat$gage_id)){
    gage_ids_missing <- base::names(ls_comid)[base::which(
        !base::names(ls_comid) %in% dt_site_feat$gage_id)]
    warning(glue::glue("The following gage_id values did not return a comid:\n
                       {paste0(gage_ids_missing,collapse=',')}"))
  }

  return(dt_site_feat)
}

read_loc_data <- function(loc_id_filepath, loc_id, fmt = 'csv'){
  #' @title Read location identifiers
  #' @description Reads directly from a csv or arrow-compatible dataset.
  #' Returns the dataset's column identifer renamed as 'gage_id' in a tibble
  #' @param loc_id_filepath csv filepath or dataset filepath/directory.
  #' @param loc_id The column name of the identifier column
  #' @param fmt The format passed to arrow::open_dataset() in the non-csv case.
  #' Default 'csv'. May also be 'parquet', 'arrow', 'feather', 'zarr', etc.
  #' @seealso [proc_attr_read_gage_ids_fs]
  #' @seealso [proc_attr_wrap]
  #' @export
  # Changelog / contributions
  #  2024-08-09 Originally created

  if (!base::is.null(loc_id_filepath)){
    # Figure out the colnames of everything in the dataset.
    cols <- arrow::open_dataset(loc_id_filepath, format = fmt) %>% base::colnames()
    # assign every col as a character string because leading zeros risk being dropped
    schema <- arrow::schema(!!!setNames(rep(list(arrow::string()), length(cols)), cols))
    # Read in dataset
    if (grepl('tsv|text|csv|txt',tools::file_ext(loc_id_filepath))){
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     col_types=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    } else {
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     schema=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    }
  } else {
    base::message(glue::glue("No location dataset defined. Reconsider designation for \n {loc_id_filepath}."))
    dat_loc <- NULL
  }
  return(dat_loc)
}

proc_attr_read_gage_ids_fs <- function(dir_dataset, ds_filenames=''){
  #' @title Read in standardized formulation-selector gage_id location identifiers
  #' @description Reads output generated using \pkg{fs_proc} python package and
  #' selects the gage_id location identifier(s) and the
  #' featureSource & featureID that correspond to the gage_id
  #' @param dir_dataset directory path to the dataset
  #' @param ds_filenames a matching string specific to dataset(s) of interest
  #' inside \code{dir_dataset}
  #' @details Returns a list of the following objects:
  #' gage_ids: array of gage_id values
  #' featureSource: The type of nhdplus feature source corresponding to gage_id
  #' featureID: The method of converting gage_id into a standardized featureSource's featureID
  #' @seealso [nhdplusTools::get_nldi_feature]
  #' @export

  # Changelog/contributions
  #  2024-07-29 Originally created, GL

  # ----  Read in a standard format filename and file type from fs_proc ---- #
  dir_ds <- base::file.path(dir_dataset)
  files_ds <- base::list.files(dir_ds)
  fns <- base::lapply(ds_filenames,
                      function(x) files_ds[base::grep(x,files_ds)]) %>% unlist()

  if (base::any(base::grepl(".nc",fns))){ # Read in a netcdf file
    fn_nc <- fns[base::grep(".nc",fns)]
    if(length(fn_nc)!=1){
      stop(glue::glue("Expected that only one netcdf file exists in dir:\n{dir_ds}"))
    }
    dat_in <- file.path(dir_dataset,fn_nc)
    nc <- ncdf4::nc_open(dat_in)

    # Grab the gage_id identifier:
    gage_ids <- nc$dim$gage_id$vals

    # Extract attributes of interest that describe what gage_id represents
    attrs <- ncdf4::ncatt_get(nc,varid=0)
    featureSource <- attrs$featureSource
    featureID <- attrs$featureID # intended to reformat gage_id into the appropriate nldi format using glue(e.g. glue('USGS-{gage_id}')
  } else {
    print(paste0("The following contents inside \n",dir_ds,
                 "\n do not match expected format:\n", paste0(fns, collapse = ", ")))
    stop("Create a different file format reader here that generates everything in the return list.")
    # TODO make this more adaptable so that it doesn't depend on running python fs_proc beforehand
    # Idea: e.g. read in user-defined gage_id data as a .csv
    # Idea: read in gage_id data inside a non-standard netcdf file, then define featureSource and featureID from a separate yaml file
  }
  return(base::list(gage_ids=gage_ids, featureSource=featureSource, featureID=featureID))
}

grab_attrs_datasets_fs_wrap <- function(Retr_Params,lyrs="network",overwrite=FALSE){
  #' @title Grab catchment attributes from processed formulation-selector input
  #' @description Wrapper function that acquires catchment attribute data from
  #' formulation-selector processed input generated via \pkg{fs_proc} package
  #' Returns list of data.table object of nhdplusTools::get_nldi_feature()
  #' for all gage_ids
  #' @param Retr_Params list of parameters built for grabbing catchment attribute data. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fs_proc}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. If 'all' is specified, then all datasets in the directory are processed.
  #'  \item \code{ds_type} character. The identifier to use in filename when writing attribute location metadata retrieved from nhdplusTools::get_nldi_feature()
  #'  }
  #' @param overwrite boolean default FALSE. Should hydrofabric data be overwritten?
  #' @param lyrs default "network" the hydrofabric layers of interest.
  #'  Only 'network' is needed for attribute grabbing.
  #' @details Runs two proc.attr.hydfab functions:
  #'  [proc_attr_read_gage_ids_fs] - retrieves the gage_ids generated by \pkg{fs_proc}
  #'  [proc_attr_gageids] - retrieves the attributes for all provided gage_ids
  #'
  #' @export
  # Changelog/contributions
  #  2024-07-30 Originally created, GL

  # 'all' an option if processing all datasets desired. Otherwise list datasets in config file
  all_ds <- base::basename(base::list.dirs(Retr_Params$paths$dir_std_base,recursive=F))
  if(base::is.null(Retr_Params$datasets)){
    datasets <- NULL
  } else if (Retr_Params$datasets[1]=='all'){ # Process all datasets inside a directory
    datasets <- all_ds
  } else { # Only process those datasets listed inside the directory
    datasets <- Retr_Params$datasets
  }

  if(base::any(!datasets %in% all_ds)){ # Run check that dataset exists
    bad_ds <- paste0(datasets[which(!datasets %in% all_ds)], collapse = ", ")
    good_ds <- paste0(all_ds, collapse = ", ")
    stop(base::paste0("The following datasets do not exist in the directory\n",
                      Retr_Params$paths$dir_std_base, "/: \n ",bad_ds,"\n",
                      "\n These options exist in that directory:\n",good_ds,
                      "\n\n Reconsider the dataset and/or directory choice."))
  }


  ls_sitefeat_all <- base::list()
  for(dataset_name in datasets){ # Looping by dataset
    message(glue::glue("--- PROCESSING {dataset_name} DATASET ---"))
    dir_dataset <- base::file.path(Retr_Params$paths$dir_std_base,dataset_name)

    # Retrieve the gage_ids, featureSource, & featureID from fs_proc standardized output
    ls_fs_std <- proc.attr.hydfab::proc_attr_read_gage_ids_fs(dir_dataset)
    # TODO add option to read in gage ids from a separate data source
    gage_ids <- ls_fs_std$gage_ids
    featureSource <- ls_fs_std$featureSource
    featureID <- ls_fs_std$featureID

    # ---------------------- Grab all needed attributes ---------------------- #
    dt_site_feat <- proc.attr.hydfab::proc_attr_gageids(gage_ids,
                                                     featureSource,
                                                     featureID,
                                                     Retr_Params,
                                                     lyrs=lyrs,
                                                     overwrite=overwrite)
    dt_site_feat$dataset_name <- dataset_name
    ls_sitefeat_all[[dataset_name]] <- dt_site_feat
  }
  # -------------------------------------------------------------------------- #
  # ------------ Grab attributes from a separate loc_id file ----------------- #
  if (!base::is.null(Retr_Params$loc_id_read$loc_id_filepath)){
    # NOTE 2024-10-25: this feature hasn't been fully developed and may be ignored
    # Generate list of identifiers
    dat_loc <- proc.attr.hydfab::read_loc_data(Retr_Params$loc_id_read$loc_id_filepath,
                                               Retr_Params$loc_id_read$gage_id,
                                               fmt = Retr_Params$loc_id_read$fmt)

    if(base::nrow(dat_loc)>0){
      # TODO bugfix this here
      loc_id <- Retr_Params$loc_id_read$loc_id
      dt_site_feat <- proc.attr.hydfab::proc_attr_gageids(gage_ids=as.array(dat_loc[['gage_id']]),
                                                           featureSource=Retr_Params$loc_id_read$featureSource_loc,
                                                           featureID=Retr_Params$loc_id_read$featureID_loc,
                                                           Retr_Params,
                                                           lyrs=lyrs,
                                                           overwrite=overwrite)
      dt_site_feat$dataset_name <- Retr_Params$loc_id_read$loc_id_filepath
    } else {
      warning("TODO: add check that user didn't provide parameter expecting to read data")
      # TODO add check that user didn't provide parameter expecting to read data
    }
    # Combine lists
    ls_sitefeat_all[[Retr_Params$loc_id_read$loc_id_filepath]] <- dt_site_feat
  }

  # -------------------------------------------------------------------------- #
  # ------------------- Write attribute metadata to file
  #
  for(ds in base::names(ls_sitefeat_all)){
    # Define the objects expected in path_meta for glue-formatting
    ds <- ds # object named ds for glue formatting e.g. nldi_feat_{ds}
    ds_type <- Retr_Params$ds_type
    dir_std_base <- Retr_Params$paths$dir_std_base
    write_type <- Retr_Params$write_type
    path_meta <- glue::glue(Retr_Params$paths$path_meta)

    bool_path_meta <- (base::is.null(path_meta)) || (base::grepl("\\{", path_meta))
    if(is.na(bool_path_meta)){ # some glue objects not defined
      objs_glue <- base::list(ds_type=ds_type,write_type=write_type,
                        dir_std_base=dir_std_base,path_meta=path_meta,
                        ds=ds)
      # Which objects that could be defined in glue are not?
      ids_need_defined <- names(objs_glue)[unlist(lapply(names(objs_glue),
                             function(x) is.null(objs_glue[[x]])))]

      stop(glue::glue("path_meta not fully defined. Be sure that Retr_Params contains
           appropriate objects, e.g. {paste0(ids_need_defined,collapse=', ')}
           for Retr_Params$paths$path_meta:\n{Retr_Params$paths$path_meta}"))
    }
    proc.attr.hydfab::write_meta_nldi_feat(dt_site_feat = ls_sitefeat_all[[ds]],
                         path_meta = path_meta)
  }

  return(ls_sitefeat_all)
}

write_meta_nldi_feat <- function(dt_site_feat, path_meta){
  #' @title Write metadata from NLDI retrieval
  #' @description
    #' A short description...
  #' @seealso [proc_attr_gageids]
  #' @param dt_site_feat data.table or data.frame of NLDI site features
  #' retrieved using nhdplusTools::get_nldi_feature() and organized
  #' using proc.attr.hydfab::proc_attr_gageids
  #' @param path_meta the filepath for writing nldi metadata. May be parquet or csv file.
  #' @export

  if(base::grepl("\\{",path_meta)){
    stop("path_meta passed into write_meta_nldi_feat still has glue formatted
         string containing '{}'. Make sure the object inside the curly brackets
         is defined before calling write_meta_nldi_feat().")
  }

  if(!base::dir.exists(base::dirname(path_meta))){
    warning(glue::glue(
      "The dataset directory is expected to exist: {base::dirname(path_meta)}. Creating it."))
    base::dir.create(base::dirname(path_meta),recursive = TRUE)
  }

  # Check to see if any sfc_POINT objects exist & remove in order to write table
  dtype_sfc_bool <- base::lapply(base::colnames(dt_site_feat),
                   function(x) base::any(base::grepl("sfc",
                                                     class(dt_site_feat[[x]]))))
  geom_cols <- base::colnames(dt_site_feat)[base::unlist(dtype_sfc_bool)]

  if (base::length(geom_cols)>0){ # Remove the sfc-formatted coordinates
    xy_df <- sf::st_coordinates(dt_site_feat[[geom_cols]])
    dt_site_feat <- dt_site_feat %>% dplyr::select(-dplyr::all_of(geom_cols))
    if(!base::any(base::grepl("X|lat|latitude",base::colnames(dt_site_feat)))){
      warning("Losing coordinates in the dataset. Consider adding them back in
              by modifying proc.attr.hydfab::write_meta_nldi_feat.")
    }
  }

  if(base::grepl('parquet',tools::file_ext(path_meta))){
    arrow::write_parquet(dt_site_feat,
                         path_meta)
  } else if(base::grepl('csv',tools::file_ext(path_meta))){
    utils::write.csv(x=dt_site_feat,
                     file=path_meta,
                     row.names = FALSE)
  } else {
    stop("File extension is not in expected format of csv or parquet")
  }
  base::message(glue::glue("Wrote nldi location metadata to {path_meta}"))
}

wrap_check_vars <- function(vars_ls){
  #' @title Internal wrapper to run checks on requested attribute variable names
  #' @param vars_ls A named list from Retr_Params$vars in the standardized format
  #' @description Given a list of variable categories, each containing vectors
  #' of variable names, check the following:
  #' 1) the variable category is a recognized category name (e.g. 'usgs_vars')
  #' 2) the variable names inside the category name are actual variable names
  #' that can be used to retrieve attributes (e.g. 'TOT_TWI' as an nhdplus attribute)

  # Get the accepted variable categories used in proc.attr.hydfab R package
  dir_pkg <- system.file("extdata",package="proc.attr.hydfab")
  cfg_attr_src <- yaml::read_yaml(base::file.path(dir_pkg,"attr_source_types.yml"))
  var_catgs <- base::lapply(cfg_attr_src,
                            function(x) base::unlist(x)[['name']]) %>%
    base::unlist() %>% base::unname()

  # Now check what var categories provided by user in the the Retr_Params$vars
  names_var_catg <- base::names(vars_ls)
  if(base::any(base::is.null(names_var_catg))){
    stop(glue::glue("Retr_Params$vars should be a sublist with sublist names ",
                    "corresponding to\n standardized names in the proc.attr.hydfab package.",
                    " These names include:\n{paste0(var_catgs,collapse='\n')}"))
  }

  # Run test that the variable name is inside
  test_bool_var_catg <- base::lapply(names_var_catg,
                                     function(x) x %in% var_catgs) %>% unlist()
  if(base::any(!test_bool_var_catg)){
    stop(glue::glue("Retr_Params$vars contains the following unrecognized ",
                    "variable category name(s): ",
                    "{paste0(names_var_catg[!test_bool_var_catg],collapse='\n')}",
                    "\nAcceptable names include:\n",
                    "{paste0(var_catgs,collapse='\n')}"
    ))
  }

  # ------------------ RUN CHECK ON INDIVIDUAL VARIABLE NAMES -------------- #
  for(var_group_name in names(vars_ls)){
    sub_vars <- vars_ls[[var_group_name]]
    proc.attr.hydfab::check_attr_selection(vars=sub_vars)
  }
}

check_attr_selection <- function(attr_cfg_path = NULL, vars = NULL, verbose = TRUE){
  #' @title Check that attributes selected by user are available
  #' @author Lauren Bolotin \email{lauren.bolotin@noaa.gov }
  #' @description Sees if the attributes requested by a user matches with the
  #'  attributes supported by the package, listed in the attribute menu.
  #'  Returns list of
  #'   - missing_vars: a list of the requested variables that were not found
  #'        in the attribute menu as specified.
  #' @param attr_cfg_path a path to a .yaml configuration file specifying which
  #'        attributes a user is requesting
  #' @param vars a list specifying which attributes a user is requesting, in lieu
  #'        of a list coming from a .yaml configuration file
  #' @export

  # Read in the menu of attributes available through formulation-selector
  dir_base <- system.file("extdata",package="proc.attr.hydfab")
  attr_menu <- base::paste0(dir_base, '/fs_attr_menu.yaml')
  attr_menu <- yaml::read_yaml(attr_menu)
  dataset_indices <- seq(1, base::length(attr_menu))

  if(!is.null(attr_cfg_path)){

    # Read in the user defined config of attributes of interest
    # attr_cfg_path <- paste0(dir_base, '/xssa_attr_config_all_vars_avail.yaml')
    attr_cfg <- yaml::read_yaml(attr_cfg_path)
    attr_cfg_sel <- attr_cfg[['attr_select']] # select the section for attributes
    attr_cfg_sel <- attr_cfg_sel[-1] # remove the s3 path to hydroatlas vars
    vars_sel <- attr_cfg_sel %>% base::unlist() %>% base::unname()

    print_query <- function(dataset_index, verbose = verbose){
      dataset <- attr_cfg_sel[[dataset_index]] %>% names()
      vars <- attr_cfg_sel[[dataset_index]] %>% unlist() %>% unname()
      if (!is.null(vars)){
        vars <- paste0(vars, collapse = ', ')
        msg <- glue::glue('Checking the ', dataset,
                          ' dataset for the following requested attributes: \n',
                          vars)
        message(msg)
      }

    }
    if(verbose){
      lapply(dataset_indices, print_query)
    }

  } else if(!is.null(vars)){
    # vars <- c("TOT_twi","TOT_PRSNOW","TOT_POPDENS90","TOT_EWT","TOT_RECHG","TOT_BFI")
    vars_sel <- vars
  } else {
    stop("Must provide attr_cfg_path or vars as arguments to check_attr_selection")
  }


  vars_menu <- NA
  # Compile the attribute menu into one list of variables
  create_menu_list <- function(dataset_index){
      dataset_vars <- attr_menu[[dataset_index]] %>% base::unlist() %>% base::names()
      vars_menu <<- c(vars_menu, dataset_vars)
    }
  lapply(dataset_indices, create_menu_list)


  # Warn the user of any requested attrs that are missing
  missing_vars <- vars_sel[base::which(!vars_sel %in% vars_menu)]
  missing_vars_list <- base::paste0(missing_vars, collapse=', ')

  # Only print a warning if the user requested unavailable attrs:
  if (base::length(missing_vars) > 0){
    # Tell the user they asked for something that's not available
    warn_msg <- glue::glue('The following attributes, as specified, were not found in the attribute menu:\n',
                 missing_vars_list, '\nPlease check spelling, capitalization, etc. and revise the *_attr_config.yaml', sep = ',')
    warning(warn_msg)
  }else{
    missing_vars <- NA
  }
  return(missing_vars)
}

hfab_config_opt <- function(hfab_config,
                            reqd_hfab=c("s3_base","s3_bucket","hf_cat_sel","source")){
  #' @title Configure hydrofabric-relevant optional params
  #' @description If an argument provided in the config file is NULL, first look
  #' for default param value from the \code{proc.attr_hydfab::proc_attr_hf}.
  #' If that is NULL, then look for default value from the
  #' \code{hfsubsetR::get_subset()} args if that param exists there. Otherwise,
  #' uses default param value in \code{proc.attr_hydfab::proc_attr_wrap}.
  #' @param hfab_config The hydrofabric-specific section from the config file, hydfab_config. list.
  #' @param reqd_hfab The non-optional item names in the hydrofabric config file
  #' @return List with default arguments populated corresponding to hfsubetR::get_subset(),
  #' @export

  # The values inside the hydrofabric configuration section from attr config file
  vals_hfab_config <- lapply(hfab_config, function(x) x[[names(x)]])
  names(vals_hfab_config) <-  base::lapply(hfab_config,
                                           function(x) base::names(x)) %>%
                base::unlist()
  # The required variables in the hydfab_config section:

  sub_hfab_config <- base::within(vals_hfab_config,base::rm(list=reqd_hfab))
  names_sub_hfab <- names(sub_hfab_config)


  xtra_cfig_hfab <- list()
  for(n in names_sub_hfab){
    x <- sub_hfab_config[[n]]
    if(base::is.null(x)){
      # Is this an argument inside proc_attr_hf()?
      bool_in_proc_attr_hf <- n %in%
        base::names(base::formals(proc.attr.hydfab::proc_attr_hf))
      # Is this an argument inside proc_attr_wrap()?
      bool_in_proc_attr_wrap <- n %in%
        base::names(base::formals(proc.attr.hydfab::proc_attr_wrap))
      # Is this an argument inside hsubsetR::get_subset()?
      bool_in_get_subset <- n %in%
        base::names(base::formals(hfsubsetR::get_subset))

      # Goal: default to value inside proc_attr_hf if default not NULL
      if (!base::is.null(base::formals(proc.attr.hydfab::proc_attr_hf)[[n]])){
        xtra_cfig_hfab[[n]] <- base::formals(proc.attr.hydfab::proc_attr_hf)[[n]]
      } else if (bool_in_get_subset) { # Otherwise use the default value in hfsubsetR::get_subset()
        def_val <- base::formals(hfsubsetR::get_subset)[[n]]
        xtra_cfig_hfab[[n]] <- def_val
      } else if (bool_in_proc_attr_wrap){ # Otherwise Use the wrapper's default value
        xtra_cfig_hfab[[n]] <- base::formals(proc.attr.hydfab::proc_attr_wrap)[[n]]
      }
    } else {
      xtra_cfig_hfab[[n]] <- x
    }
  }
  return(xtra_cfig_hfab)
}

std_miss_path <- function(dir_db_attrs){
  #' @title standardize path to file listing all missing attributes
  #' @param dir_db_attrs The directory to the attribute database
  #' @seealso `fs_algo.tfrm_attrs.std_miss_path` python package
  #' @export
  path_missing_attrs <- file.path(dir_db_attrs,"missing","needed_loc_attrs.csv")
  return(path_missing_attrs)
}

######## MISSING COMID-ATTRIBUTES ##########
fs_attrs_miss_wrap <- function(path_attr_config){
  #' @title DEPRECATED. Wrapper searching for comid-attribute data identified as
  #'  missing
  #' @details Use fs_attrs_miss_mlti_wrap instead.
  #' Given missing comid-attribute pairings previously identified
  #'  from fs_tfrm_attrs.py, and generated as a file by python function
  #'  `fs_algo.tfrm_attr.write_missing_attrs`
  #' @param path_attr_config The file path to the attribute config file
  #' @seealso `fs_algo.tfrm_attr.write_missing_attrs` python
  #' @seealso [fs_attrs_miss_mlti_wrap]
  #' @export
  # Changelog / Contributions
  #. 2024-12-31 Deprecated, GL

  # Generate the parameter list
  Retr_Params <- proc.attr.hydfab::attr_cfig_parse(path_attr_config = path_attr_config)

  path_missing_attrs <- proc.attr.hydfab::std_miss_path(Retr_Params$paths$dir_db_attrs)
  df_miss <- utils::read.csv(path_missing_attrs,header=TRUE, check.names = TRUE)#,col.names = c("X","comid"	attribute	config_file	uniq_cmbo	dl_dataset)

  bool_chck_class_comid <- df_miss[['comid']][1] %>% as.character() %>%
    as.numeric() %>% suppressWarnings() %>% is.na() # Is the comid non-numeric?
  bool_chck_if_X_col <- df_miss %>% colnames() %>% grepl("X",.) %>% any()
  bool_chck_X_loc <- df_miss %>% colnames() %>% grep("X", .) == 1

  all_tests_df_miss_fmt <- c(bool_chck_class_comid,bool_chck_if_X_col,bool_chck_X_loc)
  if(base::all(all_tests_df_miss_fmt)){
    # We know 'X' is the first colname, so it's likely that R couldn't read
    #. the indices (duplicate vals when written in python?)
    cols <- colnames(df_miss)
    # The comid column is likely labeled as 'X'
    if ('uniq_cmbo' %in% cols){
      new_cols <-  cols[!grepl("uniq_cmbo",cols)]
    } else {
      new_cols <- cols
    }

    new_cols <- new_cols[!grepl("X",new_cols)]
    sub_df_miss <- df_miss[,1:(ncol(df_miss)-1)]
    names(sub_df_miss) <- new_cols

    last_col <- cols[length(cols)]
    # and the last col (e.g. dl_dataset) may become scrambled with the 'NA' column
    if(all(is.na(sub_df_miss[last_col])) && any(is.na(colnames(sub_df_miss)))){
      idx_col_na <- which(is.na(colnames(sub_df_miss)))
      sub_df_miss[last_col] <- sub_df_miss[,idx_col_na]
      sub_df_miss[,idx_col_na] <- NULL
    }
    df_miss <- sub_df_miss
  } else if (any(grepl("index",colnames(df_miss))) && !bool_chck_class_comid &&
             !bool_chck_if_X_col){
    # Remove the index column
    df_miss['index'] <- NULL
  } else if (bool_chck_class_comid){
    stop("THERE MAY BE A FORMAT ERROR WITH THE CORRECTION. MAKE SURE LOGIC IS APPROPRIATE HERE.")
  }

  if(base::nrow(df_miss)>0){
    message("Beginning search for missing comid-attribute pairings.")
    df_miss$uniq_cmbo <- paste0(df_miss$comid,df_miss$attribute) # The unique comid-attr combo
    # Read in proc.attr.hydfab package's extdata describing attributes & data sources
    dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
    path_attr_menu <- file.path(dir_extdata, "fs_attr_menu.yaml")
    df_attr_menu <- yaml::read_yaml(path_attr_menu)

    path_attr_src_types <- file.path(dir_extdata,"attr_source_types.yml")
    df_attr_src_types <- yaml::read_yaml(path_attr_src_types)

    # Identify which attributes correspond to which datasets using the menu
    attrs <- df_miss$attribute
    df_miss$dl_dataset <- NA
    for (dl_ds in names(df_attr_menu)){
      sub_df_attr_menu <- df_attr_menu[[dl_ds]]
      sub_attrs <- names(unlist(sub_df_attr_menu))
      ls_locs_df <- base::lapply(attrs, function(a)
        base::length(base::grep(a, sub_attrs))!=0 ) |>
        base::unlist()
      idxs_this_dl_ds <- base::which(ls_locs_df==TRUE)
      if(length(idxs_this_dl_ds)>0){
        print(glue::glue("Found attributes from {dl_ds} dataset"))
        df_miss$dl_dataset[idxs_this_dl_ds] <- unlist(df_attr_src_types[[dl_ds]])[["name"]]
      } else {
        print(glue::glue("No attributes correspond to {dl_ds} dataset"))
      }
    }

    # Check to make sure all attrs identified
    if(base::any(base::is.na(df_miss$dl_dataset))){
      unk_attrs <- df_miss$attribute[which(is.na(df_miss$dl_dataset))]
      str_unk_attrs <- paste0(unk_attrs, collapse = ", ")
      warning(glue::glue("Could not identify datasets for the following attributes:
                       \n{str_unk_attrs}"))
    }

    filter_df <- df_miss
    ls_sub_dt <- list() # NOTE consider removing this object if memory issues arise
    # Attempt to retrieve missing attributes for each comid of interest
    for (comid in unique(df_miss$comid)){

      sub_df_miss <- df_miss[df_miss$comid == comid,]


      var_ls <- lapply(unique(sub_df_miss$dl_dataset),
                       function(dl_ds) sub_df_miss[sub_df_miss$dl_dataset == dl_ds,'attribute'])
      names(var_ls) <- unique(sub_df_miss$dl_dataset)

      Retr_Params$vars <- var_ls

      # Note dt_cmbo contains all data for a comid, not just the requested data!
      dt_cmbo <- proc.attr.hydfab::proc_attr_wrap(comid=comid,
                                                  Retr_Params=Retr_Params,
                                                  lyrs="network",overwrite=FALSE,
                                                  hfab_retr=FALSE)


      sub_dt_cmbo <- dt_cmbo %>% subset(attribute %in% unlist(Retr_Params$vars))
      sub_dt_cmbo$uniq_cmbo <- paste0(sub_dt_cmbo$featureID,sub_dt_cmbo$attribute)

      ls_sub_dt[[comid]] <- sub_dt_cmbo # Tracking the new data
      # TODO drop NA values?

      if(base::any(base::is.na(sub_dt_cmbo$value))){
        stop(paste0("PROBLEM: {comid} has some NA values"))
      }

      # If data successfully retrieved, remove from the missing list.
      filter_df <- filter_df[!filter_df$uniq_cmbo %in% sub_dt_cmbo$uniq_cmbo,]

    }

    if (base::nrow(filter_df)== 0){
      message("Successfully found all missing attributes!")
    } else {
      message("Some missing comid-attribute pairings still remain")
    }
    # Now update the missing comid-attribute pairing file
    write.csv(filter_df,file = path_missing_attrs,row.names = FALSE)

  } else {
    message("No missing comid-attribute pairings.")
  }
}

uniq_id_loc_attr <- function(comids,attrs){
  #' @title define the unique identifier of comid-attribute pairings
  #' @seealso [fs_attrs_miss_mlti_wrap]
  uniq_cmbo <- paste0(comids,"_",attrs)
  return(uniq_cmbo)
}

fs_attrs_miss_mlti_wrap <- function(path_attr_config){
  #' @title Wrapper searching for comid-attribute data identified as missing
  #' @details Given missing comid-attribute pairings previously identified
  #'  from fs_tfrm_attrs.py, and generated as a file by python function
  #'  `fs_algo.tfrm_attr.write_missing_attrs`
  #' @param path_attr_config The file path to the attribute config file
  #' @seealso `fs_algo.tfrm_attr.write_missing_attrs` python
  #' @seealso [fs_attrs_miss.R] Rscript that calls this wrapper
  #' @export
  # Changelog / Contributions
  #. 2024-12-31 Originally created, GL

  # Generate the parameter list
  Retr_Params <- proc.attr.hydfab::attr_cfig_parse(path_attr_config = path_attr_config)

  path_missing_attrs <- proc.attr.hydfab::std_miss_path(Retr_Params$paths$dir_db_attrs)
  df_miss <- utils::read.csv(path_missing_attrs)
  df_miss$uniq_cmbo <- proc.attr.hydfab:::uniq_id_loc_attr(df_miss$comid,df_miss$attribute)
  if(base::nrow(df_miss)>0){
    message("Beginning search for missing comid-attribute pairings.")
    # The unique comid-attr combo:
    df_miss$uniq_cmbo <- proc.attr.hydfab:::uniq_id_loc_attr(df_miss$comid,
                                                             df_miss$attribute)



    # Group by 'comid' and aggregate the sets of 'attribute' values
    grouped <- df_miss %>%
      dplyr::group_by(comid) %>%
      dplyr::summarize(attribute = list(unique(attribute))) %>%
      dplyr::ungroup()

    # Convert the lists to characters to make them hashable
    grouped <- grouped %>%
      dplyr::mutate(attribute = sapply(attribute, function(x) paste(sort(x), collapse = ",")))

    # Find which 'comid' values share the same collections of 'attribute' values
    shared_values <- grouped %>%
      dplyr::group_by(attribute) %>%
      dplyr::summarize(comid = list(comid)) %>%
      dplyr::ungroup()
    ############# Map needed attributes to names in menu #################
    # Read in proc.attr.hydfab package's extdata describing attributes & data sources
    dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
    path_attr_menu <- file.path(dir_extdata, "fs_attr_menu.yaml")
    df_attr_menu <- yaml::read_yaml(path_attr_menu)

    path_attr_src_types <- file.path(dir_extdata,"attr_source_types.yml")
    df_attr_src_types <- yaml::read_yaml(path_attr_src_types)

    # Identify which attributes correspond to which datasets using the menu
    # by looping over each unique grouping of comid-attribute pairings
    filter_df <- df_miss
    ls_have_uniq_cmbo <- list()
    for(row in 1:nrow(shared_values)){
      sub_grp <- shared_values[row,]
      comids <- sub_grp['comid'][[1]][[1]]
      attrs <- strsplit(sub_grp['attribute'][[1]],',')[[1]]
      #attrs <- df_miss$attribute
      vars_ls <- list()
      df_miss$dl_dataset <- NA
      for (dl_ds in names(df_attr_menu)){
        sub_df_attr_menu <- df_attr_menu[[dl_ds]]
        sub_attrs <- names(unlist(sub_df_attr_menu))
        ls_locs_df <- base::lapply(attrs, function(a)
          base::length(base::grep(a, sub_attrs))!=0 ) |>
          base::unlist()
        idxs_this_dl_ds <- base::which(ls_locs_df==TRUE)
        attrs_have <- attrs[idxs_this_dl_ds]

        if(length(idxs_this_dl_ds)>0){
          print(glue::glue("Found attributes from {dl_ds} dataset"))
          df_miss$dl_dataset[which(df_miss$attribute %in% attrs_have)] <-
            unlist(df_attr_src_types[[dl_ds]])[["name"]]
          vars_ls[[unlist(df_attr_src_types[[dl_ds]])[["name"]]]] <- attrs_have
        } else {
          print(glue::glue("No attributes correspond to {dl_ds} dataset"))
        }
      }

      # Check to make sure all attrs identified
      if(base::any(base::is.na(df_miss$dl_dataset))){
        unk_attrs <- df_miss$attribute[which(is.na(df_miss$dl_dataset))]
        str_unk_attrs <- paste0(unk_attrs, collapse = ", ")
        warning(glue::glue("Could not identify datasets for the following attributes:
                       \n{str_unk_attrs}"))
      }
      ############# Retrieve missing attributes #################
      # Perform retrieval using these variables that should be available
      Retr_Params$vars <- vars_ls

      # Acquire the needed variables
      message(glue::glue(
        "Retrieving {length(unlist(vars_ls))} attributes for {length(comids)} total comids.
        This may take a while."))
      dt_all <- proc.attr.hydfab::proc_attr_mlti_wrap(comids=comids,
                                            Retr_Params=Retr_Params,
                                            lyrs="network",overwrite=FALSE)

      # The unique-id key for identifying unique location-attribute combinations
      ls_have_uniq_cmbo[[row]] <- proc.attr.hydfab:::uniq_id_loc_attr(dt_all$featureID,
                                                   dt_all$attribute)


      if(base::any(base::is.na(dt_all$value))){
        idxs_na <- which(is.na(dt_all$value))
        comids_problem <- paste0(dt_all$featureID[idxs_na],collapse=', ')
        stop(base::paste0("PROBLEM: The following comids hold NA values:
                          \n{comids_problem}"))
      }
    }

    # Identify which items from the missing list may now be removed
    have_uniq_cmbo <- base::unlist(ls_have_uniq_cmbo) # Data now available
    df_still_missing <- df_miss %>%
      dplyr::filter(!uniq_cmbo %in% have_uniq_cmbo)

    if (base::nrow(df_still_missing)== 0){
      message("Successfully found all missing attributes!")
    } else {
      message("Some missing comid-attribute pairings still remain")
    }

    # Write the updated missing attributes file
    write.csv(df_still_missing,path_missing_attrs,row.names = FALSE)
  } else {
    message("No missing comid-attribute pairings.")
  }
}
