# Functions to grab catchment attributes using hydrofabric and connected datasets
# @seealso rafts_utils.R for additional package functions independent of those stored here
# Changelog / Contributions
#.   2024-07-24 Originally created, GL
#.   2025-03 Expanded and ongoing feature additions/refactoring throughout FY25
library(glue)
library(tidync)
library(dplyr)
library(arrow)
library(nhdplusTools)
library(hydrofabric)
library(hfsubsetR)
library(data.table)
library(pkgcond)
library(yaml)
library(future)
library(future.apply)
library(purrr)
library(tidyr)
library(tools)
library(curl)
library(jsonlite)
library(tibble)
library(stringr)
library(fs)




attr_cfig_parse <- function(path_attr_config){
  #' @title Read and parse the attribute config yaml file to create parameter
  #' list object
  #' @param path_attr_config full path to the attribute config file
  #' @details Parses the attribute config file to generate the parameter
  #' list `Retr_Params` used throughout proc.attr.hydfab
  #' @export
  raw_config <- yaml::read_yaml(path_attr_config)

  # Define directory paths from the config file
  # Determine if home_dir. Either defined in attribute config file or assumed to be system default.
  home_dir_read <- tryCatch({glue::glue(
    base::unlist(raw_config$file_io)[['home_dir']])},
    error = function(e) {NULL})
  if (base::is.null(home_dir_read)){
    home_dir <- Sys.getenv("HOME")
  } else if (!dir.exists(home_dir_read)){
    warning(glue::glue("The user-defined home_dir does not exist. Assigning system default."))
    home_dir <- Sys.getenv("HOME")
  } else {
    home_dir <- home_dir_read
  }
  dir_base <- glue::glue(base::unlist(raw_config$file_io)[['dir_base']])#file.path(home_dir,'noaa','regionalization','data')
  dir_std_base <- glue::glue(base::unlist(raw_config$file_io)[['dir_std_base']]) #file.path(dir_base,"input","user_data_std") # The location of standardized data generated by fs_prep python package
  dir_db_hydfab <- glue::glue(base::unlist(raw_config$file_io)[['dir_db_hydfab']]) # file.path(dir_base,'input','hydrofabric') # The local dir where hydrofabric data are stored to limit s3 connections
  dir_db_attrs <- glue::glue(base::unlist(raw_config$file_io)[['dir_db_attrs']])  # file.path(dir_base,'input','attributes') # The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}

  # datasets <- try(base::unlist(raw_config$formulation_metadata)[['datasets']])
  # if("try-error" %in% class(datasets)){
  #   # Consider multiple datasets:
  names_form_meta <- unlist(lapply(raw_config$formulation_metadata, function (x) names(x)))
  datasets <- raw_config$formulation_metadata[[which(names_form_meta=="datasets")]][['datasets']]
  # }
  ds_type <- try(base::unlist(raw_config$file_io)[['ds_type']])
  if('try-error' %in% base::class(ds_type) || is.null(ds_type)){
    warning('ds_type undefined in the attribute config file. It is generally
    expected to be "training" or "prediction"')
    ds_type <- '' # !!! Generally expected to be 'training' or 'prediction' !!!
  }
  write_type <- try(base::unlist(raw_config$file_io[['write_type']]))
  if('try-error' %in% base::class(write_type) || is.null(write_type)){
    write_type <- 'parquet'
  }

  # Figure out the dataset name(s) in order to generate path_meta appropriately
  path_meta <- base::unlist(raw_config$file_io)[['path_meta']] # Still needs glue substitution

  # Read s3 connection details
  s3_base <- base::unlist(raw_config$hydfab_config)[['s3_base']]#s3://lynker-spatial/tabular-resources" # s3 path containing hydrofabric-formatted attribute datasets
  s3_bucket <- base::unlist(raw_config$hydfab_config)[['s3_bucket']] #'lynker-spatial' # s3 bucket containing hydrofabric data

  # s3 path to hydroatlas data formatted for hydrofabric (may also be a local path)
  if ("s3_path_hydatl" %in% names(base::unlist(raw_config$attr_select))){
    s3_path_hydatl <- glue::glue(base::unlist(raw_config$attr_select)[['s3_path_hydatl']])  # glue::glue('{s3_base}/hydroATLAS/hydroatlas_vars.parquet')
  } else {
    s3_path_hydatl <- NULL
  }

  # Additional config options
  hf_cat_sel <-  base::unlist(raw_config$hydfab_config)[['hf_cat_sel']] #c("total","all")[1] # total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
  ext <- base::unlist(raw_config$hydfab_config)[['ext']] # 'gpkg'

  #-----------------------------------------------------
  # Variable listings:
  # The types of data of interest
  names_attr_sel <- base::unlist(base::lapply(raw_config$attr_select,
                                              function(x) base::names(x)))

  # Transform into single named list of lists rather than nested sublists
  idxs_vars <- base::grep("_vars", names_attr_sel)
  var_names <- names_attr_sel[idxs_vars]
  sub_attr_sel <- base::lapply(idxs_vars, function(i)
    raw_config$attr_select[[i]][[1]])
  base::names(sub_attr_sel) <- var_names

  # Subset to only those non-null variables:
  sub_attr_sel <- sub_attr_sel[base::unlist(base::lapply(sub_attr_sel,
                                                         function(x) base::any(!base::is.null(unlist(x)))))]
  var_names_sub <- base::names(sub_attr_sel)

  # Consider transformation dependencies here
  name_tform_config <- try(base::unlist(raw_config$file_io)[['name_tform_config']])
  if(!"try-error" %in% base::class(name_tform_config)){
    # Create standardized path to config file
    path_tfrm_config <- proc.attr.hydfab::build_cfig_path(path_attr_config,name_tform_config)
    # parse the transformation config file
    ls_tfrm_cfig <- proc.attr.hydfab::tform_cfig_parse(path_tfrm_config)
    # retrieve the transformation config's attributes of interest
    vars_tfrm <- base::lapply(ls_tfrm_cfig$transform_attrs,
                              function(x) x$vars) %>% base::unlist() %>%
                                  base::unique()
    # Identify which datasets correspond to these variables used in transformation
    vars_tfrm_ls <- map_attrs_to_dataset(vars_tfrm)

    # Add the transformation config's attributes to sub_attr_sel & de-dupe
    srces_integrate <- base::names(vars_tfrm_ls)[base::which(base::names(vars_tfrm_ls) %in%
                                                 base::names(sub_attr_sel))]
    for(srce in srces_integrate){
      orig_vars <- sub_attr_sel[[srce]]
      tfrm_vars <- vars_tfrm_ls[[srce]]
      sub_attr_sel[[srce]] <- base::unique(base::c(sub_attr_sel[[srce]],
                                           vars_tfrm_ls[[srce]]))
    } # Completed addition of attributes used in transformation to full attribute retrievals
  } else {
    message(glue::glue("Assuming transformations on retrieved catchment",
    "attributes are not desired.\nIf transformations are desired add ",
    "'name_tform_config' entry in the attribute config file\n{path_attr_config}"))
  }

  #-----------------------------------------------------
  Retr_Params <- base::list(paths = base::list(
    # Note that if a path is provided, ensure the
    # name includes 'path'. Same for directory having variable name with 'dir'
    dir_db_hydfab=dir_db_hydfab,
    dir_db_attrs=dir_db_attrs,
    s3_path_hydatl = s3_path_hydatl,
    dir_std_base = dir_std_base,
    home_dir = home_dir,
    path_meta = path_meta),
    vars = sub_attr_sel,
    datasets = datasets,
    ds_type = ds_type,
    write_type = write_type
  )
  return(Retr_Params)
}

tform_cfig_parse <- function(path_tfrm_config){
  #' @title Read & parse the transformation config yaml file
  #' @param path_tfrm_config Full filepath to the transformation config file
  #' @seealso \link[proc.attr.hydfab]{attr_cfig_parse}
  #' @export
  raw_cfig <- yaml::read_yaml(path_tfrm_config) %>% pkgcond::suppress_warnings()

  names_raw_lev1 <- lapply(raw_cfig, function(x) names(x)) %>% unlist()

  fio <- raw_cfig[[base::grep("file_io",names_raw_lev1)]]$file_io
  tfa <- raw_cfig[[base::grep("transform_attrs",names_raw_lev1)]]$transform_attrs
  # ------------ Parse file io section of transformation file ------------- #
  names_fio <- base::lapply(fio, function(x) names(x)) %>% base::unlist()
  ls_fio <- base::list()
  for(i in 1:base::length(names_fio)){
    name_fio <- names_fio[[i]]
    ls_fio[[name_fio]] <- fio[[i]][[name_fio]]
  }

  # ------------ Parse attributes section of transformation file ------------- #
  # The transformation names
  names_tfa <- base::lapply(tfa, function(x) names(x)) %>% unlist()
  ls_tfrm <- list()
  for(i in 1:base::length(names_tfa)){
    name_tfa <- names_tfa[i]
    tfa_sublevl1 <- tfa[[i]][[names_tfa[i]]]
    tfa_sub_catg <- base::lapply(tfa_sublevl1, function(x) names(x)) %>% unlist()

    tform_types <- tfa_sublevl1[[grep("tform_type", tfa_sub_catg)]]$tform_type
    var_desc <- tfa_sublevl1[[grep("var_desc", tfa_sub_catg)]]$var_desc
    vars <- tfa_sublevl1[[grep("vars", tfa_sub_catg)]]$vars

    ls_tfrm[[name_tfa]] <- base::list(tform_types = tform_types,
                              var_desc = var_desc,
                              vars = vars)
  }

  ls_tfrm_cfig <- base::list(file_io = ls_fio,
                             transform_attrs = ls_tfrm)

  return(ls_tfrm_cfig)
}

build_cfig_path <- function(path_known_config, path_or_name_cfig) {
  #' @title Build a standardized path for every config file used in RaFTS
  #' @param path_known_config The full filepath of a known configuration file
  #' @param path_or_name_cfig Path or name of configuration file. If only name provided, it's assumed it resides in same directory as `path_known_config`
  #' @details Ensure the 'path_known_config' exists
  #' @seealso `fs_algo.build_cfig_path` The python equivalent of this function
  #' @export
  if (!base::file.exists(path_known_config)) {
    stop(glue::glue("The provided 'known' configuration file does not exist: \n
                    {path_known_config}"))
  }

  # Determine the parent directory
  dir_parent_cfig <- base::dirname(path_known_config)

  # Check if the configuration path or name was provided
  if (!base::is.null(path_or_name_cfig)) {
    path_cfig <- base::file.path(dir_parent_cfig, path_or_name_cfig)

    # Check if the constructed path exists
    if (!base::file.exists(path_cfig)) {
      path_cfig <- base::file.path(path_or_name_cfig)
      if (!base::file.exists(path_cfig)) {
        stop(glue::glue("The following configuration file could not be found: \n
        {path_or_name_cfig}"))
      }
    }
  } else {
    warning("The configuration file may not have specified the path or file name.")
    path_cfig <- NULL
  }
  return(path_cfig)
}

map_attrs_to_dataset <- function(vars){
  #' @title Match the variable names to the data source
  #' @param vars List of variable names, possibly mixed across different data sources
  #' @export
  # Read in proc.attr.hydfab package's extdata describing attributes & data sources
  dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
  path_attr_menu <- base::file.path(dir_extdata, "fs_attr_menu.yaml")
  ls_attr_menu <- yaml::read_yaml(path_attr_menu)

  path_attr_src_types <- base::file.path(dir_extdata,"attr_source_types.yml")
  df_attr_src_types <- yaml::read_yaml(path_attr_src_types)
  srce_types <- base::lapply(df_attr_src_types, function(x) base::unlist(x)) %>%
    base::names()
  ls_internal_vars <- base::list()
  ls_internal_ds_names <- base::list()
  ctr <- 0
  for(x in df_attr_src_types){
    ctr <- ctr+1
    ls_internal_ds_names[[ctr]] <- base::lapply(x, function(j) j[['name']]) %>%
      base::unlist()
  }
  # Map the names used in the menu to the names used in the attribute config file/RaFTS code
  df_map <- data.frame(menu_name = base::unlist(srce_types),
                       rafts_name = base::unlist(ls_internal_ds_names))

  # Now figure out which variables correspond to which rafts_name
  ls_attrs_name <- base::list()
  for(i in 1:base::nrow(df_map)){
    rafts_name <- df_map$rafts_name[i]
    menu_name <- df_map$menu_name[i]
    # subset menu to this variable:
    attrs_in_menu_catg <- base::lapply(ls_attr_menu[[menu_name]],
                                       function(x) names(x)) %>% base::unlist()
    idxs_vars_in_this_catg <- base::which(vars %in% attrs_in_menu_catg)
    if(base::length(idxs_vars_in_this_catg)>0){
      ls_attrs_name[[rafts_name]] <- vars[idxs_vars_in_this_catg]
    }
  }

  # Unit test expected total number of mapped variables based on input
  if(base::length(base::unique(base::unlist(ls_attrs_name))) !=
     base::length(base::unique(vars))){
    stop("Total variables in should match total variables matched.")
  }

  return(ls_attrs_name)
}

proc_attr_std_hfsub_name <- function(comid,custom_name='', fileext='gpkg'){
  #' @title Standardidze hydrofabric subsetter's local filename
  #' @description Internal function that ensures consistent filename
  #' @param comid the USGS common identifier, generated by nhdplusTools
  #' @param custom_name Desired custom name following 'hydrofab_'
  #' @param fileext file extension of the hydrofrabric data. Default 'gpkg'

  hfsub_fn <- base::gsub(pattern = paste0(custom_name,"__"),
                         replacement = "_",
                         base::paste0('hydrofab_',custom_name,'_',comid,'.',fileext))
  return(hfsub_fn)
}

# TODO accommodate multiple id locations:
ha_vars <- c("ari_ix_sav","cly_pc_sav","snw_pc_uyr")
hf_ids <- base::c("ak-wb-15164", NA,"hi-wb-2629","hi-wb-1365","prvi-wb-752",9250320)
hf_ids <- c(1022566, 1702414)
proc_attr_hydatl_wrap <- function(hf_ids, paths_ha, ha_vars,
                                  hf_id_cols = c("hf_uid","hf_id","id")){
  #' @param hf_id_cols Possible hydrofabric id columns, *listed in priority*, meaning
  #' the `'hf_uid'` is the most important column to find, and if that is not
  #' present, move on to the `'hf_id'` and finally to `'id'`
  #' @details The hf_ids may contain 1) a COMID, corresponding to CONUS, &/or
  #' 2) a hydrofabric id for OCONUS
  # We want to make sure the hydrofabric id is a unique id
  ls_dat_ha <- list()
  for(path_ha in paths_ha){

    # TODO how do we split the hf_ids into COMIDs and non-COMIDs??
    #.  - attempt proc_attr_hydatl() for each path_ha,generate NA empties, then merge back into the appropriate order?

    colnames_ha <- arrow::open_dataset(path_ha) %>% base::colnames()
    # Identify which hydrofabric id is present in this dataset
    bool_ids <- base::lapply(
      hf_id_cols, function(i) i %in% colnames_ha) %>%
      base::unlist()
    hf_id_col <- hf_id_cols[bool_ids][1] # Use the highest priority id detected

    if (hf_id_col == "hf_uid"){ # Expected to be OCONUS
      # The uid is already created - use it
      dat_ha <- proc_attr_hydatl(hf_ids, path_ha, ha_vars,hf_id_col=hf_id_col)
        # TODO consider identifying an s3_ha path specific for this one

    } else if(hf_id_col == "hf_id"){ # Expected to be CONUS
      # The case used for the CONUS hydrofabric ids, which are actually COMIDs
      # DO NOT need to create a custom hydrofabric uid
      hf_id_num <- as.numeric(hf_ids)[6]
      dat_ha <- proc_attr_hydatl(hf_id=hf_id_num, path_ha, ha_vars,hf_id_col=hf_id_col)

    } else if(hf_id_col == "id"){
      # TODO add in the standardization hf_uid approach here
      stop("TODO: Add standardization for hf_uid")
      # TODO should we allow 'hf_id' for col_id
      # TODO

      proc.attr.hydfab::custom_hf_id(df, col_vpu = "vpu",col_id = "id")
      hf_id_col <- "hf_uid"
    } else if (base::is.na(hf_id_col)){
      stop(glue::glue("None of the expected column names present:
        {paste0(hf_id_cols,collapse=', ')}"))
    }





  }


}


proc_attr_hydatl <- function(hf_ids, path_ha, ha_vars,hf_id_col=c("hf_uid","hf_id")[2],
                             s3_ha='s3://lynker-spatial/tabular-resources/hydroATLAS/hydroatlas_vars.parquet'){
  #' @title Retrieve hydroatlas variables
  #' @description retrieves hydrofabric variables from s3 bucket or a local file
  #' @param hf_ids character or numeric, vector or atomic. The hydrofabric ids
  #' @param path_ha character. full path to the local parquet or s3 bucket's
  #'  parquet holding the hydroatlas data as formatted for the hydrofabric.
  #' @param ha_vars list of characters. The variables of interest in the hydroatlas v1
  #' @param hf_id_col Custom column name for hydrofabric id unique identifier
  #' @param s3_ha character. The s3 path containing original
  #' hydroatlas-hydrofabric dataset.
  #' @export
  #'
  #'
  # Reads hydroatlas variables https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf
  #  in a form adapted to the hydrofabric
  # Changelog / contributions
  #. 2024 Originally created, GL
  #. 2025-02-25 fix: don't force numeric hf_ids, allow custom hf_id column name
  if(base::grepl("s3",path_ha)){ # Run a check that the bucket connection works
    bucket <- try(arrow::s3_bucket(path_ha),silent=TRUE)
    if('try-error' %in% base::class(bucket)){
      stop(glue::glue("Could not connect to an s3 bucket path for hydroatlas
                      data retrieval. Reconsider the path_ha of {path_ha}"))
    }
  } else if(!file.exists(path_ha)){
      warning(glue::glue(
       "Local filepath does not exist for hydroatlas parquet file:\n{path_ha}
       \nAssigning lynker-spatial s3 path:\n{s3_ha}"))
      if(domain == 'conus'){
        path_ha <- s3_ha
      }

  } # presumed to be local path location

  # Determine whether id should be numeric or character class
  check_first_val <- arrow::open_dataset(path_ha) %>% select(hf_id_col) %>%
    head(n=1) %>% collect()
  if(base::is.numeric(check_first_val[[hf_id_col]])){
    hf_ids <- base::as.numeric(hf_ids)
  } else {
    hf_ids <- base::as.character(hf_ids)
  }

  ha <- arrow::open_dataset(path_ha) %>%
      dplyr::filter(!!dplyr::sym(hf_id_col) %in% hf_ids) %>%
      dplyr::select(hf_id_col, dplyr::all_of(ha_vars)) %>%
      dplyr::collect()

  # TODO fill in the full tibble based on the provided hf_ids
  hf_ids

  return(ha)
}

std_dir_dataset <- function(dir_std_base, ds){
  #' @title Generate the standardized dataset directory
  #' @param dir_std_base The standardized base directory
  #' @param ds The dataset name (aka dir name)
  #' @seealso \link[proc.attr.hydfab]{std_path_retr_gpkg}
  #' @export

  dir_dataset <- base::file.path(dir_std_base,ds)
  if(!base::any(dir.exists(dir_dataset))){
    mssng_ds <- dir_dataset[base::which(!base::dir.exists(dir_base))]
    stop(glue::glue("The dataset directory {mssng_ds} does not exist.
    Double check config file defining dir_std_base and dataset names"))
  }
  return(dir_dataset)
}
std_path_retr_gpkg <- function(path_fs_prep){
  #' @title Create the standardized gpkg path for coordinate data & id mapping
  #'. corresponding to the standardized input data
  #' @details The python companion function is
  #'. `fs_algo.fs_algo_train_eval._std_fs_prep_ds_companion_gpkg_path`
  #' @param path_fs_prep Path used for the standardized dataset created using
  #' [`fs_prep.proc_eval_metrics.proc_col_schema`]
  #' @seealso `fs_algo.fs_algo_train_eval._std_fs_pre[_ds_companion_gpkg_path`
  #' @seealso  \link[proc.attr.hydfab]{read_fs_retr_gpkg}
  #' @seealso \link[proc.attr.hydfab]{std_dir_dataset}
  #' @export
  path_fs_prep <- fs::path(path_fs_prep)
  sub_name_loc <- fs::path_ext_remove(path_fs_prep)
  new_name_loc <- base::paste0(fs::path_file(sub_name_loc), "_loc")
  new_name_gpkg <- fs::path_ext_set(new_name_loc, "gpkg")
  # Create the new path with the updated name
  path_gpkg_fs_prep <- fs::path(fs::path_dir(path_fs_prep), new_name_gpkg)
  return(path_gpkg_fs_prep)
}

std_path_retr_gpkg_wrap <- function(dir_std_base,ds){
  #' @title Wrapper to create the standardized geopackage path for coordinate
  #'. data and id mapping corresponding to the standardized input data from
  #' the `fs_prep` python processing steps
  #' @description Given the standardized base directory and the dataset name,
  #' generate the standardized geopackage
  #' @details These same steps happen inside \code{grab_attrs_datasets_fs_wrap}
  #' @param dir_std_base The standardized base directory
  #' @param ds The dataset name (folder inside \code{dir_std_base})
  #' @export

  dir_dataset <- proc.attr.hydfab::std_dir_dataset(dir_std_base,ds)
  # Retrieve path_dat_in from fs_prep standardized output
  fs_path <- proc.attr.hydfab::proc_attr_read_gage_ids_fs(dir_dataset)$path_dat_in
  path_save_gpkg <- proc.attr.hydfab:::std_path_retr_gpkg(fs_path)
  return(path_save_gpkg)
}


read_fs_retr_gpkg <- function(path_save_gpkg, verbose = FALSE){
  #' @title Read geopackage containing identifers and point locations
  #' @description Reads in a geopackage whose filepath is
  #'  standardized by \link[proc.attr.hydfab]{std_path_retr_gpkg}
  #'  and created by \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom_wrap}
  #' @details The renaming may be a temporary solution just-in case file
  #' originated in python fs_algo
  #' @param path_save_gpkg filepath to the geopackage
  #' @seealso \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom_wrap}
  #' @seealso \link[proc.attr.hydfab]{std_path_retr_gpkg}
  #' @seealso \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom_wrap}
  #' @export
  if(!base::file.exists(path_save_gpkg)){
    if(verbose){
      warning(glue::glue("The gpkg doesn't exist: {path_save_gpkg}"))
    }
    sf_comid_in <- NULL
  } else {
    layrs <- sf::st_layers(path_save_gpkg)
    if("outlet" %in% layrs$layer_name){
      sf_comid_in <- sf::read_sf(path_save_gpkg,layer ="outlet")
    } else {
      sf_comid_in <- sf::read_sf(path_save_gpkg)
    }

    if("geom" %in% base::colnames(sf_comid_in)){
      sf_comid_in <- sf_comid_in %>% dplyr::rename(geometry=geom)
    }
  }
  return(sf_comid_in)
}

fs_retr_nhdp_comids_geom_wrap <- function(path_save_gpkg,
                                          gage_ids,featureSource='nwissite',
                                          featureID='USGS-{gage_id}',
                                          epsg=4326){
  #' @title Read or generate a sf object from NHDplus queries for comid, and
  #'.  update file with any newly retrieved locations
  #' @description Try reading geopackage file, if it doesn't exist or is missing
  #' locations based on the gage_ids of interest, grab them & update the
  #' geopackage file
  #' @param path_save_gpkg The filepath where the geopackage file should live
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The \link[nhdplusTools]{get_nldi_feature}feature featureSource,
  #' e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for  \link[nhdplusTools]{get_nldi_feature}. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #' @param epsg The EPSG code to use for the CRS; nhdplus default is 4326
  #' @seealso \link[proc.attr.hydfab]{proc_attr_read_gage_ids_fs}
  #' @seealso \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom}
  #' @seealso `fs_algo.fs_algo_train_eval.fs_retr_nhdp_comids_geom_wrap`
  #' @export

  # Changelog/Contributions
  #. 2025-03-07 Originally created, GL

  sf_comid_in <- proc.attr.hydfab::read_fs_retr_gpkg(path_save_gpkg)

  if(!base::is.null(sf_comid_in)){ # Read in
    # TODO address what to do when gage_id not a column
    idxs_gage_ids <- base::which(gage_ids %in% sf_comid_in$gage_id)
    if(base::length(idxs_gage_ids) == base::length(gage_ids)){
      # All gage ids present, subset to the gage_ids of interest
      sf_comid <- sf_comid_in[idxs_gage_ids,] %>%
        # MUST PROVIDE GAGE_IDS in the same dimension as originally provided,
        # as expected in proc_attr_gageids
        dplyr::slice(base::match(gage_ids,gage_id)) %>%
        sf::st_as_sf(crs=epsg)
    } else { # Need comids for additional locations
      need_gids <- gage_ids[base::which(!gage_ids %in% sf_comid_in$gage_id)]

      # Grab the needed gage_ids:
      sf_comid_need <- proc.attr.hydfab::fs_retr_nhdp_comids_geom(
                                               gage_ids=need_gids,
                                               featureSource=featureSource,
                                               featureID=featureID)

      sf_cmbo <- data.table::rbindlist(base::list(sf_comid_need,
                                        sf_comid_in),use.names=TRUE,fill=TRUE)
      # Count total NA, pick least-NA rows when duplicates exist & write to file
      sf_cmbo_no_dupe <- proc.attr.hydfab::std_write_geom_map_gpkg(sf_cmbo,
                                                                   path_save_gpkg,
                                                                   epsg=epsg)
      # Reorder to original gage_ids (adding in dupes in case they were removed)
      sf_comid <- sf_cmbo_no_dupe %>%
        # MUST PROVIDE GAGE_IDS in the same dimension as originally provided,
        # as expected in proc_attr_gageids
        dplyr::slice(base::match(gage_ids,gage_id)) %>%
        sf::st_as_sf(crs=epsg)
    }
  } else { # An entirely new geopackage
    sf_comid <- proc.attr.hydfab::fs_retr_nhdp_comids_geom(gage_ids = gage_ids,
                                                featureSource=featureSource,
                                                featureID=featureID) %>%
                sf::st_as_sf(crs=epsg)
    # Write to file, DO NOT use df b/c it may not have 1:1 match with gage_ids
    # as expected in proc_attr_gageids
    sf_comid_no_dupes <- proc.attr.hydfab::std_write_geom_map_gpkg(sf_comid,
                                                          path_save_gpkg,
                                                          epsg=epsg)
  }
  return(sf_comid)
}

std_write_geom_map_gpkg <- function(sf_comid,path_save_gpkg,epsg=NULL){
  #' @title Remove duplicates and write comid-geometry mappings to file
  #' @description Removes the duplicate item corresponding to the most NA values
  #' in a row, but only for duplicated gage_id values. This is just-in-case
  #' a secondary attempt at grabbing a comid was successful.
  #' @details This can change the exact 1:1 match of gage_ids, which is expected
  #' in \link[proc.attr.hydfab]{proc_attr_gageids}. May set epsg to NULL
  #' @param sf_comid sf class data.frame of comid/gage_id/geometry mappings
  #' @param path_save_gpkg The full filepath to write .gpkg
  #' @param epsg The EPSG code to use for the CRS; Default NULL means
  #' no assignment. Note that nhdplus defaults to 4326.
  #' @seealso \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom_wrap}
  #' @seealso \link[proc.attr.hydfab]{proc_attr_gageids}
  #' @export

  # Count total NA, pick least-NA rows when duplicates exist
  sf_comid$tot_na <- base::rowSums(is.na(sf_comid))
  sf_cmbo_ordr <- sf_comid[base::order(sf_comid$gage_id,sf_comid$tot_na),]
  sf_cmbo_no_dupe <- sf_cmbo_ordr[!base::duplicated(sf_cmbo_ordr$gage_id),]

  if(!base::is.null(epsg)){ # Ensure assigned epsg
    sf_cmbo_no_dupe <- sf_cmbo_no_dupe %>% sf::st_as_sf(crs=epsg)
  }

  # Update geopackage with new data!
  sf::write_sf(sf_cmbo_no_dupe,path_save_gpkg,layer = "outlet")
  return(sf_cmbo_no_dupe)
}


fs_retr_nhdp_comids_geom <- function(gage_ids,featureSource='nwissite',
                                     featureID="USGS-{gage_id}",epsg=4326){
  #' @title Retrieve comids & point geometry based on nldi_feature identifiers
  #' @param gage_ids vector of USGS gage_ids
  #' @param featureSource The  \link[nhdplusTools]{get_nldi_feature} feature
  #' featureSource, default 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for  \link[nhdplusTools]{get_nldi_feature}. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #'  Refer to ?dataRetrieval::get_nldi_sources() for options to use with nldi_feature
  #' @param epsg The EPSG code to use for the CRS; nhdplus defaults 4326
  #' @seealso \link[proc.attr.hydfab]{proc_attr_read_gage_ids_fs}
  #' @seealso \link[proc.attr.hydfab]{fs_retr_nhdp_comids_geom_wrap}
  #' @seealso `fs_algo.fs_algo_train_eval.fs_retr_nhdp_comids_geom`
  #' @return data.table of comid and geometric point in epsg 4326
  #' @export
  # Changelog/Contributions
  #. 2025-03-07 Originally created, GL

  # Pre-allocate lists
  ls_featid <- base::lapply(1:length(gage_ids),function(x) NULL)
  ls_sitefeat <- base::lapply(1:length(gage_ids),function(x) NULL)
  for (i in 1:length(gage_ids)){ #
    gage_id <- gage_ids[[i]]
    if(!base::exists("gage_id")){
      stop("MUST use 'gage_id' as the object name!!! \n
      Expected when defining nldi_feat$featureID")
    }

    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html
    nldi_feat <- base::list(featureSource =featureSource,
                            featureID = as.character(glue::glue(featureID)))
    # NOTE: featureID string should expect {'gage_id'} as a variable!
    ls_featid[[i]] <- nldi_feat


    site_feature <- try(nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat))
    if('try-error' %in% base::class(site_feature)){
      stop(glue::glue("The following nldi features didn't work. You may need to
                 revisit the configuration yaml file that processes this dataset in
                fs_prep: \n {featureSource}, and featureID={featureID}"))
    } else if (base::is.null(site_feature)){
      if(nldi_feat$featureSource=="nwissite"){
        # Try manual api retrieval specific for nwissite (USGS-{gage_id})
        message(glue::glue("Attempting manual connection to api for {nldi_feat$featureID}"))
        url_build <- base::paste0("https://api.water.usgs.gov/nldi/linked-data/nwissite/",
                                  nldi_feat$featureID,"?f=json")
        json_file <- jsonlite::read_json(url_build)
        site_feature <- tibble::tibble(identifier=nldi_feat$featureID,
              comid=json_file$features[[1]]$properties$comid,
              geometry=sf::st_sfc(sf::st_point(
                base::c(json_file$features[[1]]$geometry$coordinates[[2]],
                  json_file$features[[1]]$geometry$coordinates[[1]])),crs=epsg)
              )
      } else {
        # Try again with discover_nhdplus_id
        warning(glue::glue("^^ Could not retrieve geometry for {nldi_feat$featureID}."))
        comid <- try(nhdplusTools::discover_nhdplus_id(point=site_feature$geometry))
        if("try-error" %in% base::class(comid)){ # Assign NA values for everything
          site_feature <- tibble::tibble(identifier=nldi_feat$featureID,comid=NA,
                                   geometry=sf::st_sfc(sf::st_point(),crs=epsg))
        } else { # Assign NA values for geometry
          site_feature <- tibble::tibble(identifier=nldi_feat$featureID,comid=comid,
                                   geometry = sf::st_sfc(sf::st_point(),crs=epsg))
        }
      }
    }
    if("sfc_LINESTRING" %in% base::class(site_feature$geometry)){
      # We want a singular point for the comid, so pick the middle point
      if (base::length(site_feature$geometry) > 1){
        stop("Unexpected format - anticipating just one row in site_feature sf/df")
      } else {
        site_feature$geometry <- sf::st_line_sample(site_feature$geometry[[1]],
                                                         sample = 0.5) %>%
                                          sf::st_cast("POINT")
      }
    }
    ls_sitefeat[[i]] <- site_feature
  }

  dt_all_geom <- data.table::rbindlist(ls_sitefeat,fill = TRUE,use.names = TRUE)
  # Rename columns
  name_lookup = base::c(featureID = 'identifier')
  dt_comid_geom <- dt_all_geom %>%
    dplyr::rename(dplyr::any_of(name_lookup),) # any_of allows situations when 'identifier' doesn't exist
  dt_comid_geom$featureSource <- featureSource
  dt_comid_geom$gage_id <- base::as.character(gage_ids)
  return(dt_comid_geom)
}


proc_attr_usgs_nhd <- function(comid,usgs_vars){
  #' @title Retrieve USGS variables based on comid
  #' @param comid character or numeric class. The common identifier USGS
  #' location code for a surface water feature. May be multiple comids.
  #' @param usgs_vars list class. The standardized names of NHDplus variables.
  #' @seealso \link[nhdplusTools]{get_characteristics_metadata}
  #' @export
  #'
  # Changelog/contributions
  #. 2024-12-20 Adapt to parallel processing and multi-comid retrieval, GL

  comid <- base::as.numeric(comid) # Ensure comid is numeric in order to run query

  # Get the s3 urls for each variable of interest
  usgs_meta <- nhdplusTools::get_characteristics_metadata() %>%
    dplyr::filter(ID %in% usgs_vars)

  # Plan for parallel processing
  future::plan(multisession)

  # Extract the variable data corresponding to the COMID in parallel
  ls_usgs_mlti <- try(future.apply::future_lapply(1:nrow(usgs_meta), function(r) {
    var_id <- usgs_meta$ID[r]
    arrow::open_dataset(usgs_meta$s3_url[r]) %>%
      dplyr::select(dplyr::all_of(c("COMID", var_id))) %>%
      dplyr::filter(COMID %in% comid) %>%
      dplyr::collect() %>%
      suppressWarnings()
  }))

  # Combine all the results
  usgs_subvars <- purrr::reduce(ls_usgs_mlti, dplyr::full_join, by = 'COMID')

  # Combining it all
  usgs_subvars <- ls_usgs_mlti %>% purrr::reduce(dplyr::full_join, by = 'COMID')

  return(usgs_subvars)
}


proc_attr_hf <- function(comid, dir_db_hydfab,custom_name="{lyrs}_",fileext = 'gpkg',
                         lyrs=c('divides','network')[2],
                         hf_cat_sel=TRUE,
                         overwrite=FALSE,
                         hf_version = NULL,
                         type = NULL,
                         domain = NULL
                         ){

  #' @title Retrieve hydrofabric data of interest based on location identifier
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Checks to see if a local dataset exists. If not, retrieve from lynker-spatial s3 bucket
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param dir_db_hydfab character class. Local directory path for storing hydrofabric data
  #' @param custom_name character class. A custom name to insert into hydrofabric file. Default \code{glue("{lyrs}_")}
  #' @param fileext character class. file extension of hydrofabric file. Default 'gpkg'
  #' @param lyrs character class. The layer name(s) of interest from hydrofabric. Default 'network'.
  #' @param hf_cat_sel boolean. TRUE for a total catchment characterization specific to a single comid, FALSE (or anything else) for all subcatchments
  #' @param overwrite boolean. Overwrite local data when pulling from hydrofabric s3 bucket? Default to FALSE.
  #' @param hf_version character class. The hydrofabric version. When NULL, defaults to same as \code{hfsubsetR::get_subset()}
  #' @param type hydrofabric type. When NULL, defaults to same as \code{hfsubsetR::get_subset()}, likely 'nextgen'
  #' @param domain hydrofabric domain. When NULL, defaults to same as \code{hfsubsetR::get_subset()}, likely 'conus'
  #' @export

  warning("proc_attr_hf DOES NOT WORK AS EXPECTED!!")

  # Build the hydfab filepath
  name_file <- proc.attr.hydfab:::proc_attr_std_hfsub_name(comid=comid,
                                   custom_name=glue::glue('{lyrs}_'),
                                   fileext=fileext)
  fp_cat <- base::file.path(dir_db_hydfab, name_file)

  # Set to the defaults in hfsubsetR if not defined.
  if(is.null(type)){
    type <- base::formals(hfsubsetR::get_subset)$type
  }
  if(is.null(hf_version)){
    hf_version <- base::formals(hfsubsetR::get_subset)$hf_version
  }
  if(is.null(domain)){
    domain <- base::formals(hfsubsetR::get_subset)$domain
  }
  if(is.null(overwrite)){
    overwrite <- base::formals(hfsubsetR::get_subset)$overwrite
  }


  if(!base::dir.exists(dir_db_hydfab)){
    warning(glue::glue("creating the following directory: {dir_db_hydfab}"))
    base::dir.create(dir_db_hydfab)
  }

  # Generate the nldi feature listing ?dataRetrieval::get_nldi_sources()
  nldi_feat <- base::list(featureSource ="COMID",
                         featureID = as.character(comid))

  # Download hydrofabric file if it doesn't exist already
  # Utilize hydrofabric subsetter for the catchment and download to local path
  pkgcond::suppress_warnings(hfsubsetR::get_subset(
                             comid = as.character(comid),
                        outfile = fp_cat,
                        lyrs = lyrs,
                        hf_version = hf_version,
                        type = type,
                        domain = domain,
                        overwrite=overwrite),pattern="exists and overwrite is FALSE")

  # Read the hydrofabric file gpkg for each layer
  hfab_ls <- proc.attr.hydfab(path_gpkg=fp_cat,layers=NULL)

  net <- hfab_ls[[lyrs]] %>%
    dplyr::select(divide_id, hf_id) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(divide_id) %>% dplyr::slice(1)

  if (hf_cat_sel==TRUE){
    # interested in the single location's aggregated catchment data
    net <- net %>% base::subset(hf_id==base::as.numeric(comid))
  }
  return(net)
}

proc_attr_exst_wrap <- function(path_attrs,vars_ls,bucket_conn=NA){
  #' @title Existing attribute data checker
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Retrieves what attribute data already exists in a data storage
  #'  path for a given comid and identifies missing attributes.
  #'  Returns list of
  #'   - dt_all: a data.table of existing comid data,
  #'   - need_vars: a list of datasource ids containing a list of variable
  #'        names that will be downloaded.

  #' @param path_attrs character. Path to attribute file data storage location
  #' @param vars_ls list. Variable names
  #' @param bucket_conn TODO add cloud conn details in case data stored in s3
  #' @seealso \link[proc.attr.hydfab]{proc_attr_wrap}
  #' @export
  #'
  # Changelog / Contributions
  #  2024-07-25 Originally created, GL
  #. 2024-12-23 remove comid as arg, GL

  # TODO adapt this check if stored in cloud (e.g. s3 connection checker)
  # Check that data has been created
  path_attrs_exst <- base::any(base::c(base::file.exists(path_attrs)))

  # Also make sure the directory exists:
  if(!dir.exists(base::dirname(path_attrs)) && is.na(bucket_conn)){
    base::dir.create(base::dirname(path_attrs))
  } # TODO adapt if stored in cloud (e.g. s3 connection checker)

  if(path_attrs_exst==TRUE){
    if(tools::file_ext(path_attrs)==""){
      # This is a directory, so list all parquet files inside it
      files_attrs <-  base::list.files(path_attrs, pattern = "parquet")
      if(length(files_attrs)==0){
        stop(glue::glue("No parquet files found inside {path_attrs}"))
      }
      # Read in all parquet files inside the directory
      paths_file_attrs <- base::file.path(path_attrs, files_attrs)
      dt_all <- arrow::open_dataset(paths_file_attrs) %>%
        data.table::as.data.table()
    } else { # Read in the parquet file(s) passed into this function
      dt_all <- arrow::open_dataset(path_attrs) %>%
        data.table::as.data.table() %>%
        base::suppressWarnings()
        # suppress the warning:
        # 'R metadata may have unsafe or invalid elements Type: "externalptr" '
    }

    need_vars <- list()
    for(var_srce in names(vars_ls)){
      # Compare/contrast what is there vs. desired
      attrs_reqd <- vars_ls[[var_srce]]
      attrs_needed <- attrs_reqd[which(!attrs_reqd %in% dt_all$attribute)]

      if(length(attrs_needed)>0){ # Only build list of variables needed
        need_vars[[var_srce]] <- attrs_needed
      }
    }
  } else {
    # No variable subsetting required. Grab them all for this comid
    need_vars <- vars_ls
    dt_all <- data.table::data.table() # to be populated.
  }
  return(list(dt_all=dt_all,need_vars=need_vars))
}


std_attr_data_fmt <- function(attr_data){
  #' @title Standardize the catchment attribute data to read/write in parquet files
  #' @param attr_data list of data.frame of attribute data
  #' @seealso \link[proc.attr.hydfab]{retr_attr_new}
  #' @export
  # Changelog/Contributions
  #. 2024-12-23 Originally created, GL
  # Ensure consistent format of dataset
  attr_data_ls <- list()
  for(dat_srce in base::names(attr_data)){
    sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
    if(base::nrow(sub_dt_dat)==0){
      warning(glue::glue("Unexpected missing data with {dat_srce}"))
      next()
    } else {
      # Even though COMID always expected, use featureSource and featureID for
      #.  full compatibility with potential custom datasets


      # TODO oconus: Should COMID always be expected now??

      sub_dt_dat$featureID <- base::as.character(sub_dt_dat$COMID)
      sub_dt_dat$featureSource <- "COMID"
      sub_dt_dat$data_source <- base::as.character(dat_srce)
      sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
        base::format(Sys.time()),tz="UTC"))
      sub_dt_dat <- sub_dt_dat %>% dplyr::select(-COMID)
      # Convert from wide to long format, convert factors to char
      attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
           id.vars = c('featureID','featureSource','data_source','dl_timestamp'),
           variable.name = 'attribute') %>% dplyr::arrange(featureID) %>%
           dplyr::mutate(dplyr::across(dplyr::where(is.factor), as.character))
    }
  }
  return(attr_data_ls)
}

retr_attr_new <- function(comids,need_vars,path_ha){
  #' @title Retrieve new attributes that haven't been acquired yet
  #' @param comids The list of of the comid identifier
  #' @param need_vars The needed attributes that haven't been acquired yet
  #' @param path_ha character, the filepath where hydroatlas data.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @seealso \link[proc.attr.hydfab]{proc_attr_wrap}
  #' @seealso \link[proc.attr.hydfab]{proc_attr_mlti_wrap}
  #' @export
  # -------------------------------------------------------------------------- #
  # --------------- dataset grabber ---------------- #
  attr_data <- list()

  # --------------- Hydroatlas version 1 ---------------
  if (('ha_vars' %in% base::names(need_vars)) &&
      (base::all(!base::is.na(need_vars$ha_vars)))){
    # Hydroatlas variable query; list name formatted as {dataset_name}__v{ver_num}
    attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(
        path_ha=path_ha,
        hf_id=comids,
        ha_vars=need_vars$ha_vars)

    # TODO handle the COMID thing
      # ensures 'COMID' exists as colname
      #dplyr::rename("COMID" = "hf_id")
    # Standardize into featureSource and featureID columns
    df_hydatl_std <- proc.attr.hydfab::std_feat_id(df=df_hydatl,
                                  name_featureSource)

    attr_data[['hydroatlas__v1']] <- df_hydatl_std

  }

  # --------------- USGS NHD Plus attributes ---------------
  if( (base::any(base::grepl("usgs_vars", base::names(need_vars)))) &&
      (base::all(!base::is.na(need_vars$usgs_vars))) ){
    # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{ver_number}
    df_nhd <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=comids,
                              usgs_vars=need_vars$usgs_vars)
    # Standardize into featureSource and featureID columns
    df_nhd_std <- proc.attr.hydfab::std_feat_id(df=df_nhd,col_featureID="COMID")
    attr_data[['usgs_nhdplus__v2']] <- df_nhd_std

  }

  ########## May add more data sources here and append to attr_data ###########

  # ----------- dataset standardization ------------ #
  # TODO mar25 CHANGE THIS!!! We don't want COMID as a column
  if (!base::all(base::unlist(( # A qa/qc check
    base::lapply(attr_data, function(x)
      base::any(base::grepl("COMID", base::colnames(x)))))))){
    stop("Expecting 'COMID' as a column name identifier in every dataset")
  }

  # Convert from wide to long format
  attr_data <- proc.attr.hydfab::std_attr_data_fmt(attr_data)

  return(attr_data)
}

std_path_attrs <- function(comid, dir_db_attrs){
  #' @title standardized path to attribute parquet file
  #' @param comid character. USGS COMID value of interest
  #' @param dir_db_attrs character. Directory where attribute .parquet files live
  #' @seealso \link[proc.attr.hydfab]{proc_attr_wrap}
  #' @seealso `fs_algo.fs_algo_train_eval.fs_read_attr_comid()` python function
  #' that reads these files
  #' @export

  path_attrs <- base::file.path(dir_db_attrs,
                                base::paste0("comid_",comid,"_attrs.parquet"))
  return(path_attrs)
}

io_attr_dat <- function(dt_new_dat,path_attrs,
                        distinct_cols=c("featureID", "data_source",
                                        "attribute")  ){
  #' @title Write the updated basin attribute data.table
  #' @details Checks to see if data already exists. If so, read it in. Then
  #' merges new data with existing data and remove any duplicates
  #' @param dt_cmbo The standardized data.table of attributes
  #' @param path_attrs parquet filepath for attribute data
  #' @param distinct_cols The column names in dt_new_dat that must be distinct
  #' @seealso \link[proc.attr.hydfab]{retrieve_attr_exst} for retrieving existing attributes
  #' @seealso \link[proc.attr.hydfab]{std_attr_data_fmt}
  #' @seealso \link[proc.attr.hydfab]{std_path_attrs}
  #' @export
  # TODO consider implementing the read existing/update/write all here.

  logl_write_parq <- TRUE
  # Double-check by first reading a possible dataset
  dt_exist <- try(arrow::read_parquet(path_attrs),silent = TRUE)
  if ('try-error' %in% base::class(dt_exist)){
    dt_cmbo <- dt_new_dat
  } else if(base::nrow(dt_exist)>0 && base::nrow(dt_new_dat)>0){
      # Merge & duplicate check based on a subset of columns
      dt_cmbo <- data.table::merge.data.table(dt_exist,dt_new_dat,
                                              all=TRUE,no.dups=TRUE) %>%
                  dplyr::group_by(dplyr::across(dplyr::all_of(distinct_cols))) %>%
                  dplyr::arrange(dl_timestamp) %>%
                  dplyr::slice(1) %>% dplyr::ungroup()
  } else { # If dt_new_dat is empty, then nothing changes
    dt_cmbo <- dt_exist
    logl_write_parq <- FALSE
  }

  # Remove all factors to make arrow::open_dataset() easier to work with
  dt_cmbo <- dt_cmbo %>% dplyr::mutate(dplyr::across(
    dplyr::where(is.factor), as.character))

  # Run a data quality check - a single comid file should only contain one comid
  if (base::length(base::unique(dt_cmbo$featureID))>1){
    stop(glue::glue("PROBLEM: more than one comid destined for {path_attrs}"))
  }

  if(logl_write_parq){ # Write update to file
    try_to_write <- try(arrow::write_parquet(dt_cmbo,sink=path_attrs),silent=TRUE)
    if("try-error" %in% class(try_to_write)){
      # Try deleting the file first, then writing it.
      # We can do this because of merge.data.table(dt_exist,dt_new_dat)
      base::file.remove(path_attrs)
      arrow::write_parquet(dt_cmbo,path_attrs)
    }
  }
  return(dt_cmbo)
}


proc_attr_mlti_wrap <- function(comids, Retr_Params,lyrs="network",
                                overwrite=FALSE){
  #' @title Wrapper to retrieve variables from multiple comids when processing
  #' attributes. Returns all attribute data for all comid locations
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Identifies a comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Re-processing runs only download data that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   featureID - e.g. USGS common identifier (default)
  #'   featureSource - e.g. "COMID" (default)
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comids list of character. The common identifier USGS location codes for surface water features.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @seealso \link[proc.attr.hydfab]{proc_attrs_gageids}
  #' @export
  #'
  # Changelog/Contributions
  # 2024 Originally Created, GL
  # 2025-05-06 reduce the missing variable grabbing to 'still_need' logic, GL
  # TODO integrate id_attrs_sel_wrap here
  vars_ls <- Retr_Params$vars

  # ------- Retr_Params$vars format checker --------- #
  # Check requested variables for retrieval are compatible/correctly formatted:
  proc.attr.hydfab:::wrap_check_vars(vars_ls)

  # ----------- existing dataset checker ----------- #
  # Define the path to the attribute parquet file (name contains comid)
  # All the filepaths for each comid
  paths_attrs <- proc.attr.hydfab::std_path_attrs(comid=comids,
                         dir_db_attrs=Retr_Params$paths$dir_db_attrs)
  # The comids that are stored already (have) & those that are new (need)
  comids_attrs_have <- comids[unlist(lapply(paths_attrs,
                                            function(x) file.exists(x)))]
  comids_attrs_need <- comids[unlist(lapply(paths_attrs[which(!is.na(comids))],
                                            function(x) !file.exists(x)))]


  # The full paths of attribute data for e/ comid that we (1) have and (2) need
  paths_attrs_have <- paths_attrs[base::unlist( # Do have these comids
    base::lapply(paths_attrs, function(x) base::file.exists(x)))]
  paths_attrs_need <-paths_attrs[base::unlist( # Don't have these comids
    base::lapply(paths_attrs, function(x) !base::file.exists(x)))]

  # From those comid locs that we do have, do we have all needed attrs?
  ls_attr_exst <- base::lapply(paths_attrs_have,
                            function(x) proc.attr.hydfab::proc_attr_exst_wrap(
                              path_attrs=x,
                              vars_ls=vars_ls,
                              bucket_conn=NA))
  base::names(ls_attr_exst) <- paths_attrs_have

  # ----- Extract the need vars
  need_vars <- base::lapply(ls_attr_exst, function(x) x$need_vars) #%>%
                          #base::unique() %>% base::unlist(recursive=FALSE)
  miss_var_types_by_file <- base::lapply(need_vars, function(x) base::names(x))
  # The indices corresponding to locations missing vars, based on need_vars
  idxs_still_need_them <- base::grep(TRUE,base::lapply(miss_var_types_by_file,
                              function(x) !base::is.null(x)) %>% base::unlist())

  ls_dt_exst <- base::lapply(ls_attr_exst, function(x) x$dt_all)
  dt_exst_all <- data.table::rbindlist(ls_dt_exst,use.names = TRUE,fill = TRUE)
  need_vars_og <- need_vars # Create a copy in case this gets modified
  comids_all <- comids

  # -------------------------------------------------------------------------- #
  # ------------------ new attribute grab & write updater -------------------- #
  # This section retrieves attribute data that is not yet part of the database
  #. and then updates the database with the new data
  ls_attr_data <- list()
  ls_attr_data[['already_exist']] <- list('pre-exist'=dt_exst_all)
  # Acquire attributes for locations that haven't been retrieved yet
  if(base::length(comids_attrs_need)>0 )  {
    # We'll need all variables for these new locations that don't have data
    # Grab all the attribute data for these comids that don't exist yet
    ls_attr_data[['new_comid']] <- proc.attr.hydfab::retr_attr_new(
                                          comids=comids_attrs_need,
                                          need_vars=Retr_Params$vars,
                                          path_ha=Retr_Params$paths$s3_path_hydatl)
    # Compile all locations into a single datatable
    dt_new_dat <- data.table::rbindlist(ls_attr_data[['new_comid']],
                                        use.names = TRUE,fill=TRUE)

    # Write new data to file for e/ comid because we know comid has no attributes
    for(new_comid in dt_new_dat$featureID){
      sub_dt_new_loc <- dt_new_dat[dt_new_dat$featureID==new_comid,]
      path_new_comid <- proc.attr.hydfab::std_path_attrs(comid=new_comid,
                            dir_db_attrs=Retr_Params$paths$dir_db_attrs)
      # if(base::file.exists(path_new_comid)){
      #   warning(glue::glue("Problem with logic\n{path_new_comid} should not exist"))
      # }
      # ------------------- Write data to file -------------------
      dat_cmbo_comid <- proc.attr.hydfab::io_attr_dat(dt_new_dat=sub_dt_new_loc,
                                                      path_attrs=path_new_comid)
    }
  }

  # Acquire attributes that still haven't been retrieved (but some attrs exist)
  if(base::length(idxs_still_need_them)>0){
    comids_attrs_still_need <- comids_attrs_have[idxs_still_need_them]
    still_need_vars <- need_vars[idxs_still_need_them]
    # retrieve the needed attributes:
    ls_attr_data[['pre-exist']] <- proc.attr.hydfab::retr_attr_new(
                                                locids=comids_attrs_still_need,
                                                 need_vars=still_need_vars,
                                                 paths_ha=Retr_Params$paths$paths_ha)

    dt_prexst_dat <- data.table::rbindlist(ls_attr_data[['pre-exist']],
                                           use.names = TRUE,fill=TRUE )

    # Write new attribute data to pre-existing comid file
    for(exst_comid in dt_prexst_dat$featureID){
      sub_dt_new_attrs <- dt_prexst_dat[dt_prexst_dat$featureID==exst_comid,]
      path_exst_comid <- proc.attr.hydfab::std_path_attrs(
                            comid=exst_comid,
                            dir_db_attrs=Retr_Params$paths$dir_db_attrs)
      # ------------------- Write data to file -------------------
      dat_cmbo_comid <- proc.attr.hydfab::io_attr_dat(
                                  dt_new_dat=sub_dt_new_attrs,
                                  path_attrs=path_exst_comid)
    }
  }
  # -------------------------------------------------------------------------- #
  # Compile all requested data of interest (e.g. to use for training/testing)
  # Merge the existing data with new data
  ls_attrs <- purrr::flatten(ls_attr_data)
  dt_all <- data.table::rbindlist(ls_attrs,use.names=TRUE,fill=TRUE) %>%
    dplyr::mutate(dplyr::across(dplyr::where(is.factor), as.character))

  # Check/reporting which comids could not acquire certain attributes
  # Find comid values that do not have all expected attribute values
  if(base::nrow(dt_all)>0){
    proc.attr.hydfab::check_miss_attrs_comid_io(dt_all=dt_all,
                              attr_vars = Retr_Params$vars,
                              dir_db_attrs <- Retr_Params$paths$dir_db_attrs)
  }

  return(dt_all)
}

check_miss_attrs_comid_io <- function(dt_all, attr_vars, dir_db_attrs){
  #' @title Find comid values that do not have all expected attribute values
  #' @details Writes to file the missing comid-attribute pairings after
  #' first updating the existing known missing data
  #' @param dt_all Dataframe/datatable of all locations and attributes
  #' @param attr_vars List of the data source and expected attributes
  #' (e.g. list('usgs_vars' = c("TOT_BFI","TOT_TWI")) from Retr_Params$vars)
  #' @param dir_db_attrs Directory where attribute data are stored.
  #' @seealso \link[proc.attr.hydfab]{proc_attr_mlti_wrap}
  #' @seealso \link[proc.attr.hydfab]{retr_attr_new}
  #' @export

  # The standard path for recording missing attributes
  path_miss_attrs <- file.path(dir_db_attrs,'missing_data',"missing_attrs_locs.csv")
  base::dir.create(base::dirname(path_miss_attrs),
                   showWarnings=FALSE,recursive=FALSE)
  # Run check
  exp_attrs <- base::unique(base::unlist(base::unname(attr_vars)))
  if(base::nrow(dt_all) >0){
    df_miss_attrs_nest <- dt_all %>% dplyr::group_by(featureID) %>%
      dplyr::summarize(attribute = base::list(base::setdiff(exp_attrs,
                                                            base::unique(attribute)))) %>%
      dplyr::filter(base::lengths(attribute) > 0)
    # Convert to long format & add timestamp:
    df_miss_attrs <- df_miss_attrs_nest  %>% tidyr::unnest(attribute)
  } else {
    df_miss_attrs <- data.frame()
  }

  if(base::nrow(df_miss_attrs)>0){
    df_miss_attrs$dl_timestamp <- base::as.character(base::as.POSIXct(
      base::format(Sys.time()),tz="UTC"))

    # Add the data source id compatible with `proc.attr.hydfab::retr_attr_new`
    df_miss_attrs$data_source_type <- NA
    idxs_in <- list()
    for(srce in base::names(attr_vars)){
      print(srce)
      idxs_in[[srce]] <- base::which(df_miss_attrs$attribute %in% attr_vars[[srce]])
      if(base::length(idxs_in)>0){
        df_miss_attrs$data_source_type[idxs_in[[srce]]] <- srce
      }
    }#Finish associated attribute source type to df (usgs_vars, ha_vars,etc)

    warn_msg <- "The following comids could not acquire some attributes: \n"

    for(n in 1:base::nrow(df_miss_attrs_nest)){
      row_msg <- paste0(df_miss_attrs_nest[n,'featureID'],": ",
                        paste0(df_miss_attrs_nest[n,'attribute'][[1]][[1]],
                               collapse="|"))
      warn_msg <- paste0(warn_msg,'\n',row_msg,'\n')
    }
    warning(warn_msg)
    # First check to see if missing dataset exists, if so - update
    if(base::file.exists(path_miss_attrs)){
      exst_data <- utils::read.csv(path_miss_attrs,stringsAsFactors = FALSE)
      exst_data$featureID <- as.character(exst_data$featureID)
      # Check for new data
      new_data <- dplyr::anti_join(df_miss_attrs, exst_data,
                                   by = c("featureID", "attribute"))
      updt_data <- dplyr::bind_rows(exst_data, new_data)
    } else{
      updt_data <- df_miss_attrs
    }
    utils::write.csv(updt_data, path_miss_attrs,row.names = FALSE)
  }
}


proc_attr_wrap <- function(comid, Retr_Params, lyrs='network',overwrite=FALSE,hfab_retr=FALSE){
  #' @title DEPRECATED. Wrapper to retrieve variables when processing attributes
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description DEPRECATED. Use [proc_attr_mlti_wrap] instead.
  #' Identifies a single comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Re-processing runs only download data that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   featureID - e.g. USGS common identifier (default)
  #'   featureSource - e.g. "COMID" (default)
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comid character. The common identifier USGS location code for a surface water feature.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @param hfab_retr boolean. Should the hydrofabric geopackage data be retrieved? Default FALSE.
  #' @seealso \link[proc.attr.hydfab]{proc_attrs_gageids}
  #' @seealso \link[proc.attr.hydfab]{proc_attr_mlti_wrap}
  #' @export

  # Changelog / Contributions
  #   2024-07-25 Originally created, GL
  message(base::paste0("Processing COMID ",comid))

  if(hfab_retr){ # Retreive the hydrofabric data, downloading to dir_db_hydfab
    # Retrieve the hydrofabric id
    # TODO proc_attr_hf doesn't work as expected. Consider overhaul
    net <- try(proc.attr.hydfab::proc_attr_hf(comid=comid,
                                              dir_db_hydfab=Retr_Params$paths$dir_db_hydfab,
                                              custom_name ="{lyrs}_",
                                              lyrs=Retr_Params$xtra_hfab$lyrs,
                                              hf_version = Retr_Params$xtra_hfab$hf_version,
                                              type = Retr_Params$xtra_hfab$type,
                                              domain = Retr_Params$xtra_hfab$domain,
                                              overwrite=overwrite))
    if ('try-error' %in% base::class(net)){
      warning(glue::glue("Could not acquire hydrofabric for comid {comid}. Proceeding to acquire variables of interest without hydrofabric."))
      net <- list()
      net$hf_id <- comid
    }
  } else {
    net <- list()
    net$hf_id <- comid
  }

  # Define the path to the attribute parquet file (name contains comid)
  path_attrs <- proc.attr.hydfab::std_path_attrs(comid=net$hf_id,
                                   dir_db_attrs=Retr_Params$paths$dir_db_attrs)

  vars_ls <- Retr_Params$vars
  # ------- Retr_Params$vars format checker --------- #
  # Run check on requested variables for retrieval:
  proc.attr.hydfab:::wrap_check_vars(vars_ls)

  # ----------- existing dataset checker ----------- #
  ls_chck <- proc.attr.hydfab::proc_attr_exst_wrap(comid,path_attrs,
                                                   vars_ls,bucket_conn=NA)
  dt_all <- ls_chck$dt_all
  need_vars <- ls_chck$need_vars

  # --------------- dataset grabber ---------------- #
  # attr_data <- list()
  # if (('ha_vars' %in% base::names(need_vars)) &&
  #     (base::all(!base::is.na(need_vars$ha_vars)))){
  #   # Hydroatlas variable query; list name formatted as {dataset_name}__v{version_number}
  #   attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(path_ha=Retr_Params$paths$s3_path_hydatl,
  #                                         hf_id=net$hf_id,
  #                                         ha_vars=need_vars$ha_vars) %>%
  #                               # ensures 'COMID' exists as colname
  #                               dplyr::rename("COMID" = "hf_id")
  # }
  # if( (base::any(base::grepl("usgs_vars", base::names(need_vars)))) &&
  #     (base::all(!base::is.na(need_vars$usgs_vars))) ){
  #   # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{version_number}
  #   attr_data[['usgs_nhdplus__v2']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=net$hf_id,
  #                                                               usgs_vars=need_vars$usgs_vars)
  # }
  attr_data <- proc.attr.hydfab::retr_attr_new(comids=net$hf_id,need_vars=need_vars,
                             path_ha=Retr_Params$paths$s3_path_hydatl)

  ########## May add more data sources here and append to attr_data ###########
  # ----------- dataset standardization ------------ #
  # if (!base::all(base::unlist(( # A qa/qc check
  #         base::lapply(attr_data, fu <- tion(x)
  #                 base::any(base::grepl("COMID", colnames(x)))))))){
  #   stop("Expecting 'COMID' as a column name identifier in every dataset")
  # }

  # Ensure consistent format of dataset
  # attr_data_ls <- list()
  # for(dat_srce in base::names(attr_data)){
  #   sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
  #   # Even though COMID always expected, use featureSource and featureID for
  #   #.  full compatibility with potential custom datasets
  #   sub_dt_dat$featureID <- base::as.character(sub_dt_dat$COMID)
  #   sub_dt_dat$featureSource <- "COMID"
  #   sub_dt_dat$data_source <- base::as.character(dat_srce)
  #   sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
  #     base::format(Sys.time()),tz="UTC"))
  #   sub_dt_dat <- sub_dt_dat %>% dplyr::select(-COMID)
  #   # Convert from wide to long format
  #   attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
  #                            id.vars = c('featureID','featureSource', 'data_source','dl_timestamp'),
  #                            variable.name = 'attribute')
  # }

  # Combine freshly-acquired data
  dt_new_dat <- data.table::rbindlist(attr_data,use.names=TRUE,fill=TRUE)
  #dt_new_dat <- data.table::rbindlist(attr_data_ls)

  # Combined dt of existing data and newly acquired data
  if(base::dim(dt_all)[1]>0 && base::dim(dt_new_dat)[1]>0){
    dt_cmbo <- data.table::merge.data.table(dt_all,dt_new_dat,
                                            all=TRUE,no.dups=TRUE)
  } else if (base::dim(dt_new_dat)[1] >0){
    dt_cmbo <- dt_new_dat
  } else {
    dt_cmbo <- dt_all
  }
  # Remove all factors to make arrow::open_dataset() easier to work with
  dt_cmbo <- dt_cmbo %>% dplyr::mutate(across(where(is.factor), as.character))

  # Write attribute variable data specific to a comid here
  arrow::write_parquet(dt_cmbo,path_attrs)
  return(dt_cmbo)
}

std_path_map_loc_ids <- function(dir_db_attrs){
  #' @title Standardize the path of the csv file that maps NLDI IDs to comids
  #' @description Uses a sub-directory in the dir_db_attrs to place data
  #' @param dir_db_attrs The attributes database path
  dir_meta_loc <- file.path(dir_db_attrs,'meta_loc')
  path_meta_loc <- file.path(dir_meta_loc,"comid_featID_map.csv")
  if(!dir.exists(dir_meta_loc)){
    base::dir.create(base::dirname(path_meta_loc),showWarnings = FALSE)
  }
  return(path_meta_loc)
}


retr_comids <- function(gage_ids,featureSource,featureID,dir_db_attrs,
                        path_save_gpkg=NULL){
  #' @title Retrieve comids based on provided gage_ids and expected NLDI format
  #' @details The gage_id-comid mappings are saved to file to avoid exceeding
  #' the NLDI database connection rate limit
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The [nhdplusTools::get_nldi_feature]feature featureSource,
  #' e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for [nhdplusTools::get_nldi_feature]. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #'  Refer to ?dataRetrieval::get_nldi_sources() for options to use with nldi_feature
  #' @param dir_db_attrs Attribute directory path, where the standardized
  #' comid-gage_id will be stored as a .csv
  #' @param path_save_gpkg The filepath where the geopackage containing
  #' comid-gageid-geometry mappings are saved. Default NULL, but strongly recommended
  #' to use!
  #' @note 2025-03-07 This needs a deeper refactoring
  #' @export
  #'
  # Changelog/Contributions
  #  2025-03-07 Refactor: add in the geometry retrieval & return nested list
  #   with sf_comid from fs_retr_nhdp_comids_geom_wrap, GL
  # ---------------- COMID & COORDINATE RETRIEVAL ---------------- #
  # Populate the comids & coordinates for each gage_id
  if(base::is.null(path_save_gpkg)){
    warning("Strongly recommended to provide a path_save_gpkg to proc.attr.hydfab::retr_comids!!")
    sf_comid <- data.table::data.table()
  } else {
    sf_comid <- proc.attr.hydfab::fs_retr_nhdp_comids_geom_wrap(
      path_save_gpkg=path_save_gpkg,
      gage_ids=gage_ids,featureSource=featureSource,
      featureID=featureID)
  }
  # ---------------- COMID RETRIEVAL: DOUBLE CHECK ------------------- #
  # Use std function that makes the path_meta_loc
  path_meta_loc <- proc.attr.hydfab:::std_path_map_loc_ids(dir_db_attrs)
  if(base::file.exists(path_meta_loc)){
    if(!base::grepl('csv',path_meta_loc)){
      stop(glue::glue("Expecting the file path to metadata to be a csv:
                      \n{path_meta_loc}"))
    }
    df_comid_featid <- utils::read.csv(path_meta_loc,colClasses = 'character')
  } else {
    df_comid_featid <- base::data.frame()
  }
  ls_featid <- base::list()
  ls_comid <- base::list()
  for (i in 1:length(gage_ids)){ #
    gage_id <- gage_ids[[i]]
    if(!base::exists("gage_id")){
      stop("MUST use 'gage_id' as the object name!!! \n
        Expected when defining nldi_feat$featureID")
    }

    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html
    nldi_feat <- base::list(featureSource =featureSource,
                            featureID = as.character(glue::glue(featureID)) # This should expect {'gage_id'} as a variable!
    )
    ls_featid[[i]] <- nldi_feat
    if(nldi_feat$featureSource == 'comid'){ # A simple case that may not off
      comid <- nldi_feat$featureID
    } else if(base::any(df_comid_featid$featureID == nldi_feat$featureID)){
      # Check the comid-featureID mapped database first
      comid <- df_comid_featid$comid[df_comid_featid$featureID == nldi_feat$featureID]

      if(base::length(comid)>1){
        stop(glue::glue("Problem with comid database logic. Look at how many
        entries exist for comid {comid} in the comid_featID_map.csv"))
      }
    } else if (base::any(sf_comid$gage_id == gage_id)){
      # Then check the geopackage database
      comid <- sf_comid$comid[sf_comid$gage_id == gage_id]
      if(base::length(comid)!=1){
        stop(glue::glue("Problem with geopackage logic. Look at how many
        entries exist for comid {comid} in {path_save_gpkg}"))
      }
    } else if (base::is.na(gage_id) || base::is.null(gage_id)) {
      comid <- NA # Hand NA/NULL values for gage_id
    } else {
      # TODO 2025-03-07: This section could be simplified now that
      #. fs_retr_nhdp_comids_geom_wrap() exists. For now it serves a double-check.
      comid <- try(nhdplusTools::discover_nhdplus_id(nldi_feature = nldi_feat))
      if('try-error' %in% base::class(comid)||length(comid)==0){
        site_feature <- try(nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat))

        if('try-error' %in% base::class(site_feature)){
          stop(glue::glue("The following nldi features didn't work. You may need to
                 revisit the configuration yaml file that processes this dataset in
                fs_prep: \n {featureSource}, and featureID={featureID}"))
        } else if (!is.null(site_feature)){
          if(!base::is.na(site_feature['comid']$comid)){
            comid <- site_feature['comid']$comid
          } else {
            message(glue::glue("Could not retrieve comid for {nldi_feat$featureID}."))
            comid <- nhdplusTools::discover_nhdplus_id(point=site_feature$geometry)
            message(glue::glue("Geospatial search found a comid value of: {comid}"))
          }
        }
      }
    }
    ls_comid[[i]] <- comid
  }

  # Combine/Update the custom mapper and write to file:
  df_featid_new <- data.frame(featureID = as.character(unlist(base::lapply(ls_featid, function(x) (x$featureID)))),
                              featureSource = as.character(featureSource),
                              gage_id = base::as.character(gage_ids))
  df_featid_new$comid <- base::as.character(base::unlist(ls_comid))
  if(base::nrow(df_comid_featid)>0){
    df_featid_cmbo <- dplyr::bind_rows(df_featid_new,df_comid_featid[,c("featureID","featureSource","gage_id","comid")]) %>%
      dplyr::distinct()
  } else {
    df_featid_cmbo <- df_featid_new %>% dplyr::distinct()
  }



  if(!dir.exists(dirname(path_meta_loc))){
    base::dir.create(base::dirname(path_meta_loc),recursive = TRUE)
  }

  # Remove locations where gage_id and comid are NA before writing to file!!
  idx_na <- base::intersect(base::which(is.na(df_featid_cmbo$gage_id)), which(is.na(df_featid_cmbo$comid)))
  if(base::length(idx_na)>0){
    df_featid_cmbo <- df_featid_cmbo[-idx_na,]
  }
  # Save the metadata mappings to file
  utils::write.csv(x = df_featid_cmbo,file = path_meta_loc,row.names = FALSE)

  ls_retr_comids <- list(ls_comid = ls_comid, sf_comid = sf_comid)
  return(ls_retr_comids)
}


proc_attr_gageids <- function(gage_ids,featureSource,featureID,Retr_Params,
                              path_save_gpkg,lyrs="network",overwrite=FALSE){
  #' @title Process catchment attributes based on vector of gage ids.
  #' @description
  #' Prepares inputs for main processing step. Iterates over each location
  #' for grabbing catchment attribute data corresponding to the gage_id
  #' location. Acquires user-requested variables from multiple catchment
  #' attribute sources. Calls [proc_attr_wrap] which writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Returns a data.table of all data returned from \code{nhdplusTools::get_nldi_feature}
  #' that corresponded to the gage_ids.
  #' @details Intendended for a single dataset, aka input file directory
  #' containing data. For multiple datasets, refer to dataset looping inside
  #' \link[proc.attr.hydfab]{grab_attrs_datasets_fs_wrap}
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The \code{nhdplusTools::get_nldi_feature} feature
  #' featureSource, e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for \code{nhdplusTools::get_nldi_feature}. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #' @param Retr_Params list. List of list structure with parameters/paths
  #' needed to acquire variables of interest. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fs_prep}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. Not used in \code{proc_attr_gageids}
  #'  }
  #' @param path_save_gpkg character. The filepath where the geopackage of comid/gage_id/geometry mappings get saved
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @seealso \link[proc.attr.hydfab]{grab_attrs_datasets_fs_wrap} loops over multiple datasets in calling this gage_id retrieval
  #' @seealso \link[proc.attr.hydfab]{proc_attr_mlti_wrap} Acquire attributes
  #' @export
  #  Changelog/Contributions
  #   2024-07-29 Originally created, GL
  #.  2025-03-07 add path_save_gpkg capability, GL
  # Path checker/maker of anything that's a directory not formatted for later glue::glue() calls

  if(!base::is.null(path_save_gpkg)){ # Add path save gpkg to parameter object
    Retr_Params$paths$path_save_gpkg <- path_save_gpkg
  } # Now we're ready for creating non-existent directories!
  for(dir in Retr_Params$paths){
    if(base::grepl('dir',dir)){
      if(!base::dir.exists(dir) && !base::grepl("\\{",dir)){
        message(glue::glue("Creating {dir}"))
        base::dir.create(dir)
      }
    }
  }

  # Should hydrofabric data be retrieved?
  hfab_retr <- Retr_Params$xtra_hfab$hfab_retr
  if(base::is.null(hfab_retr)){ # Use default in the proc_attr_wrap() function
    hfab_retr <- base::formals(proc.attr.hydfab::proc_attr_wrap)$hfab_retr
  }

  ls_retr_comid <- proc.attr.hydfab::retr_comids(gage_ids=gage_ids,
                          featureSource=featureSource,
                          featureID=featureID,
                          path_save_gpkg=path_save_gpkg,
                          dir_db_attrs=Retr_Params$paths$dir_db_attrs)
  base::names(ls_retr_comid$ls_comid) <- gage_ids
  just_comids <- ls_retr_comid$ls_comid %>% base::unname() %>% base::unlist()

  # ---------- RETRIEVE DESIRED ATTRIBUTE DATA FOR EACH LOCATION ------------- #
  dt_site_feat_retr <- proc.attr.hydfab::proc_attr_mlti_wrap(
                    comids=just_comids,Retr_Params=Retr_Params,
                    lyrs=lyrs,overwrite=overwrite)

  # Add the original gage_id back into dataset **and ensure character class!!**
  df_map_comid_gageid <- base::data.frame(featureID=as.character(just_comids),
                                          gage_id=as.character(gage_ids))
  dt_site_feat_retr$featureID <- as.character(dt_site_feat_retr$featureID)
  non_dupe_dt_site_feat_retr <- dt_site_feat_retr %>% dplyr::distinct()
  dt_site_feat <- data.table::merge.data.table(non_dupe_dt_site_feat_retr,
                                               df_map_comid_gageid,
                                               by="featureID",
                                               all=TRUE,no.dups=TRUE)

  if(base::any(!base::names(ls_retr_comid$ls_comid) %in% dt_site_feat$gage_id) ||
     base::any(is.na(dt_site_feat$featureID))){
    # Check for missing comids
    missing_the_comid <- dt_site_feat$gage_id[base::which(
      base::is.na(dt_site_feat$featureID))]
    # Check for missing gage_ids
    gage_ids_missing <- base::names(ls_retr_comid$ls_comid)[base::which(
        !base::names(ls_retr_comid$ls_comid) %in% dt_site_feat$gage_id)]

    gage_ids_missing <- c(missing_the_comid,gage_ids_missing) %>% base::unique()
    warning(glue::glue("The following gage_id values did not return a comid:\n
                       {paste0(gage_ids_missing,collapse=',')}"))
  }

  return(dt_site_feat)
}

read_loc_data <- function(loc_id_filepath, loc_id, fmt = 'csv'){
  #' @title Read location identifiers
  #' @description Reads directly from a csv or arrow-compatible dataset.
  #' Returns the dataset's column identifer renamed as 'gage_id' in a tibble
  #' @param loc_id_filepath csv filepath or dataset filepath/directory.
  #' @param loc_id The column name of the identifier column
  #' @param fmt The format passed to arrow::open_dataset() in the non-csv case.
  #' Default 'csv'. May also be 'parquet', 'arrow', 'feather', 'zarr', etc.
  #' @seealso \link[proc.attr.hydfab]{proc_attr_read_gage_ids_fs}
  #' @seealso \link[proc.attr.hydfab]{proc_attr_wrap}
  #' @export
  # Changelog / contributions
  #  2024-08-09 Originally created

  if (!base::is.null(loc_id_filepath)){
    # Figure out the colnames of everything in the dataset.
    cols <- arrow::open_dataset(loc_id_filepath, format = fmt) %>% base::colnames()
    # assign every col as a character string because leading zeros risk being dropped
    schema <- arrow::schema(!!!setNames(rep(list(arrow::string()), length(cols)), cols))
    # Read in dataset
    if (grepl('tsv|text|csv|txt',tools::file_ext(loc_id_filepath))){
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     col_types=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    } else {
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     schema=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    }
  } else {
    base::message(glue::glue("No location dataset defined. Reconsider designation for \n {loc_id_filepath}."))
    dat_loc <- NULL
  }
  return(dat_loc)
}

std_path_dataset <- function(dir_dataset, ds_filenames = ''){
  #' @title Return filepath of standardized formulation-selector gage_id
  #' location identifiers
  #' @param dir_dataset directory path containing the dataset
  #' @param ds_filenames a matching string specific to dataset(s) of interest
  #' inside \code{dir_dataset}
  #' @export
  # Changelog/contributions
  #. 2025-02-21 Refactored from proc_attr_read_gage_ids_fs
  # ----  Read in a standard format filename and file type from fs_prep ---- #
  # TODO make this more adaptable so that it doesn't depend on running python fs_prep beforehand
  dir_ds <- base::file.path(dir_dataset)
  files_ds <- base::list.files(dir_ds)
  fns <- base::lapply(ds_filenames,
                      function(x) files_ds[base::grep(x,files_ds)]) %>% unlist()

  if (base::any(base::grepl(".nc",fns))){ # Read in a netcdf file
    fn_nc <- fns[base::grep(".nc",fns)]
    if(length(fn_nc)!=1){
      stop(glue::glue("Expected that only one netcdf file exists in dir:\n{dir_ds}"))
    }
    path_dataset_in <- file.path(dir_dataset,fn_nc)
  } else {
    print(paste0("The following contents inside \n",dir_ds,
                 "\n do not match expected format:\n", paste0(fns, collapse = ", ")))
    stop("Create a different file format reader here that generates everything in the return list.")
    # TODO make this more adaptable so that it doesn't depend on running python fs_prep beforehand
    # Idea: e.g. read in user-defined gage_id data as a .csv
    # Idea: read in gage_id data inside a non-standard netcdf file, then define featureSource and featureID from a separate yaml file
  }
  return(path_dataset_in)
}

proc_attr_read_gage_ids_fs <- function(dir_dataset, ds_filenames=''){
  #' @title Read in standardized formulation-selector gage_id location identifiers
  #' @description Reads output generated using \pkg{fs_prep} python package and
  #' selects the gage_id location identifier(s) and the
  #' featureSource & featureID that correspond to the gage_id
  #' @param dir_dataset directory path to the dataset
  #' @param ds_filenames a matching string specific to dataset(s) of interest
  #' inside \code{dir_dataset}
  #' @details Returns a list of the following objects:
  #' gage_ids: array of gage_id values
  #' featureSource: The type of nhdplus feature source corresponding to gage_id
  #' featureID: The method of converting gage_id into a standardized featureSource's featureID
  #' @seealso \link[nhdplusTools]{get_nldi_feature}
  #' @export

  # Changelog/contributions
  #  2024-07-29 Originally created, GL
  #. 2025-02-21 Refactor with std_path_dataset
  #. 2025-03-07 Add path_dat_in as additional return object
  # ----  Read in a standard format filename and file type from fs_prep ---- #
  path_dat_in <- proc.attr.hydfab:::std_path_dataset(dir_dataset, ds_filenames)
  # Read the netcdf
  nc <- ncdf4::nc_open(path_dat_in)

  # Grab the gage_id identifier:
  gage_ids <- nc$dim$gage_id$vals

  # Extract attributes of interest that describe what gage_id represents
  attrs <- ncdf4::ncatt_get(nc,varid=0)
  featureSource <- attrs$featureSource
  featureID <- attrs$featureID # intended to reformat gage_id into the
  # appropriate nldi format using glue(e.g. glue('USGS-{gage_id}')

  return(base::list(gage_ids=gage_ids, featureSource=featureSource,
                    featureID=featureID, path_dat_in=path_dat_in))
}

grab_attrs_datasets_fs_wrap <- function(Retr_Params,lyrs="network",overwrite=FALSE,
                                        path_save_gpkg_cstm=NULL){
  #' @title Grab catchment attributes from processed formulation-selector input
  #' @description Wrapper function that acquires catchment attribute data from
  #' formulation-selector processed input generated via \pkg{fs_prep} package
  #' Returns list of data.table object of nhdplusTools::get_nldi_feature()
  #' for all gage_ids
  #' @param Retr_Params list of parameters built for grabbing catchment attribute data. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fs_prep}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. If 'all' is specified, then all datasets in the directory are processed.
  #'  \item \code{ds_type} character. The identifier to use in filename when writing attribute location metadata retrieved from nhdplusTools::get_nldi_feature()
  #'  }
  #' @param overwrite boolean default FALSE. Should hydrofabric data be overwritten?
  #' @param lyrs default "network" the hydrofabric layers of interest.
  #'  Only 'network' is needed for attribute grabbing.
  #' @param path_save_gpkg_cstm A custom path to save the geopackage mapping
  #' comid/gage_id/coords. Default NULL means that the path is automated based
  #' on the standardized input dataset location \link[proc.attr.hydfab]{std_path_retr_gpkg}
  #' @details Runs two proc.attr.hydfab functions:
  #'  \link[proc.attr.hydfab]{proc_attr_read_gage_ids_fs} - retrieves the gage_ids generated by \pkg{fs_prep}
  #'  \link[proc.attr.hydfab]{proc_attr_gageids} - retrieves the attributes for all provided gage_ids
  #'
  #' @export
  # Changelog/contributions
  #  2024-07-30 Originally created, GL
  #. 2025-03-12 Add path_save_gpkg_cstm, GL
  # 'all' an option if processing all datasets desired. Otherwise list datasets in config file
  all_ds <- base::basename(base::list.dirs(Retr_Params$paths$dir_std_base,
                                           recursive=FALSE))
  if(base::is.null(Retr_Params$datasets)){
    datasets <- NULL # No datasets defined - will grab from separate loc_id file
  } else if (Retr_Params$datasets[1]=='all'){ # Process all datasets inside a directory
    datasets <- all_ds
  } else { # Only process those datasets listed inside the directory
    datasets <- Retr_Params$datasets
  }

  if(base::any(!datasets %in% all_ds)){ # Run check that dataset exists
    bad_ds <- paste0(datasets[which(!datasets %in% all_ds)], collapse = ", ")
    good_ds <- paste0(all_ds, collapse = ", ")
    stop(base::paste0("The following datasets do not exist in the directory\n",
                      Retr_Params$paths$dir_std_base, "/: \n ",bad_ds,"\n",
                      "\n These options exist in that directory:\n",good_ds,
                      "\n\n Reconsider the dataset and/or directory choice."))
  }


  ls_sitefeat_all <- base::list()
  for(ds in datasets){ # Looping by dataset
    message(glue::glue("--- PROCESSING {ds} DATASET ---"))

    dir_dataset <- proc.attr.hydfab::std_dir_dataset(Retr_Params$paths$dir_std_base,ds)

    # Retrieve the gage_ids, featureSource, & featureID from fs_prep standardized output
    ls_fs_std <- proc.attr.hydfab::proc_attr_read_gage_ids_fs(dir_dataset)

    # TODO add option to read in gage ids from a separate data source
    gage_ids <- ls_fs_std$gage_ids
    featureSource <- ls_fs_std$featureSource
    featureID <- ls_fs_std$featureID
    fs_path <- ls_fs_std$path_dat_in
    if(base::is.null(path_save_gpkg_cstm)){
      # The standardized geopackage filepath
      path_save_gpkg <- proc.attr.hydfab:::std_path_retr_gpkg(fs_path)
    } else {
      path_save_gpkg <- path_save_gpkg_cstm
    }


    # ---------------------- Grab all needed attributes ---------------------- #
    dt_site_feat <- proc.attr.hydfab::proc_attr_gageids(gage_ids,
                                                 featureSource,
                                                 featureID,
                                                 Retr_Params,
                                                 path_save_gpkg=path_save_gpkg,
                                                 lyrs=lyrs,
                                                 overwrite=overwrite)
    dt_site_feat$dataset_name <- ds
    ls_sitefeat_all[[ds]] <- dt_site_feat
  }
  # -------------------------------------------------------------------------- #
  # ------------ Grab attributes from a separate loc_id file ----------------- #
  if (!base::is.null(Retr_Params$loc_id_read$loc_id_filepath)){
    # NOTE 2024-10-25: this feature hasn't been fully developed and may be ignored
    # Generate list of identifiers
    dat_loc <- proc.attr.hydfab::read_loc_data(Retr_Params$loc_id_read$loc_id_filepath,
                                               Retr_Params$loc_id_read$gage_id,
                                               fmt = Retr_Params$loc_id_read$fmt)

    if(base::nrow(dat_loc)>0){
      # TODO bugfix this here
      # ls_fs_std <- proc.attr.hydfab::proc_attr_read_gage_ids_fs(dir_dataset)
      # path_save_gpkg <- proc.attr.hydfab:::std_path_retr_gpkg(fs_path)
      if(base::is.null(path_save_gpkg_cstm)){
        warning("STRONGLY RECOMMENDED that user provide path_save_gpkg_cstm to
                proc.attr.hydfab::grab_attrs_datasets_fs_wrap.")
      } else {
        path_save_gpkg <- path_save_gpkg_cstm
      }

      dt_site_feat <- proc.attr.hydfab::proc_attr_gageids(gage_ids=as.array(dat_loc[['gage_id']]),
                                                           featureSource=Retr_Params$loc_id_read$featureSource_loc,
                                                           featureID=Retr_Params$loc_id_read$featureID_loc,
                                                           Retr_Params,
                                                           path_save_gpkg=path_save_gpkg,
                                                           lyrs=lyrs,
                                                           overwrite=overwrite)
      dt_site_feat$dataset_name <- Retr_Params$loc_id_read$loc_id_filepath
    } else {
      warning("TODO: add check that user didn't provide parameter expecting to read data")
      # TODO add check that user didn't provide parameter expecting to read data
    }
    # Combine lists
    ls_sitefeat_all[[Retr_Params$loc_id_read$loc_id_filepath]] <- dt_site_feat
  }

  # -------------------------------------------------------------------------- #
  # ------------------- Write attribute metadata to file

  for(ds in base::names(ls_sitefeat_all)){
    # Define the objects expected in path_meta for glue-formatting
    ds <- ds # object named ds for glue formatting e.g. nldi_feat_{ds}
    ds_type <- Retr_Params$ds_type
    dir_std_base <- Retr_Params$paths$dir_std_base
    write_type <- Retr_Params$write_type
    path_meta <- glue::glue(Retr_Params$paths$path_meta)

    bool_path_meta <- (base::is.null(path_meta)) || (base::grepl("\\{", path_meta))
    if(is.na(bool_path_meta)){ # some glue objects not defined
      objs_glue <- base::list(ds_type=ds_type,write_type=write_type,
                        dir_std_base=dir_std_base,path_meta=path_meta,
                        ds=ds)
      # Which objects that could be defined in glue are not?
      ids_need_defined <- names(objs_glue)[unlist(lapply(names(objs_glue),
                             function(x) is.null(objs_glue[[x]])))]

      stop(glue::glue("path_meta not fully defined. Be sure that Retr_Params contains
           appropriate objects, e.g. {paste0(ids_need_defined,collapse=', ')}
           for Retr_Params$paths$path_meta:\n{Retr_Params$paths$path_meta}"))
    }
    proc.attr.hydfab::write_meta_nldi_feat(dt_site_feat = ls_sitefeat_all[[ds]],
                         path_meta = path_meta)
  }

  return(ls_sitefeat_all)
}

write_meta_nldi_feat <- function(dt_site_feat, path_meta){
  #' @title Write metadata from NLDI retrieval
  #' @description
    #' A short description...
  #' @seealso \link[proc.attr.hydfab]{proc_attr_gageids}
  #' @param dt_site_feat data.table or data.frame of NLDI site features
  #' retrieved using nhdplusTools::get_nldi_feature() and organized
  #' using proc.attr.hydfab::proc_attr_gageids
  #' @param path_meta the filepath for writing nldi metadata. May be parquet or csv file.
  #' @export

  if(base::grepl("\\{",path_meta)){
    stop("path_meta passed into write_meta_nldi_feat still has glue formatted
         string containing '{}'. Make sure the object inside the curly brackets
         is defined before calling write_meta_nldi_feat().")
  }

  if(!base::dir.exists(base::dirname(path_meta))){
    warning(glue::glue(
      "The dataset directory is expected to exist: {base::dirname(path_meta)}. Creating it."))
    base::dir.create(base::dirname(path_meta),recursive = TRUE)
  }

  # Check to see if any sfc_POINT objects exist & remove in order to write table
  dtype_sfc_bool <- base::lapply(base::colnames(dt_site_feat),
                   function(x) base::any(base::grepl("sfc",
                                                     class(dt_site_feat[[x]]))))
  geom_cols <- base::colnames(dt_site_feat)[base::unlist(dtype_sfc_bool)]

  if (base::length(geom_cols)>0){ # Remove the sfc-formatted coordinates
    xy_df <- sf::st_coordinates(dt_site_feat[[geom_cols]])
    dt_site_feat <- dt_site_feat %>% dplyr::select(-dplyr::all_of(geom_cols))
    if(!base::any(base::grepl("X|lat|latitude",base::colnames(dt_site_feat)))){
      warning("Losing coordinates in the dataset. Consider adding them back in
              by modifying proc.attr.hydfab::write_meta_nldi_feat.")
    }
  }

  if(base::grepl('parquet',tools::file_ext(path_meta))){
    arrow::write_parquet(dt_site_feat,
                         path_meta)
  } else if(base::grepl('csv',tools::file_ext(path_meta))){
    utils::write.csv(x=dt_site_feat,
                     file=path_meta,
                     row.names = FALSE)
  } else {
    stop("File extension is not in expected format of csv or parquet")
  }
  base::message(glue::glue("Wrote nldi location metadata to {path_meta}"))
}

wrap_check_vars <- function(vars_ls){
  #' @title Internal wrapper to run checks on requested attribute variable names
  #' @param vars_ls A named list from Retr_Params$vars in the standardized format
  #' @description Given a list of variable categories, each containing vectors
  #' of variable names, check the following:
  #' 1) the variable category is a recognized category name (e.g. 'usgs_vars')
  #' 2) the variable names inside the category name are actual variable names
  #' that can be used to retrieve attributes (e.g. 'TOT_TWI' as an nhdplus attribute)

  # Get the accepted variable categories used in proc.attr.hydfab R package
  dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
  cfg_attr_src <- yaml::read_yaml(base::file.path(dir_extdata,"attr_source_types.yml"))
  var_catgs <- base::lapply(cfg_attr_src,
                            function(x) base::unlist(x)[['name']]) %>%
    base::unlist() %>% base::unname()

  # Now check what var categories provided by user in the the Retr_Params$vars
  names_var_catg <- base::names(vars_ls)
  if(base::any(base::is.null(names_var_catg))){
    stop(glue::glue("Retr_Params$vars should be a sublist with sublist names ",
                    "corresponding to\n standardized names in the proc.attr.hydfab package.",
                    " These names include:\n{paste0(var_catgs,collapse='\n')}"))
  }

  # Run test that the variable name is inside
  test_bool_var_catg <- base::lapply(names_var_catg,
                                     function(x) x %in% var_catgs) %>% unlist()
  if(base::any(!test_bool_var_catg)){
    stop(glue::glue("Retr_Params$vars contains the following unrecognized ",
                    "variable category name(s): ",
                    "{paste0(names_var_catg[!test_bool_var_catg],collapse='\n')}",
                    "\nAcceptable names include:\n",
                    "{paste0(var_catgs,collapse='\n')}"
    ))
  }

  # ------------------ RUN CHECK ON INDIVIDUAL VARIABLE NAMES -------------- #
  for(var_group_name in names(vars_ls)){
    sub_vars <- vars_ls[[var_group_name]]
    proc.attr.hydfab::check_attr_selection(vars=sub_vars)
  }
}

check_attr_selection <- function(attr_cfg_path = NULL, vars = NULL, verbose = TRUE){
  #' @title Check that attributes selected by user are available
  #' @author Lauren Bolotin \email{lauren.bolotin@noaa.gov }
  #' @description Sees if the attributes requested by a user matches with the
  #'  attributes supported by the package, listed in the attribute menu.
  #'  Returns list of
  #'   - missing_vars: a list of the requested variables that were not found
  #'        in the attribute menu as specified.
  #' @param attr_cfg_path a path to a .yaml configuration file specifying which
  #'        attributes a user is requesting
  #' @param vars a list specifying which attributes a user is requesting, in lieu
  #'        of a list coming from a .yaml configuration file
  #' @export

  # Read in the menu of attributes available through formulation-selector
  dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
  path_attr_menu <- base::file.path(dir_extdata, 'fs_attr_menu.yaml')
  attr_menu <- yaml::read_yaml(path_attr_menu)
  dataset_indices <- base::seq(1, base::length(attr_menu))

  if(!is.null(attr_cfg_path)){

    # Read in the user defined config of attributes of interest
    # attr_cfg_path <- paste0(dir_base, '/xssa_attr_config_all_vars_avail.yaml')
    attr_cfg <- yaml::read_yaml(attr_cfg_path)
    attr_cfg_sel <- attr_cfg[['attr_select']] # select the section for attributes
    attr_cfg_sel <- attr_cfg_sel[-1] # remove the s3 path to hydroatlas vars
    vars_sel <- attr_cfg_sel %>% base::unlist() %>% base::unname()

    print_query <- function(dataset_index, verbose = verbose){
      dataset <- attr_cfg_sel[[dataset_index]] %>% names()
      vars <- attr_cfg_sel[[dataset_index]] %>% unlist() %>% unname()
      if (!is.null(vars)){
        vars <- paste0(vars, collapse = ', ')
        msg <- glue::glue('Checking the ', dataset,
                          ' dataset for the following requested attributes: \n',
                          vars)
        message(msg)
      }

    }
    if(verbose){
      lapply(dataset_indices, print_query)
    }

  } else if(!is.null(vars)){
    # vars <- c("TOT_twi","TOT_PRSNOW","TOT_POPDENS90","TOT_EWT","TOT_RECHG","TOT_BFI")
    vars_sel <- vars
  } else {
    stop("Must provide attr_cfg_path or vars as arguments to check_attr_selection")
  }


  vars_menu <- NA
  # Compile the attribute menu into one list of variables
  create_menu_list <- function(dataset_index){
      dataset_vars <- attr_menu[[dataset_index]] %>% base::unlist() %>% base::names()
      vars_menu <<- c(vars_menu, dataset_vars)
    }
  lapply(dataset_indices, create_menu_list)


  # Warn the user of any requested attrs that are missing
  missing_vars <- vars_sel[base::which(!vars_sel %in% vars_menu)]
  missing_vars_list <- base::paste0(missing_vars, collapse=', ')

  # Only print a warning if the user requested unavailable attrs:
  if (base::length(missing_vars) > 0){
    # Tell the user they asked for something that's not available
    warn_msg <- glue::glue('The following attributes, as specified, were not found in the attribute menu:\n',
                 missing_vars_list, '\nPlease check spelling, capitalization, etc. and revise the *_attr_config.yaml', sep = ',')
    warning(warn_msg)
  }else{
    missing_vars <- NA
  }
  return(missing_vars)
}

hfab_config_opt <- function(hfab_config,
                            reqd_hfab=c("s3_base","s3_bucket","hf_cat_sel","source")){
  #' @title Configure hydrofabric-relevant optional params
  #' @description If an argument provided in the config file is NULL, first look
  #' for default param value from the \code{proc.attr_hydfab::proc_attr_hf}.
  #' If that is NULL, then look for default value from the
  #' \code{hfsubsetR::get_subset()} args if that param exists there. Otherwise,
  #' uses default param value in \code{proc.attr_hydfab::proc_attr_wrap}.
  #' @param hfab_config The hydrofabric-specific section from the config file, hydfab_config. list.
  #' @param reqd_hfab The non-optional item names in the hydrofabric config file
  #' @return List with default arguments populated corresponding to hfsubetR::get_subset(),
  #' @export

  # The values inside the hydrofabric configuration section from attr config file
  vals_hfab_config <- lapply(hfab_config, function(x) x[[names(x)]])
  names(vals_hfab_config) <-  base::lapply(hfab_config,
                                           function(x) base::names(x)) %>%
                base::unlist()
  # The required variables in the hydfab_config section:

  sub_hfab_config <- base::within(vals_hfab_config,base::rm(list=reqd_hfab))
  names_sub_hfab <- names(sub_hfab_config)


  xtra_cfig_hfab <- list()
  for(n in names_sub_hfab){
    x <- sub_hfab_config[[n]]
    if(base::is.null(x)){
      # Is this an argument inside proc_attr_hf()?
      bool_in_proc_attr_hf <- n %in%
        base::names(base::formals(proc.attr.hydfab::proc_attr_hf))
      # Is this an argument inside proc_attr_wrap()?
      bool_in_proc_attr_wrap <- n %in%
        base::names(base::formals(proc.attr.hydfab::proc_attr_wrap))
      # Is this an argument inside hsubsetR::get_subset()?
      bool_in_get_subset <- n %in%
        base::names(base::formals(hfsubsetR::get_subset))

      # Goal: default to value inside proc_attr_hf if default not NULL
      if (!base::is.null(base::formals(proc.attr.hydfab::proc_attr_hf)[[n]])){
        xtra_cfig_hfab[[n]] <- base::formals(proc.attr.hydfab::proc_attr_hf)[[n]]
      } else if (bool_in_get_subset) { # Otherwise use the default value in hfsubsetR::get_subset()
        def_val <- base::formals(hfsubsetR::get_subset)[[n]]
        xtra_cfig_hfab[[n]] <- def_val
      } else if (bool_in_proc_attr_wrap){ # Otherwise Use the wrapper's default value
        xtra_cfig_hfab[[n]] <- base::formals(proc.attr.hydfab::proc_attr_wrap)[[n]]
      }
    } else {
      xtra_cfig_hfab[[n]] <- x
    }
  }
  return(xtra_cfig_hfab)
}

std_path_miss_tfrm <- function(dir_db_attrs){
  #' @title Create a standardized path for storing missing comid-attribute
  #' pairings needed for attribute transformation
  #' @param dir_db_attrs The directory to the attribute database
  #' @seealso `fs_algo.tfrm_attrs.std_path_miss_tfrm` python package
  #' @export
  path_missing_attrs <- file.path(dir_db_attrs,"missing_tform",
                                  "needed_loc_attrs_for_tform.csv")
  return(path_missing_attrs)
}

std_path_miss_tfrm_io <- function(path_missing_attrs,  read=TRUE, df_miss=NULL){
  #' @title Read or write the standardized missing comid file for transformation
  #' @param path_missing_attrs The full path to file created by \link[proc.attr.hydfab]{std_path_miss_tfrm}
  #' @param df_miss The dataframe if interested in writing. Default NULL for reading.
  #' @param read Boolean. Should the missing data be read? Default TRUE
  #' @export
  if(read){
    df_miss <- utils::read.csv(path_missing_attrs,header=TRUE, check.names=TRUE)
    return(df_miss)
  } else if (!base::is.null(df_miss)) {
    utils::write.csv(x=df_miss,file = path_missing_attrs,row.names = FALSE)
  } else {
    stop("Inappropriate application of the standardized file i/o for missing comids")
  }
}

######## MISSING COMID-ATTRIBUTES ##########
fs_attrs_miss_wrap <- function(path_attr_config){
  #' @title DEPRECATED. Wrapper searching for comid-attribute data identified as
  #'  missing
  #' @details Use fs_attrs_miss_mlti_wrap instead.
  #' Given missing comid-attribute pairings previously identified
  #'  from fs_tfrm_attrs.py, and generated as a file by python function
  #'  `fs_algo.tfrm_attr.write_missing_attrs`
  #' @param path_attr_config The file path to the attribute config file
  #' @seealso `fs_algo.tfrm_attr.write_missing_attrs` python
  #' @seealso \link[proc.attr.hydfab]{fs_attrs_miss_mlti_wrap}
  #' @export
  # Changelog / Contributions
  #. 2024-12-31 Deprecated, GL

  # Generate the parameter list
  Retr_Params <- proc.attr.hydfab::attr_cfig_parse(path_attr_config = path_attr_config)

  path_missing_attrs <- proc.attr.hydfab::std_path_miss_tfrm(Retr_Params$paths$dir_db_attrs)
  df_miss <- proc.attr.hydfab::std_path_miss_tfrm_io(path_missing_attrs, read=TRUE)

  bool_chck_class_comid <- df_miss[['comid']][1] %>% as.character() %>%
    as.numeric() %>% suppressWarnings() %>% is.na() # Is the comid non-numeric?
  bool_chck_if_X_col <- df_miss %>% colnames() %>% grepl("X",.) %>% any()
  bool_chck_X_loc <- df_miss %>% colnames() %>% grep("X", .) == 1

  all_tests_df_miss_fmt <- c(bool_chck_class_comid,bool_chck_if_X_col,bool_chck_X_loc)
  if(base::all(all_tests_df_miss_fmt)){
    # We know 'X' is the first colname, so it's likely that R couldn't read
    #. the indices (duplicate vals when written in python?)
    cols <- colnames(df_miss)
    # The comid column is likely labeled as 'X'
    if ('uniq_cmbo' %in% cols){
      new_cols <-  cols[!grepl("uniq_cmbo",cols)]
    } else {
      new_cols <- cols
    }

    new_cols <- new_cols[!basegrepl("X",new_cols)]
    sub_df_miss <- df_miss[,1:(ncol(df_miss)-1)]
    names(sub_df_miss) <- new_cols

    last_col <- cols[length(cols)]
    # and the last col (e.g. dl_dataset) may become scrambled with the 'NA' column
    if(all(is.na(sub_df_miss[last_col])) && any(is.na(colnames(sub_df_miss)))){
      idx_col_na <- which(is.na(colnames(sub_df_miss)))
      sub_df_miss[last_col] <- sub_df_miss[,idx_col_na]
      sub_df_miss[,idx_col_na] <- NULL
    }
    df_miss <- sub_df_miss
  } else if (any(grepl("index",colnames(df_miss))) && !bool_chck_class_comid &&
             !bool_chck_if_X_col){
    # Remove the index column
    df_miss['index'] <- NULL
  } else if (bool_chck_class_comid){
    stop("THERE MAY BE A FORMAT ERROR WITH THE CORRECTION. MAKE SURE LOGIC IS APPROPRIATE HERE.")
  }

  if(base::nrow(df_miss)>0){
    message("Beginning search for missing comid-attribute pairings.")
    df_miss$uniq_cmbo <- paste0(df_miss$comid,df_miss$attribute) # The unique comid-attr combo
    # Read in proc.attr.hydfab package's extdata describing attributes & data sources
    dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
    path_attr_menu <- file.path(dir_extdata, "fs_attr_menu.yaml")
    df_attr_menu <- yaml::read_yaml(path_attr_menu)

    path_attr_src_types <- file.path(dir_extdata,"attr_source_types.yml")
    df_attr_src_types <- yaml::read_yaml(path_attr_src_types)

    # Identify which attributes correspond to which datasets using the menu
    attrs <- df_miss$attribute
    df_miss$dl_dataset <- NA
    for (dl_ds in names(df_attr_menu)){
      sub_df_attr_menu <- df_attr_menu[[dl_ds]]
      sub_attrs <- names(unlist(sub_df_attr_menu))
      ls_locs_df <- base::lapply(attrs, function(a)
        base::length(base::grep(a, sub_attrs))!=0 ) |>
        base::unlist()
      idxs_this_dl_ds <- base::which(ls_locs_df==TRUE)
      if(length(idxs_this_dl_ds)>0){
        print(glue::glue("Found attributes from {dl_ds} dataset"))
        df_miss$dl_dataset[idxs_this_dl_ds] <- unlist(df_attr_src_types[[dl_ds]])[["name"]]
      } else {
        print(glue::glue("No attributes correspond to {dl_ds} dataset"))
      }
    }

    # Check to make sure all attrs identified
    if(base::any(base::is.na(df_miss$dl_dataset))){
      unk_attrs <- df_miss$attribute[which(is.na(df_miss$dl_dataset))]
      str_unk_attrs <- paste0(unk_attrs, collapse = ", ")
      warning(glue::glue("Could not identify datasets for the following attributes:
                       \n{str_unk_attrs}"))
    }

    filter_df <- df_miss
    ls_sub_dt <- list() # NOTE consider removing this object if memory issues arise
    # Attempt to retrieve missing attributes for each comid of interest
    for (comid in unique(df_miss$comid)){

      sub_df_miss <- df_miss[df_miss$comid == comid,]


      var_ls <- lapply(unique(sub_df_miss$dl_dataset),
                       function(dl_ds) sub_df_miss[sub_df_miss$dl_dataset == dl_ds,'attribute'])
      names(var_ls) <- unique(sub_df_miss$dl_dataset)

      Retr_Params$vars <- var_ls

      # Note dt_cmbo contains all data for a comid, not just the requested data!
      dt_cmbo <- proc.attr.hydfab::proc_attr_wrap(comid=comid,
                                                  Retr_Params=Retr_Params,
                                                  lyrs="network",overwrite=FALSE,
                                                  hfab_retr=FALSE)


      sub_dt_cmbo <- dt_cmbo %>% subset(attribute %in% unlist(Retr_Params$vars))
      sub_dt_cmbo$uniq_cmbo <- paste0(sub_dt_cmbo$featureID,sub_dt_cmbo$attribute)

      ls_sub_dt[[comid]] <- sub_dt_cmbo # Tracking the new data
      # TODO drop NA values?

      if(base::any(base::is.na(sub_dt_cmbo$value))){
        stop(paste0("PROBLEM: {comid} has some NA values"))
      }

      # If data successfully retrieved, remove from the missing list.
      filter_df <- filter_df[!filter_df$uniq_cmbo %in% sub_dt_cmbo$uniq_cmbo,]

    }

    if (base::nrow(filter_df)== 0){
      message("Successfully found all missing attributes!")
    } else {
      message("Some missing comid-attribute pairings still remain")
    }
    # Now update the transformation's missing comid-attribute pairing file
    proc.attr.hydfab::std_path_miss_tfrm_io(path_missing_attrs,df_miss=filter_df,read=FALSE)

  } else {
    message("No missing comid-attribute pairings.")
  }
}

uniq_id_loc_attr <- function(comids,attrs){
  #' @title define the unique identifier of comid-attribute pairings
  #' @seealso \link[proc.attr.hydfab]{fs_attrs_miss_mlti_wrap}
  uniq_cmbo <- paste0(comids,"_",attrs)
  return(uniq_cmbo)
}

fs_attrs_miss_mlti_wrap <- function(path_attr_config){
  #' @title Wrapper searching for comid-attribute data identified as missing
  #' @details Given missing comid-attribute pairings previously identified
  #'  from fs_tfrm_attrs.py, and generated as a file by python function
  #'  `fs_algo.tfrm_attr.write_missing_attrs`
  #' @param path_attr_config The file path to the attribute config file
  #' @seealso `fs_algo.tfrm_attr.write_missing_attrs` python
  #' @seealso [fs_attrs_miss.R] Rscript that calls this wrapper
  #' @export
  # Changelog / Contributions
  #. 2024-12-31 Originally created, GL

  # Generate the parameter list
  Retr_Params <- proc.attr.hydfab::attr_cfig_parse(path_attr_config = path_attr_config)

  # Missing attributes specifically needed for transformation
  path_missing_attrs <- proc.attr.hydfab::std_path_miss_tfrm(Retr_Params$paths$dir_db_attrs)
  df_miss <- proc.attr.hydfab::std_path_miss_tfrm_io(path_missing_attrs, read=TRUE)
  # Remove any null comids:
  idxs_none <- base::which(df_miss$comid == "None")
  if(base::length(idxs_none)>0){
    df_miss <- df_miss[-idxs_none,]
  }
  df_miss$uniq_cmbo <- proc.attr.hydfab:::uniq_id_loc_attr(df_miss$comid,df_miss$attribute)
  if(base::nrow(df_miss)>0){
    message("Beginning search for missing comid-attribute pairings.")
    # The unique comid-attr combo:
    df_miss$uniq_cmbo <- proc.attr.hydfab:::uniq_id_loc_attr(df_miss$comid,
                                                             df_miss$attribute)



    # Group by 'comid' and aggregate the sets of 'attribute' values
    grouped <- df_miss %>%
      dplyr::group_by(comid) %>%
      dplyr::summarize(attribute = list(unique(attribute))) %>%
      dplyr::ungroup()

    # Convert the lists to characters to make them hashable
    grouped <- grouped %>%
      dplyr::mutate(attribute = sapply(attribute, function(x) paste(sort(x), collapse = ",")))

    # Find which 'comid' values share the same collections of 'attribute' values
    shared_values <- grouped %>%
      dplyr::group_by(attribute) %>%
      dplyr::summarize(comid = list(comid)) %>%
      dplyr::ungroup()
    ############# Map needed attributes to names in menu #################
    # Read in proc.attr.hydfab package's extdata describing attributes & data sources
    dir_extdata <- system.file("extdata",package="proc.attr.hydfab")
    path_attr_menu <- base::file.path(dir_extdata, "fs_attr_menu.yaml")
    df_attr_menu <- yaml::read_yaml(path_attr_menu)

    path_attr_src_types <- file.path(dir_extdata,"attr_source_types.yml")
    df_attr_src_types <- yaml::read_yaml(path_attr_src_types)

    # Identify which attributes correspond to which datasets using the menu
    # by looping over each unique grouping of comid-attribute pairings
    filter_df <- df_miss
    ls_have_uniq_cmbo <- list()
    for(row in 1:base::nrow(shared_values)){
      sub_grp <- shared_values[row,]
      comids <- sub_grp['comid'][[1]][[1]]
      attrs <- base::strsplit(sub_grp['attribute'][[1]],',')[[1]]
      #attrs <- df_miss$attribute
      vars_ls <- list()
      df_miss$dl_dataset <- NA
      for (dl_ds in base::names(df_attr_menu)){
        sub_df_attr_menu <- df_attr_menu[[dl_ds]]
        sub_attrs <- names(unlist(sub_df_attr_menu))
        ls_locs_df <- base::lapply(attrs, function(a)
          base::length(base::grep(a, sub_attrs))!=0 ) |>
          base::unlist()
        idxs_this_dl_ds <- base::which(ls_locs_df==TRUE)
        attrs_have <- attrs[idxs_this_dl_ds]

        if(base::length(idxs_this_dl_ds)>0){
          print(glue::glue("Found attributes from {dl_ds} dataset"))
          df_miss$dl_dataset[base::which(df_miss$attribute %in% attrs_have)] <-
            unlist(df_attr_src_types[[dl_ds]])[["name"]]
          vars_ls[[base::unlist(df_attr_src_types[[dl_ds]])[["name"]]]] <- attrs_have
        } else {
          print(glue::glue("No attributes correspond to {dl_ds} dataset"))
        }
      }

      # Check to make sure all attrs identified
      if(base::any(base::is.na(df_miss$dl_dataset))){
        unk_attrs <- df_miss$attribute[which(is.na(df_miss$dl_dataset))]
        str_unk_attrs <- paste0(unk_attrs, collapse = ", ")
        warning(glue::glue("Could not identify datasets for the following attributes:
                       \n{str_unk_attrs}"))
      }
      ############# Retrieve missing attributes #################
      # Perform retrieval using these variables that should be available
      Retr_Params$vars <- vars_ls

      # Acquire the needed variables
      message(glue::glue(
        "Retrieving {length(unlist(vars_ls))} attributes for {length(comids)} total comids.
        This may take a while."))
      dt_all <- try(proc.attr.hydfab::proc_attr_mlti_wrap(comids=comids,
                                            Retr_Params=Retr_Params,
                                            lyrs="network",overwrite=FALSE))
      if("try-error" %in% base::class(dt_all) || base::nrow(dt_all) == 0){
        comids_str <- base::paste0(comids,collapse='\n')
        warning(glue::glue("Could not retrieve attribute data for the following comids:
                {comids_str}"))
        next()
      }

      # The unique-id key for identifying unique location-attribute combinations
      ls_have_uniq_cmbo[[row]] <- proc.attr.hydfab:::uniq_id_loc_attr(dt_all$featureID,
                                                   dt_all$attribute)


      if(base::any(base::is.na(dt_all$value))){
        idxs_na <- which(is.na(dt_all$value))
        comids_problem <- paste0(dt_all$featureID[idxs_na],collapse=', ')
        stop(base::paste0("PROBLEM: The following comids hold NA values:
                          \n{comids_problem}"))
      }
    }

    # Identify which items from the missing list may now be removed
    have_uniq_cmbo <- base::unlist(ls_have_uniq_cmbo) # Data now available
    df_still_missing <- df_miss %>%
      dplyr::filter(!uniq_cmbo %in% have_uniq_cmbo)

    if (base::nrow(df_still_missing)== 0){
      message("Successfully found all missing attributes!")
    } else {
      message("Some missing comid-attribute pairings still remain")
    }
    # Now update the transformation's missing comid-attribute pairing file
    proc.attr.hydfab::std_path_miss_tfrm_io(path_missing_attrs,
                                       df_miss=df_still_missing,read=FALSE)
  } else {
    message("No missing comid-attribute pairings.")
  }
}

